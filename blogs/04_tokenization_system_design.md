# ğŸ”¥ HuggingFace Transformersåº“æ·±åº¦è§£æç³»åˆ—ï¼ˆå››ï¼‰ï¼šTokenizationç³»ç»Ÿè®¾è®¡ä¸ä¼˜åŒ–

> ä½œä¸ºOpenAIçš„æŠ€æœ¯æ¶æ„å¸ˆï¼Œä»Šå¤©æˆ‘å°†æ·±å…¥å‰–æTransformersåº“çš„Tokenizationç³»ç»Ÿã€‚è¿™æ˜¯NLPæ¨¡å‹çš„åŸºç¡€è®¾æ–½ï¼Œå…¶è®¾è®¡ç›´æ¥å½±å“æ¨¡å‹çš„æ€§èƒ½å’Œæ•ˆæœã€‚æœ¬æ–‡å°†ä»æºç å±‚é¢å½»åº•è§£æå„ç§åˆ†è¯ç®—æ³•çš„å®ç°åŸç†å’Œä¼˜åŒ–æŠ€æœ¯ã€‚

## ğŸ“‹ ç›®å½•

- [Tokenizationç³»ç»Ÿçš„æ ¸å¿ƒä½œç”¨](#tokenizationç³»ç»Ÿçš„æ ¸å¿ƒä½œç”¨)
- [æ•´ä½“æ¶æ„è®¾è®¡](#æ•´ä½“æ¶æ„è®¾è®¡)
- [åˆ†è¯ç®—æ³•çš„æ•°å­¦åŸç†ä¸å®ç°](#åˆ†è¯ç®—æ³•çš„æ•°å­¦åŸç†ä¸å®ç°)
- [WordPieceç®—æ³•æ·±åº¦å‰–æ](#wordpieceç®—æ³•æ·±åº¦å‰–æ)
- [BPEç®—æ³•å®ç°åˆ†æ](#bpeç®—æ³•å®ç°åˆ†æ)
- [SentencePieceç®—æ³•è§£æ](#sentencepieceç®—æ³•è§£æ)
- [é¢„å¤„ç†æŠ€æœ¯è¯¦è§£](#é¢„å¤„ç†æŠ€æœ¯è¯¦è§£)
- [è¯æ±‡è¡¨ç®¡ç†æœºåˆ¶](#è¯æ±‡è¡¨ç®¡ç†æœºåˆ¶)
- [ç‰¹æ®Štokenå¤„ç†ç­–ç•¥](#ç‰¹æ®Štokenå¤„ç†ç­–ç•¥)
- [å¿«é€Ÿåˆ†è¯å™¨å®ç°](#å¿«é€Ÿåˆ†è¯å™¨å®ç°)
- [å¤šè¯­è¨€æ”¯æŒä¸Unicodeå¤„ç†](#å¤šè¯­è¨€æ”¯æŒä¸unicodeå¤„ç†)
- [ç¼“å­˜æœºåˆ¶ä¸æ€§èƒ½ä¼˜åŒ–](#ç¼“å­˜æœºåˆ¶ä¸æ€§èƒ½ä¼˜åŒ–)
- [å®æˆ˜ä»£ç ç¤ºä¾‹](#å®æˆ˜ä»£ç ç¤ºä¾‹)
- [æ€§èƒ½å¯¹æ¯”ä¸æœ€ä½³å®è·µ](#æ€§èƒ½å¯¹æ¯”ä¸æœ€ä½³å®è·µ)
- [æ€»ç»“ä¸å±•æœ›](#æ€»ç»“ä¸å±•æœ›)

---

## ğŸ¯ Tokenizationç³»ç»Ÿçš„æ ¸å¿ƒä½œç”¨

Tokenizationæ˜¯NLPæ¨¡å‹çš„**ç¬¬ä¸€é“å·¥åº**ï¼Œå…¶è´¨é‡ç›´æ¥å½±å“æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ã€‚

### ğŸ”‘ å…³é”®ä½œç”¨

1. **æ–‡æœ¬æ•°å­—åŒ–**ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯å¤„ç†çš„æ•°å­—IDåºåˆ—
2. **è¯æ±‡å‹ç¼©**ï¼šå°†æ— é™è¯æ±‡è¡¨å‹ç¼©åˆ°å¯ç®¡ç†çš„å¤§å°
3. **è¯­ä¹‰ä¿æŒ**ï¼šå°½å¯èƒ½ä¿æŒåŸå§‹æ–‡æœ¬çš„è¯­ä¹‰ä¿¡æ¯
4. **è¾¹ç•Œå¤„ç†**ï¼šæ­£ç¡®å¤„ç†è¯è¾¹ç•Œå’Œå­è¯è¾¹ç•Œ
5. **ç‰¹æ®Šæ ‡è®°**ï¼šæ·»åŠ CLSã€SEPã€PADç­‰ç‰¹æ®Šæ ‡è®°

### ğŸ“Š æ€§èƒ½å½±å“

- **æ¨¡å‹æ•ˆæœ**ï¼šåˆ†è¯è´¨é‡ç›´æ¥å½±å“æ¨¡å‹ç†è§£èƒ½åŠ›
- **æ¨ç†é€Ÿåº¦**ï¼šåˆ†è¯ç®—æ³•å¤æ‚åº¦å½±å“é¢„å¤„ç†æ—¶é—´
- **å†…å­˜å ç”¨**ï¼šè¯æ±‡è¡¨å¤§å°å½±å“æ¨¡å‹å†…å­˜å ç”¨
- **æ³›åŒ–èƒ½åŠ›**ï¼šå¤„ç†æœªçŸ¥è¯çš„èƒ½åŠ›å½±å“æ¨¡å‹æ³›åŒ–æ€§

---

## ğŸ—ï¸ æ•´ä½“æ¶æ„è®¾è®¡

### ğŸ“ åˆ†å±‚æ¶æ„

```
åº”ç”¨å±‚ (Tokenizer API)
    â†“
ä¸šåŠ¡å±‚ (PreTrainedTokenizer)
    â†“
ç®—æ³•å±‚ (WordPiece/BPE/SentencePiece)
    â†“
é¢„å¤„ç†å±‚ (BasicTokenizer)
    â†“
æ•°æ®å±‚ (Vocabularyç®¡ç†)
    â†“
å·¥å…·å±‚ (Trie/æ­£åˆ™è¡¨è¾¾å¼)
```

### ğŸ—ºï¸ æ ¸å¿ƒç»„ä»¶å…³ç³»å›¾

```mermaid
graph TB
    subgraph "ç”¨æˆ·æ¥å£å±‚"
        A[AutoTokenizer]
        B[Tokenizer API]
    end

    subgraph "æŠ½è±¡åŸºç±»å±‚"
        C[PreTrainedTokenizerBase]
        D[PreTrainedTokenizer]
    end

    subgraph "å…·ä½“å®ç°å±‚"
        E[BertTokenizer]
        F[GPT2Tokenizer]
        G[LLaMATokenizer]
        H[T5Tokenizer]
    end

    subgraph "ç®—æ³•å±‚"
        I[WordPiece]
        J[BPE]
        K[SentencePiece]
    end

    subgraph "é¢„å¤„ç†å±‚"
        L[BasicTokenizer]
        M[Normalizer]
        N[PreTokenizer]
    end

    subgraph "æ•°æ®å±‚"
        O[Vocabulary]
        P[Trie]
        Q[SpecialTokens]
    end

    A --> D
    B --> D
    D --> E
    D --> F
    D --> G
    D --> H
    E --> I
    F --> J
    G --> K
    I --> L
    J --> M
    K --> N
    L --> O
    M --> P
    N --> Q
```

---

## ğŸ§® åˆ†è¯ç®—æ³•çš„æ•°å­¦åŸç†ä¸å®ç°

### ğŸ¯ WordPieceç®—æ³•

#### 1. **æ•°å­¦åŸç†**

WordPieceç®—æ³•åŸºäº**æœ€å¤§ä¼¼ç„¶ä¼°è®¡**ï¼Œç›®æ ‡æ˜¯æœ€å°åŒ–è¯æ±‡è¡¨å¤§å°åŒæ—¶æœ€å¤§åŒ–è®­ç»ƒæ•°æ®çš„ä¼¼ç„¶ã€‚

```python
# ç»™å®šè®­ç»ƒè¯­æ–™ C = {wâ‚, wâ‚‚, ..., wâ‚™}
# ç›®æ ‡ï¼šæ‰¾åˆ°è¯æ±‡è¡¨ V æœ€å°åŒ– -log P(C|V)

# æ¯ä¸ªè¯çš„ä¼¼ç„¶ï¼š
P(w) = âˆ_{i=1}^{|w|} P(s_i|s_{1:i-1})

# å…¶ä¸­ s_i æ˜¯ç¬¬iä¸ªå­è¯ï¼Œæ¡ä»¶æ¦‚ç‡ï¼š
P(s_i|s_{1:i-1}) = count(s_{1:i}) / count(s_{1:i-1})
```

#### 2. **è´ªå©ªåˆå¹¶ç­–ç•¥**

```python
# è¿­ä»£åˆå¹¶æœ€æœ‰ä»·å€¼çš„å­è¯å¯¹
while len(vocab) < target_size:
    # è®¡ç®—æ‰€æœ‰ç›¸é‚»å­è¯å¯¹çš„åˆ†æ•°
    scores = {}
    for pair in get_all_adjacent_pairs(corpus):
        new_token = pair[0] + pair[1]
        score = count(new_token) / (count(pair[0]) * count(pair[1]))
        scores[pair] = score

    # é€‰æ‹©åˆ†æ•°æœ€é«˜çš„å¯¹è¿›è¡Œåˆå¹¶
    best_pair = max(scores, key=scores.get)
    vocab.add(best_pair[0] + best_pair[1])

    # æ›´æ–°è¯­æ–™ç»Ÿè®¡
    update_corpus_counts(best_pair)
```

### ğŸ¯ BPE (Byte Pair Encoding) ç®—æ³•

#### 1. **æ•°å­¦åŸç†**

BPEåŸºäº**é¢‘ç‡ç»Ÿè®¡**ï¼Œè¿­ä»£åˆå¹¶æœ€é¢‘ç¹çš„å­—èŠ‚å¯¹ã€‚

```python
# åˆå§‹çŠ¶æ€ï¼šæ¯ä¸ªå­—ç¬¦ä½œä¸ºä¸€ä¸ªtoken
tokens = set(all_characters_in_corpus)

# è¿­ä»£è¿‡ç¨‹ï¼š
while len(tokens) < target_size:
    # ç»Ÿè®¡æ‰€æœ‰ç›¸é‚»tokenå¯¹çš„é¢‘ç‡
    pair_counts = count_all_adjacent_pairs(corpus)

    # æ‰¾åˆ°é¢‘ç‡æœ€é«˜çš„å¯¹
    most_frequent_pair = max(pair_counts, key=pair_counts.get)

    # åˆå¹¶è¿™å¯¹token
    new_token = most_frequent_pair[0] + most_frequent_pair[1]
    tokens.add(new_token)

    # æ›´æ–°è¯­æ–™ï¼šæ›¿æ¢æ‰€æœ‰å‡ºç°çš„ä½ç½®
    corpus = replace_all_occurrences(corpus, most_frequent_pair, new_token)
```

#### 2. **å®ç°å¤æ‚åº¦åˆ†æ**

- **æ—¶é—´å¤æ‚åº¦**ï¼šO(n Ã— k)ï¼Œå…¶ä¸­næ˜¯è¯­æ–™å¤§å°ï¼Œkæ˜¯ç›®æ ‡è¯æ±‡è¡¨å¤§å°
- **ç©ºé—´å¤æ‚åº¦**ï¼šO(v)ï¼Œå…¶ä¸­væ˜¯è¯æ±‡è¡¨å¤§å°
- **åˆå¹¶æ“ä½œ**ï¼šæ¯æ¬¡åˆå¹¶éœ€è¦O(n)æ—¶é—´æ‰«ææ•´ä¸ªè¯­æ–™

### ğŸ¯ SentencePieceç®—æ³•

#### 1. **æ•°å­¦åŸç†**

SentencePieceå°†æ–‡æœ¬è§†ä¸º**Unicodeåºåˆ—**ï¼Œä¸ä¾èµ–ç©ºæ ¼åˆ†è¯ã€‚

```python
# å°†æ–‡æœ¬è½¬æ¢ä¸ºUnicodeç ç‚¹åºåˆ—
text = "Hello world"
unicode_sequence = [ord(c) for c in text]  # [72, 101, 108, 108, 111, 32, 119, 111, 114, 108, 100]

# åŸºäºè¯­è¨€æ¨¡å‹çš„æ— ç›‘ç£åˆ†è¯
P(text) = âˆ_{i=1}^n P(u_i | u_{1:i-1})

# ä½¿ç”¨æ”¹è¿›çš„BPEç®—æ³•ï¼Œè€ƒè™‘è¯­è¨€æ¨¡å‹æ¦‚ç‡
score(pair) = frequency(pair) * language_model_probability(pair)
```

#### 2. **ä¼˜åŠ¿ç‰¹ç‚¹**

- **è¯­è¨€æ— å…³**ï¼šé€‚ç”¨äºä»»ä½•è¯­è¨€ï¼ŒåŒ…æ‹¬ç©ºæ ¼åˆ†éš”ç¬¦ä¸æ˜æ˜¾çš„è¯­è¨€
- **ä¸€è‡´æ€§**ï¼šé¢„å¤„ç†å’Œåˆ†è¯ä½¿ç”¨åŒä¸€ç®—æ³•
- **å¯é€†æ€§**ï¼šå¯ä»¥æ— æŸåœ°è¿˜åŸåŸå§‹æ–‡æœ¬

---

## ğŸ” WordPieceç®—æ³•æ·±åº¦å‰–æ

è®©æˆ‘ä»¬æ·±å…¥åˆ†æBERTä½¿ç”¨çš„WordPieceç®—æ³•å®ç°ï¼š

### ğŸ“ æ ¸å¿ƒæ•°æ®ç»“æ„

```python
# tokenization_utils.py:52-150
class Trie:
    """
    Trieæ•°æ®ç»“æ„ï¼Œç”¨äºé«˜æ•ˆåŒ¹é…added_tokens
    """
    def __init__(self, *args):
        self.data = {}
        self._tokens = set()
        self._termination_char = ""
        self.update(*args)

    def add(self, word: str):
        """
        æ·»åŠ å•è¯åˆ°Trie
        """
        if not word:
            return

        self._tokens.add(word)
        ref = self.data
        for char in word:
            ref = ref.setdefault(char, {})
        ref[self._termination_char] = 1

    def find(self, word: str) -> List[str]:
        """
        åœ¨å•è¯ä¸­æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„token
        """
        tokens = []
        for i in range(len(word)):
            for j in range(i + 1, len(word) + 1):
                substring = word[i:j]
                if substring in self._tokens:
                    tokens.append(substring)
        return tokens

    def split(self, text: str) -> List[str]:
        """
        ä½¿ç”¨æœ€é•¿åŒ¹é…ç®—æ³•åˆ†å‰²æ–‡æœ¬
        """
        tokens = []
        i = 0
        while i < len(text):
            longest_match = ""
            for j in range(i + 1, len(text) + 1):
                substring = text[i:j]
                if substring in self._tokens:
                    longest_match = substring
            if longest_match:
                tokens.append(longest_match)
                i += len(longest_match)
            else:
                tokens.append(text[i])
                i += 1
        return tokens
```

### ğŸ“ WordPieceTokenizerå®ç°

```python
# models/bert/tokenization_bert.py:800-900
class WordpieceTokenizer:
    """
    WordPieceåˆ†è¯å™¨å®ç°
    """
    def __init__(self, vocab, unk_token, max_input_chars_per_word=100):
        self.vocab = vocab
        self.unk_token = unk_token
        self.max_input_chars_per_word = max_input_chars_per_word

    def tokenize(self, text):
        """
        WordPieceåˆ†è¯å®ç°
        """
        output_tokens = []
        for token in whitespace_tokenize(text):
            chars = list(token)
            if len(chars) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []

            while start < len(chars):
                end = len(chars)
                cur_substr = None
                while start < end:
                    substr = "".join(chars[start:end])
                    if start > 0:
                        substr = "##" + substr
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1

                if cur_substr is None:
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)
                start = end

            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)

        return output_tokens
```

### ğŸ“ è´ªå©ªåˆ†è¯ç®—æ³•

```python
# models/bert/tokenization_bert.py:900-1000
def greedy_wordpiece_tokenize(self, token: str) -> List[str]:
    """
    è´ªå©ªWordPieceåˆ†è¯ç®—æ³•
    """
    if len(token) == 0:
        return []

    # ç‰¹æ®Šå¤„ç†è¿‡é•¿token
    if len(token) > self.max_input_chars_per_word:
        return [self.unk_token]

    output_tokens = []
    start = 0
    while start < len(token):
        # ä»åå‘å‰æŸ¥æ‰¾æœ€é•¿åŒ¹é…
        end = len(token)
        current_substring = None
        while start < end:
            substr = token[start:end]
            # é™¤äº†ç¬¬ä¸€ä¸ªå­è¯ï¼Œå…¶ä»–å­è¯éœ€è¦æ·»åŠ ##å‰ç¼€
            if start > 0:
                substr = "##" + substr
            if substr in self.vocab:
                current_substring = substr
                break
            end -= 1

        if current_substring is None:
            # æ— æ³•åˆ†è¯çš„æƒ…å†µ
            return [self.unk_token]

        output_tokens.append(current_substring)
        start = end

    return output_tokens
```

---

## ğŸ”§ BPEç®—æ³•å®ç°åˆ†æ

### ğŸ“ GPT2Tokenizerçš„BPEå®ç°

```python
# models/gpt2/tokenization_gpt2.py:100-200
class GPT2Tokenizer(PreTrainedTokenizer):
    """
    GPT2ä½¿ç”¨BPEç®—æ³•
    """
    def __init__(
        self,
        vocab_file,
        merges_file,
        errors="replace",
        unk_token="<|endoftext|>",
        **kwargs
    ):
        # 1. åŠ è½½è¯æ±‡è¡¨
        self.encoder = load_vocab(vocab_file)
        self.decoder = {v: k for k, v in self.encoder.items()}

        # 2. åŠ è½½åˆå¹¶è§„åˆ™
        self.bpe_ranks = load_bpe_merges(merges_file)
        self.cache = {}  # BPEç¼“å­˜

        # 3. ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
        self.pat = re.compile(
            r"""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
        )

    def bpe(self, token):
        """
        BPEç¼–ç å®ç°
        """
        if token in self.cache:
            return self.cache[token]

        word = tuple(token)
        word = list(word)[:-1] + [word[-1] + "</w>"]

        # ä½¿ç”¨åˆå¹¶è§„åˆ™è¿›è¡Œè¿­ä»£
        pairs = get_pairs(word)
        if not pairs:
            return token

        while True:
            # æ‰¾åˆ°ä¼˜å…ˆçº§æœ€é«˜çš„åˆå¹¶å¯¹
            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float("inf")))
            if bigram not in self.bpe_ranks:
                break

            # æ‰§è¡Œåˆå¹¶
            first, second = bigram
            new_word = []
            i = 0
            while i < len(word):
                try:
                    j = word.index(first, i)
                    new_word.extend(word[i:j])
                    i = j
                except:
                    new_word.extend(word[i:])
                    break

                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:
                    new_word.append(first + second)
                    i += 2
                else:
                    new_word.append(word[i])
                    i += 1

            new_word = tuple(new_word)
            word = new_word
            if len(word) == 1:
                break
            else:
                pairs = get_pairs(word)

        # ç¼“å­˜ç»“æœ
        word = " ".join(word)
        word = word.replace("</w>", "").strip()
        self.cache[token] = word
        return word

def get_pairs(word):
    """
    è·å–æ‰€æœ‰ç›¸é‚»å­—ç¬¦å¯¹
    """
    pairs = set()
    prev_char = word[0]
    for char in word[1:]:
        pairs.add((prev_char, char))
        prev_char = char
    return pairs
```

### ğŸ“ BPEè®­ç»ƒè¿‡ç¨‹

```python
# models/gpt2/tokenization_gpt2.py:300-400
def train_bpe(corpus, vocab_size, min_frequency=2):
    """
    è®­ç»ƒBPEè¯æ±‡è¡¨
    """
    # 1. åˆå§‹åŒ–ï¼šæ¯ä¸ªå­—ç¬¦ä½œä¸ºä¸€ä¸ªtoken
    vocab = set()
    for text in corpus:
        for char in text:
            vocab.add(char)

    # 2. ç»Ÿè®¡å­—ç¬¦é¢‘ç‡
    char_freq = {}
    for text in corpus:
        for char in text:
            char_freq[char] = char_freq.get(char, 0) + 1

    # 3. è¿­ä»£åˆå¹¶
    while len(vocab) < vocab_size:
        # ç»Ÿè®¡ç›¸é‚»å¯¹é¢‘ç‡
        pair_freq = {}
        for text in corpus:
            chars = list(text)
            for i in range(len(chars) - 1):
                pair = (chars[i], chars[i + 1])
                pair_freq[pair] = pair_freq.get(pair, 0) + 1

        # è¿‡æ»¤ä½é¢‘å¯¹
        pair_freq = {k: v for k, v in pair_freq.items() if v >= min_frequency}

        if not pair_freq:
            break

        # é€‰æ‹©æœ€é¢‘ç¹çš„å¯¹
        best_pair = max(pair_freq, key=pair_freq.get)
        new_token = best_pair[0] + best_pair[1]

        # æ·»åŠ åˆ°è¯æ±‡è¡¨
        vocab.add(new_token)

        # æ›´æ–°è¯­æ–™
        for i in range(len(corpus)):
            corpus[i] = corpus[i].replace(
                best_pair[0] + best_pair[1], new_token
            )

    return vocab
```

---

## ğŸŒ SentencePieceç®—æ³•è§£æ

### ğŸ“ SentencePieceæ ¸å¿ƒæ¦‚å¿µ

SentencePieceå°†æ–‡æœ¬è§†ä¸ºUnicodeåºåˆ—ï¼Œä¸ä¾èµ–ç©ºæ ¼ï¼š

```python
# ç‰¹ç‚¹ï¼š1. æ— ç©ºæ ¼ 2. å¯é€† 3. è¯­è¨€æ— å…³

class SentencePieceTokenizer:
    def __init__(self, model_prefix, vocab_size):
        self.model_prefix = model_prefix
        self.vocab_size = vocab_size
        self.vocab = None
        self.model = None

    def train(self, corpus):
        """
        è®­ç»ƒSentencePieceæ¨¡å‹
        """
        # 1. é¢„å¤„ç†ï¼šæ ‡å‡†åŒ–
        normalized_corpus = self._normalize(corpus)

        # 2. ç»Ÿè®¡Unicodeé¢‘ç‡
        char_freq = self._count_unicode_chars(normalized_corpus)

        # 3. åˆå§‹åŒ–è¯æ±‡è¡¨
        vocab = set(char_freq.keys())

        # 4. ä½¿ç”¨æ”¹è¿›çš„BPEç®—æ³•
        while len(vocab) < self.vocab_size:
            # ç»Ÿè®¡æ‰€æœ‰å¯èƒ½çš„å­è¯
            all_subwords = self._extract_all_subwords(normalized_corpus)

            # è®¡ç®—æ¯ä¸ªå­è¯çš„åˆ†æ•°
            subword_scores = {}
            for subword in all_subwords:
                score = self._calculate_subword_score(subword, normalized_corpus)
                subword_scores[subword] = score

            # é€‰æ‹©æœ€ä½³å­è¯
            best_subword = max(subword_scores, key=subword_scores.get)
            vocab.add(best_subword)

        return vocab

    def _calculate_subword_score(self, subword, corpus):
        """
        è®¡ç®—å­è¯åˆ†æ•°ï¼ˆåŸºäºè¯­è¨€æ¨¡å‹æ¦‚ç‡ï¼‰
        """
        # ä½¿ç”¨æ”¹è¿›çš„BPEåˆ†æ•°è®¡ç®—
        freq = corpus.count(subword)
        left_context_freq = 0
        right_context_freq = 0

        for i in range(len(corpus) - len(subword)):
            if corpus[i:i+len(subword)] == subword:
                if i > 0:
                    left_context_freq += 1
                if i + len(subword) < len(corpus):
                    right_context_freq += 1

        # è€ƒè™‘ä¸Šä¸‹æ–‡ä¿¡æ¯
        score = freq * math.log(left_context_freq + 1) * math.log(right_context_freq + 1)
        return score
```

### ğŸ“ LLaMAçš„SentencePieceå®ç°

```python
# models/llama/tokenization_llama.py:100-200
class LlamaTokenizer(PreTrainedTokenizer):
    """
    LLaMAä½¿ç”¨SentencePieceåˆ†è¯å™¨
    """
    def __init__(
        self,
        vocab_file,
        merges_file,
        unk_token="<unk>",
        bos_token="<s>",
        eos_token="</s>",
        **kwargs
    ):
        # 1. åŠ è½½SentencePieceæ¨¡å‹
        self.sp_model = load_sentencepiece_model(vocab_file)

        # 2. æ„å»ºè¯æ±‡è¡¨
        self.vocab = {self.sp_model.id_to_piece(i): i for i in range(self.sp_model.vocab_size())}

        # 3. ç‰¹æ®Štokenæ˜ å°„
        self.bos_token = bos_token
        self.eos_token = eos_token
        self.unk_token = unk_token

        # 4. é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
        self.pat = re.compile(
            r"""<\|startoftext\|>|<\|endoftext\|>|'s|'t|'re|'ve|'m|'ll|'d|[\p{L}]+|[\p{N}]|[^\s\p{L}\p{N}]+""",
            re.IGNORECASE,
        )

    def _tokenize(self, text):
        """
        SentencePieceåˆ†è¯å®ç°
        """
        # 1. é¢„å¤„ç†ï¼šæ·»åŠ ç‰¹æ®Štoken
        text = text.replace("<n>", "\n")

        # 2. ä½¿ç”¨SentencePieceæ¨¡å‹åˆ†è¯
        tokens = self.sp_model.encode(text, out_type=str)

        return tokens

    def _convert_token_to_id(self, token):
        """
        å°†tokenè½¬æ¢ä¸ºID
        """
        return self.sp_model.piece_to_id(token)

    def _convert_id_to_token(self, index):
        """
        å°†IDè½¬æ¢ä¸ºtoken
        """
        return self.sp_model.id_to_piece(index)
```

---

## ğŸ§¹ é¢„å¤„ç†æŠ€æœ¯è¯¦è§£

### ğŸ“ BasicTokenizerå®ç°

```python
# models/bert/tokenization_bert.py:200-300
class BasicTokenizer:
    """
    åŸºç¡€åˆ†è¯å™¨ï¼Œè´Ÿè´£æ–‡æœ¬é¢„å¤„ç†
    """
    def __init__(
        self,
        do_lower_case=True,
        never_split=None,
        tokenize_chinese_chars=True,
        strip_accents=None,
    ):
        self.do_lower_case = do_lower_case
        self.never_split = never_split if never_split is not None else []
        self.tokenize_chinese_chars = tokenize_chinese_chars
        self.strip_accents = strip_accents

    def tokenize(self, text):
        """
        åŸºç¡€åˆ†è¯å®ç°
        """
        # 1. æ¸…ç†æ–‡æœ¬
        text = self._clean_text(text)

        # 2. å¤„ç†é‡éŸ³ç¬¦å·
        if self.strip_accents is not False:
            text = self._run_strip_accents(text)

        # 3. è½¬æ¢ä¸ºå°å†™
        if self.do_lower_case:
            text = text.lower()

        # 4. ä¸­æ–‡åˆ†è¯
        if self.tokenize_chinese_chars:
            text = self._tokenize_chinese_chars(text)

        # 5. ç©ºæ ¼åˆ†è¯
        tokens = whitespace_tokenize(text)

        return tokens

    def _clean_text(self, text):
        """
        æ¸…ç†æ–‡æœ¬ä¸­çš„æ— æ•ˆå­—ç¬¦
        """
        output = []
        for char in text:
            cp = ord(char)
            # æ§åˆ¶å­—ç¬¦å’Œæ— æ•ˆå­—ç¬¦å¤„ç†
            if cp == 0 or cp == 0xFFFD or _is_control(char):
                continue
            # ç©ºæ ¼å­—ç¬¦å¤„ç†
            if _is_whitespace(char):
                output.append(" ")
            else:
                output.append(char)
        return "".join(output)

    def _tokenize_chinese_chars(self, text):
        """
        ä¸­æ–‡åˆ†è¯
        """
        output = []
        for char in text:
            cp = ord(char)
            # åˆ¤æ–­æ˜¯å¦ä¸ºä¸­æ–‡å­—ç¬¦
            if (
                (cp >= 0x4E00 and cp <= 0x9FFF)
                or (cp >= 0x3400 and cp <= 0x4DBF)
                or (cp >= 0x20000 and cp <= 0x2A6DF)
                or (cp >= 0x2A700 and cp <= 0x2B73F)
                or (cp >= 0x2B740 and cp <= 0x2B81F)
                or (cp >= 0x2B820 and cp <= 0x2CEAF)
                or (cp >= 0xF900 and cp <= 0xFAFF)
                or (cp >= 0x2F800 and cp <= 0x2FA1F)
            ):
                output.append(" ")
                output.append(char)
                output.append(" ")
            else:
                output.append(char)
        return "".join(output)

    def _run_strip_accents(self, text):
        """
        ç§»é™¤é‡éŸ³ç¬¦å·
        """
        text = unicodedata.normalize("NFD", text)
        output = []
        for char in text:
            cat = unicodedata.category(char)
            if cat == "Mn":
                continue
            output.append(char)
        return "".join(output)
```

### ğŸ“ æ–‡æœ¬æ ‡å‡†åŒ–

```python
# tokenization_utils.py:500-600
def normalize_text(text, do_lower_case=True, strip_accents=True):
    """
    æ–‡æœ¬æ ‡å‡†åŒ–
    """
    # 1. Unicodeæ ‡å‡†åŒ–
    text = unicodedata.normalize("NFC", text)

    # 2. å¤„ç†é‡éŸ³ç¬¦å·
    if strip_accents:
        text = "".join(
            c for c in unicodedata.normalize("NFD", text)
            if unicodedata.category(c) != "Mn"
        )

    # 3. å¤§å°å†™è½¬æ¢
    if do_lower_case:
        text = text.lower()

    # 4. æ ‡å‡†åŒ–ç©ºç™½å­—ç¬¦
    text = re.sub(r"\s+", " ", text)
    text = text.strip()

    return text

def clean_text(text):
    """
    æ¸…ç†æ–‡æœ¬ä¸­çš„ç‰¹æ®Šå­—ç¬¦
    """
    # ç§»é™¤æ§åˆ¶å­—ç¬¦
    text = "".join(char for char in text if ord(char) != 0xFFFD)

    # æ ‡å‡†åŒ–è¿å­—ç¬¦
    text = re.sub(r"[-â€“â€”]", "-", text)

    # æ ‡å‡†åŒ–å¼•å·
    text = re.sub(r"[""''â€â€œâ€]", '"', text)

    # æ ‡å‡†åŒ–çœç•¥å·
    text = re.sub(r"â€¦", "...", text)

    return text
```

---

## ğŸ“š è¯æ±‡è¡¨ç®¡ç†æœºåˆ¶

### ğŸ“ è¯æ±‡è¡¨åŠ è½½ä¸ä¿å­˜

```python
# models/bert/tokenization_bert.py:30-40
def load_vocab(vocab_file):
    """
    åŠ è½½è¯æ±‡è¡¨æ–‡ä»¶
    """
    vocab = collections.OrderedDict()
    with open(vocab_file, "r", encoding="utf-8") as reader:
        tokens = reader.readlines()
    for index, token in enumerate(tokens):
        token = token.rstrip("\n")
        vocab[token] = index
    return vocab

def save_vocab(vocab, vocab_file):
    """
    ä¿å­˜è¯æ±‡è¡¨æ–‡ä»¶
    """
    with open(vocab_file, "w", encoding="utf-8") as writer:
        for token, index in sorted(vocab.items(), key=lambda x: x[1]):
            writer.write(token + "\n")
```

### ğŸ“ è¯æ±‡è¡¨æŸ¥è¯¢ä¼˜åŒ–

```python
# tokenization_utils.py:1000-1100
class VocabLookup:
    """
    ä¼˜åŒ–çš„è¯æ±‡è¡¨æŸ¥è¯¢ç±»
    """
    def __init__(self, vocab_file):
        self.vocab = self._load_vocab(vocab_file)
        self.reverse_vocab = {v: k for k, v in self.vocab.items()}

        # æ„å»ºTrieåŠ é€Ÿå‰ç¼€åŒ¹é…
        self.trie = self._build_trie(self.vocab.keys())

    def _load_vocab(self, vocab_file):
        """
        åŠ è½½è¯æ±‡è¡¨
        """
        vocab = {}
        with open(vocab_file, "r", encoding="utf-8") as f:
            for idx, line in enumerate(f):
                token = line.strip()
                vocab[token] = idx
        return vocab

    def _build_trie(self, tokens):
        """
        æ„å»ºTrieæ ‘
        """
        trie = {}
        for token in tokens:
            node = trie
            for char in token:
                if char not in node:
                    node[char] = {}
                node = node[char]
            node["__END__"] = True
        return trie

    def lookup(self, token):
        """
        æŸ¥æ‰¾tokençš„ID
        """
        return self.vocab.get(token, self.vocab.get("[UNK]"))

    def reverse_lookup(self, token_id):
        """
        æ ¹æ®IDæŸ¥æ‰¾token
        """
        return self.reverse_vocab.get(token_id, "[UNK]")

    def find_prefix_matches(self, prefix):
        """
        æŸ¥æ‰¾æ‰€æœ‰å‰ç¼€åŒ¹é…çš„token
        """
        matches = []
        node = self.trie

        # éå†å‰ç¼€
        for char in prefix:
            if char not in node:
                return matches
            node = node[char]

        # æ”¶é›†æ‰€æœ‰åŒ¹é…çš„token
        self._collect_tokens(node, prefix, matches)
        return matches

    def _collect_tokens(self, node, current_token, matches):
        """
        æ”¶é›†Trieä¸­çš„æ‰€æœ‰token
        """
        if "__END__" in node:
            matches.append(current_token)

        for char, child_node in node.items():
            if char != "__END__":
                self._collect_tokens(child_node, current_token + char, matches)
```

---

## ğŸ¯ ç‰¹æ®Štokenå¤„ç†ç­–ç•¥

### ğŸ“ ç‰¹æ®Štokenå®šä¹‰

```python
# tokenization_utils_base.py:100-200
class SpecialTokensMixin:
    """
    ç‰¹æ®Štokenå¤„ç†æ··å…¥ç±»
    """
    def __init__(self, **kwargs):
        # ç‰¹æ®Štokenæ˜ å°„
        self.special_tokens_map = {
            "unk_token": kwargs.get("unk_token", "[UNK]"),
            "sep_token": kwargs.get("sep_token", "[SEP]"),
            "pad_token": kwargs.get("pad_token", "[PAD]"),
            "cls_token": kwargs.get("cls_token", "[CLS]"),
            "mask_token": kwargs.get("mask_token", "[MASK]"),
            "bos_token": kwargs.get("bos_token", "<s>"),
            "eos_token": kwargs.get("eos_token", "</s>"),
        }

        # ç‰¹æ®Štoken IDæ˜ å°„
        self.special_tokens_map_reverse = {
            v: k for k, v in self.special_tokens_map.items()
        }

    @property
    def unk_token(self):
        return self.special_tokens_map["unk_token"]

    @property
    def sep_token(self):
        return self.special_tokens_map["sep_token"]

    @property
    def pad_token(self):
        return self.special_tokens_map["pad_token"]

    @property
    def cls_token(self):
        return self.special_tokens_map["cls_token"]

    @property
    def mask_token(self):
        return self.special_tokens_map["mask_token"]

    def add_special_tokens(self, special_tokens_dict):
        """
        æ·»åŠ ç‰¹æ®Štoken
        """
        for token, token_id in special_tokens_dict.items():
            self.special_tokens_map[token] = token_id
            self.special_tokens_map_reverse[token_id] = token
```

### ğŸ“ ç‰¹æ®Štokenæ·»åŠ é€»è¾‘

```python
# tokenization_utils.py:1200-1300
def add_special_tokens(self, token_ids, special_tokens_mask=None):
    """
    æ·»åŠ ç‰¹æ®Štoken
    """
    if special_tokens_mask is None:
        special_tokens_mask = self.get_special_tokens_mask(token_ids)

    # æ·»åŠ CLS token
    if self.cls_token is not None:
        token_ids.insert(0, self.cls_token_id)
        special_tokens_mask.insert(0, 1)

    # æ·»åŠ SEP token
    if self.sep_token is not None:
        token_ids.append(self.sep_token_id)
        special_tokens_mask.append(1)

    return token_ids, special_tokens_mask

def get_special_tokens_mask(self, token_ids, already_has_special_tokens=False):
    """
    è·å–ç‰¹æ®Štokençš„mask
    """
    if already_has_special_tokens:
        return [1 if token in self.special_tokens_map_reverse else 0 for token in token_ids]

    return [0] * len(token_ids)

def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):
    """
    æ„å»ºå¸¦æœ‰ç‰¹æ®Štokençš„è¾“å…¥
    """
    if token_ids_1 is None:
        # å•ä¸ªåºåˆ—
        return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]
    else:
        # åºåˆ—å¯¹
        return (
            [self.cls_token_id]
            + token_ids_0
            + [self.sep_token_id]
            + token_ids_1
            + [self.sep_token_id]
        )
```

---

## âš¡ å¿«é€Ÿåˆ†è¯å™¨å®ç°

### ğŸ“ Rustå®ç°çš„å¿«é€Ÿåˆ†è¯å™¨

```python
# tokenization_utils_fast.py:100-200
class PreTrainedTokenizerFast(PreTrainedTokenizerBase):
    """
    åŸºäºRustçš„å¿«é€Ÿåˆ†è¯å™¨
    """
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        # å¯¼å…¥Rustå®ç°çš„åˆ†è¯å™¨
        try:
            from tokenizers import Tokenizer
            self._tokenizer = Tokenizer.from_file(kwargs["tokenizer_file"])
        except ImportError:
            raise ImportError(
                "Tokenizers library is required to use fast tokenizers. "
                "Please install it with `pip install tokenizers`."
            )

        # è®¾ç½®ç‰¹æ®Štoken
        self._set_special_tokens()

    def _set_special_tokens(self):
        """
        è®¾ç½®ç‰¹æ®Štoken
        """
        # æ·»åŠ ç‰¹æ®Štokenåˆ°Ruståˆ†è¯å™¨
        if self.unk_token is not None:
            self._tokenizer.add_special_tokens([self.unk_token])
        if self.sep_token is not None:
            self._tokenizer.add_special_tokens([self.sep_token])
        if self.pad_token is not None:
            self._tokenizer.add_special_tokens([self.pad_token])
        if self.cls_token is not None:
            self._tokenizer.add_special_tokens([self.cls_token])
        if self.mask_token is not None:
            self._tokenizer.add_special_tokens([self.mask_token])

    def tokenize(self, text, **kwargs):
        """
        å¿«é€Ÿåˆ†è¯å®ç°
        """
        # ä½¿ç”¨Rustå®ç°çš„é«˜æ€§èƒ½åˆ†è¯
        encoding = self._tokenizer.encode(text)
        return encoding.tokens

    def convert_tokens_to_ids(self, tokens):
        """
        è½¬æ¢tokenä¸ºID
        """
        return self._tokenizer.convert_tokens_to_ids(tokens)

    def convert_ids_to_tokens(self, ids):
        """
        è½¬æ¢IDä¸ºtoken
        """
        return self._tokenizer.convert_ids_to_tokens(ids)

    def encode(self, text, **kwargs):
        """
        ç¼–ç æ–‡æœ¬
        """
        return self._tokenizer.encode(text, **kwargs)

    def decode(self, token_ids, **kwargs):
        """
        è§£ç tokenåºåˆ—
        """
        return self._tokenizer.decode(token_ids, **kwargs)

    def batch_encode_plus(self, batch_text_or_text_pairs, **kwargs):
        """
        æ‰¹é‡ç¼–ç 
        """
        return self._tokenizer.encode_batch(batch_text_or_text_pairs, **kwargs)

    def batch_decode(self, batch_token_ids, **kwargs):
        """
        æ‰¹é‡è§£ç 
        """
        return self._tokenizer.decode_batch(batch_token_ids, **kwargs)
```

### ğŸ“ æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯

```python
# tokenization_utils_fast.py:300-400
class OptimizedTokenizer(PreTrainedTokenizerFast):
    """
    ä¼˜åŒ–çš„å¿«é€Ÿåˆ†è¯å™¨
    """
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        # ç¼“å­˜æœºåˆ¶
        self._encode_cache = {}
        self._decode_cache = {}

        # å¹¶è¡Œå¤„ç†
        self._max_workers = kwargs.get("max_workers", 4)

    def encode_with_cache(self, text):
        """
        å¸¦ç¼“å­˜çš„ç¼–ç 
        """
        cache_key = hash(text)
        if cache_key in self._encode_cache:
            return self._encode_cache[cache_key]

        # æ‰§è¡Œç¼–ç 
        result = self._tokenizer.encode(text)

        # ç¼“å­˜ç»“æœ
        self._encode_cache[cache_key] = result

        # é™åˆ¶ç¼“å­˜å¤§å°
        if len(self._encode_cache) > 10000:
            # éšæœºåˆ é™¤ä¸€éƒ¨åˆ†ç¼“å­˜
            keys_to_remove = list(self._encode_cache.keys())[:1000]
            for key in keys_to_remove:
                del self._encode_cache[key]

        return result

    def batch_encode_parallel(self, texts, **kwargs):
        """
        å¹¶è¡Œæ‰¹é‡ç¼–ç 
        """
        from concurrent.futures import ThreadPoolExecutor

        with ThreadPoolExecutor(max_workers=self._max_workers) as executor:
            results = list(executor.map(
                lambda text: self.encode_with_cache(text),
                texts
            ))

        return results

    def optimize_memory_usage(self):
        """
        ä¼˜åŒ–å†…å­˜ä½¿ç”¨
        """
        # æ¸…ç†ç¼“å­˜
        self._encode_cache.clear()
        self._decode_cache.clear()

        # è§¦å‘åƒåœ¾å›æ”¶
        import gc
        gc.collect()

        # é‡Šæ”¾Ruståˆ†è¯å™¨å†…å­˜
        if hasattr(self._tokenizer, "clear_cache"):
            self._tokenizer.clear_cache()
```

---

## ğŸŒ å¤šè¯­è¨€æ”¯æŒä¸Unicodeå¤„ç†

### ğŸ“ Unicodeæ ‡å‡†åŒ–

```python
# tokenization_utils.py:1500-1600
class UnicodeNormalizer:
    """
    Unicodeæ ‡å‡†åŒ–å¤„ç†å™¨
    """
    def __init__(self, normalization_form="NFC"):
        self.normalization_form = normalization_form

    def normalize(self, text):
        """
        Unicodeæ ‡å‡†åŒ–
        """
        return unicodedata.normalize(self.normalization_form, text)

    def normalize_whitespace(self, text):
        """
        æ ‡å‡†åŒ–ç©ºç™½å­—ç¬¦
        """
        # å°†æ‰€æœ‰ç©ºç™½å­—ç¬¦è½¬æ¢ä¸ºç©ºæ ¼
        text = re.sub(r"\s+", " ", text)

        # å»é™¤é¦–å°¾ç©ºç™½
        text = text.strip()

        return text

    def handle_combining_characters(self, text):
        """
        å¤„ç†ç»„åˆå­—ç¬¦
        """
        # åˆ†è§£ç»„åˆå­—ç¬¦
        text = unicodedata.normalize("NFD", text)

        # ç§»é™¤é‡éŸ³ç¬¦å·
        text = "".join(
            char for char in text
            if unicodedata.category(char) != "Mn"
        )

        # é‡æ–°ç»„åˆ
        text = unicodedata.normalize("NFC", text)

        return text

    def handle_emoji(self, text):
        """
        å¤„ç†emoji
        """
        # emojiæ ‡å‡†åŒ–
        emoji_pattern = re.compile(
            "["
            "\U0001F600-\U0001F64F"  # emoticons
            "\U0001F300-\U0001F5FF"  # symbols & pictographs
            "\U0001F680-\U0001F6FF"  # transport & map symbols
            "\U0001F1E0-\U0001F1FF"  # flags (iOS)
            "\U00002702-\U000027B0"
            "\U000024C2-\U0001F251"
            "]+",
            flags=re.UNICODE,
        )

        return emoji_pattern.sub(r" \g<0> ", text)
```

### ğŸ“ å¤šè¯­è¨€åˆ†è¯ç­–ç•¥

```python
# tokenization_utils.py:1700-1800
class MultilingualTokenizer:
    """
    å¤šè¯­è¨€åˆ†è¯å™¨
    """
    def __init__(self, language_specific_rules=None):
        self.language_specific_rules = language_specific_rules or {}
        self.language_detector = self._init_language_detector()

    def _init_language_detector(self):
        """
        åˆå§‹åŒ–è¯­è¨€æ£€æµ‹å™¨
        """
        try:
            from langdetect import detect
            return detect
        except ImportError:
            # ç®€å•çš„è¯­è¨€æ£€æµ‹
            return self._simple_language_detection

    def _simple_language_detection(self, text):
        """
        ç®€å•çš„è¯­è¨€æ£€æµ‹
        """
        # åŸºäºå­—ç¬¦é›†çš„è¯­è¨€æ£€æµ‹
        if re.search(r'[\u4e00-\u9fff]', text):
            return "zh"
        elif re.search(r'[\u3040-\u309f\u30a0-\u30ff]', text):
            return "ja"
        elif re.search(r'[\u0400-\u04ff]', text):
            return "ru"
        elif re.search(r'[\u0600-\u06ff]', text):
            return "ar"
        else:
            return "en"

    def tokenize_with_language_detection(self, text):
        """
        å¸¦è¯­è¨€æ£€æµ‹çš„åˆ†è¯
        """
        # æ£€æµ‹è¯­è¨€
        language = self.language_detector(text)

        # åº”ç”¨è¯­è¨€ç‰¹å®šè§„åˆ™
        if language in self.language_specific_rules:
            rules = self.language_specific_rules[language]
            for rule in rules:
                text = rule(text)

        # æ‰§è¡Œåˆ†è¯
        tokens = self._tokenize(text)

        return tokens

    def get_language_specific_rules(self, language):
        """
        è·å–è¯­è¨€ç‰¹å®šè§„åˆ™
        """
        rules = {
            "zh": [
                self._handle_chinese_punctuation,
                self._handle_chinese_numbers,
            ],
            "ja": [
                self._handle_japanese_punctuation,
                self._handle_japanese_hiragana_katakana,
            ],
            "ar": [
                self._handle_arabic_punctuation,
                self._handle_arabic_numbers,
            ],
            "ru": [
                self._handle_cyrillic_punctuation,
                self._handle_cyrillic_numbers,
            ],
        }

        return rules.get(language, [])
```

---

## ğŸ’¾ ç¼“å­˜æœºåˆ¶ä¸æ€§èƒ½ä¼˜åŒ–

### ğŸ“ åˆ†è¯ç»“æœç¼“å­˜

```python
# tokenization_utils.py:2000-2100
class TokenizationCache:
    """
    åˆ†è¯ç»“æœç¼“å­˜
    """
    def __init__(self, max_size=10000):
        self.max_size = max_size
        self.cache = {}
        self.access_times = {}
        self.lock = threading.Lock()

    def get(self, key):
        """
        è·å–ç¼“å­˜é¡¹
        """
        with self.lock:
            if key in self.cache:
                # æ›´æ–°è®¿é—®æ—¶é—´
                self.access_times[key] = time.time()
                return self.cache[key]
            return None

    def put(self, key, value):
        """
        å­˜å…¥ç¼“å­˜
        """
        with self.lock:
            # æ£€æŸ¥ç¼“å­˜å¤§å°
            if len(self.cache) >= self.max_size:
                self._evict_lru()

            # å­˜å…¥ç¼“å­˜
            self.cache[key] = value
            self.access_times[key] = time.time()

    def _evict_lru(self):
        """
        æ·˜æ±°æœ€è¿‘æœ€å°‘ä½¿ç”¨çš„é¡¹
        """
        if not self.access_times:
            return

        # æ‰¾åˆ°æœ€æ—§çš„é¡¹
        oldest_key = min(self.access_times, key=self.access_times.get)

        # åˆ é™¤æœ€æ—§çš„é¡¹
        del self.cache[oldest_key]
        del self.access_times[oldest_key]

    def clear(self):
        """
        æ¸…ç©ºç¼“å­˜
        """
        with self.lock:
            self.cache.clear()
            self.access_times.clear()

    def stats(self):
        """
        ç¼“å­˜ç»Ÿè®¡
        """
        return {
            "size": len(self.cache),
            "max_size": self.max_size,
            "hit_rate": self._calculate_hit_rate(),
        }

    def _calculate_hit_rate(self):
        """
        è®¡ç®—å‘½ä¸­ç‡
        """
        # è¿™é‡Œéœ€è¦å®ç°å‘½ä¸­ç‡è®¡ç®—é€»è¾‘
        return 0.0
```

### ğŸ“ æ‰¹é‡å¤„ç†ä¼˜åŒ–

```python
# tokenization_utils.py:2200-2300
class BatchTokenizer:
    """
    æ‰¹é‡åˆ†è¯å™¨
    """
    def __init__(self, base_tokenizer, batch_size=32):
        self.base_tokenizer = base_tokenizer
        self.batch_size = batch_size
        self.cache = TokenizationCache()

    def tokenize_batch(self, texts):
        """
        æ‰¹é‡åˆ†è¯
        """
        results = []

        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(texts), self.batch_size):
            batch = texts[i:i + self.batch_size]

            # æ£€æŸ¥ç¼“å­˜
            cached_results = []
            uncached_texts = []
            uncached_indices = []

            for j, text in enumerate(batch):
                cache_key = hash(text)
                cached_result = self.cache.get(cache_key)
                if cached_result is not None:
                    cached_results.append((j, cached_result))
                else:
                    uncached_texts.append(text)
                    uncached_indices.append(j)

            # å¤„ç†æœªç¼“å­˜çš„æ–‡æœ¬
            if uncached_texts:
                # å¹¶è¡Œå¤„ç†
                with ThreadPoolExecutor(max_workers=4) as executor:
                    uncached_results = list(executor.map(
                        self.base_tokenizer.tokenize,
                        uncached_texts
                    ))

                # ç¼“å­˜ç»“æœ
                for text, result in zip(uncached_texts, uncached_results):
                    cache_key = hash(text)
                    self.cache.put(cache_key, result)

                # åˆå¹¶ç»“æœ
                batch_results = [None] * len(batch)
                for idx, result in cached_results:
                    batch_results[idx] = result
                for idx, result in zip(uncached_indices, uncached_results):
                    batch_results[idx] = result
            else:
                batch_results = [result for _, result in cached_results]

            results.extend(batch_results)

        return results

    def encode_batch(self, texts, **kwargs):
        """
        æ‰¹é‡ç¼–ç 
        """
        # åˆ†æ‰¹å¤„ç†
        results = []
        for i in range(0, len(texts), self.batch_size):
            batch = texts[i:i + self.batch_size]

            # æ£€æŸ¥ç¼“å­˜
            cached_results = []
            uncached_texts = []
            uncached_indices = []

            for j, text in enumerate(batch):
                cache_key = hash(str(text) + str(kwargs))
                cached_result = self.cache.get(cache_key)
                if cached_result is not None:
                    cached_results.append((j, cached_result))
                else:
                    uncached_texts.append(text)
                    uncached_indices.append(j)

            # å¤„ç†æœªç¼“å­˜çš„æ–‡æœ¬
            if uncached_texts:
                uncached_results = self.base_tokenizer.batch_encode_plus(
                    uncached_texts, **kwargs
                )

                # ç¼“å­˜ç»“æœ
                for text, result in zip(uncached_texts, uncached_results):
                    cache_key = hash(str(text) + str(kwargs))
                    self.cache.put(cache_key, result)

                # åˆå¹¶ç»“æœ
                batch_results = [None] * len(batch)
                for idx, result in cached_results:
                    batch_results[idx] = result
                for idx, result in zip(uncached_indices, uncached_results):
                    batch_results[idx] = result
            else:
                batch_results = [result for _, result in cached_results]

            results.extend(batch_results)

        return results
```

---

## ğŸ’» å®æˆ˜ä»£ç ç¤ºä¾‹

### ğŸ¯ ç¤ºä¾‹1ï¼šä»é›¶å®ç°WordPieceåˆ†è¯å™¨

```python
import re
import collections
import math
from typing import List, Dict, Tuple, Optional

class WordPieceTrainer:
    """
    WordPieceè®­ç»ƒå™¨
    """
    def __init__(self, vocab_size: int = 30000, min_frequency: int = 2):
        self.vocab_size = vocab_size
        self.min_frequency = min_frequency
        self.vocab = set()
        self.token_to_id = {}
        self.id_to_token = {}

    def train(self, corpus: List[str]):
        """
        è®­ç»ƒWordPieceè¯æ±‡è¡¨
        """
        # 1. åˆå§‹åŒ–ï¼šæ‰€æœ‰å­—ç¬¦ä½œä¸ºåˆå§‹token
        all_chars = set()
        for text in corpus:
            for char in text:
                all_chars.add(char)

        self.vocab = all_chars.copy()
        self._update_vocab_mappings()

        # 2. ç»Ÿè®¡æ‰€æœ‰å¯èƒ½çš„å­è¯
        word_freqs = self._count_words(corpus)
        subword_freqs = self._count_subwords(corpus)

        # 3. è¿­ä»£æ·»åŠ æœ€æœ‰ä»·å€¼çš„å­è¯
        while len(self.vocab) < self.vocab_size:
            best_subword = self._find_best_subword(subword_freqs, word_freqs)

            if best_subword is None:
                break

            self.vocab.add(best_subword)
            self._update_vocab_mappings()

            # æ›´æ–°é¢‘ç‡ç»Ÿè®¡
            self._update_subword_frequencies(best_subword, subword_freqs, word_freqs)

        return self.vocab

    def _count_words(self, corpus: List[str]) -> Dict[str, int]:
        """
        ç»Ÿè®¡è¯é¢‘ç‡
        """
        word_freqs = collections.defaultdict(int)
        for text in corpus:
            words = text.split()
            for word in words:
                word_freqs[word] += 1
        return word_freqs

    def _count_subwords(self, corpus: List[str]) -> Dict[str, int]:
        """
        ç»Ÿè®¡å­è¯é¢‘ç‡
        """
        subword_freqs = collections.defaultdict(int)

        for text in corpus:
            words = text.split()
            for word in words:
                # ç»Ÿè®¡æ‰€æœ‰å¯èƒ½çš„å­è¯
                for i in range(len(word)):
                    for j in range(i + 1, len(word) + 1):
                        subword = word[i:j]
                        subword_freqs[subword] += 1

        return subword_freqs

    def _find_best_subword(self, subword_freqs: Dict[str, int], word_freqs: Dict[str, int]) -> Optional[str]:
        """
        æ‰¾åˆ°æœ€æœ‰ä»·å€¼çš„å­è¯
        """
        best_score = -float('inf')
        best_subword = None

        for subword in subword_freqs:
            if subword in self.vocab:
                continue

            if subword_freqs[subword] < self.min_frequency:
                continue

            # è®¡ç®—åˆ†æ•°
            score = self._calculate_subword_score(subword, subword_freqs, word_freqs)

            if score > best_score:
                best_score = score
                best_subword = subword

        return best_subword

    def _calculate_subword_score(self, subword: str, subword_freqs: Dict[str, int], word_freqs: Dict[str, int]) -> float:
        """
        è®¡ç®—å­è¯åˆ†æ•°
        """
        # WordPieceåˆ†æ•°è®¡ç®—
        freq = subword_freqs[subword]

        # è®¡ç®—ç»„æˆè¯¥å­è¯çš„å­—ç¬¦é¢‘ç‡
        if len(subword) == 1:
            return freq

        char_freqs = []
        for char in subword:
            if char in subword_freqs:
                char_freqs.append(subword_freqs[char])
            else:
                char_freqs.append(1)

        # é¿å…é™¤é›¶é”™è¯¯
        product = 1
        for f in char_freqs:
            product *= f

        if product == 0:
            return freq

        # WordPieceåˆ†æ•°å…¬å¼
        score = freq / math.sqrt(product)

        return score

    def _update_subword_frequencies(self, new_subword: str, subword_freqs: Dict[str, int], word_freqs: Dict[str, int]):
        """
        æ›´æ–°å­è¯é¢‘ç‡ç»Ÿè®¡
        """
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„å®ç°ï¼Œå®é™…åº”ç”¨ä¸­éœ€è¦æ›´å¤æ‚çš„æ›´æ–°é€»è¾‘
        pass

    def _update_vocab_mappings(self):
        """
        æ›´æ–°è¯æ±‡è¡¨æ˜ å°„
        """
        self.token_to_id = {token: idx for idx, token in enumerate(sorted(self.vocab))}
        self.id_to_token = {idx: token for token, idx in self.token_to_id.items()}

class WordPieceTokenizer:
    """
    WordPieceåˆ†è¯å™¨
    """
    def __init__(self, vocab: Dict[str, int], unk_token: str = "[UNK]"):
        self.vocab = vocab
        self.unk_token = unk_token
        self.unk_token_id = vocab.get(unk_token, 0)

        # æ„å»ºå‰ç¼€æ ‘
        self.trie = self._build_trie(vocab.keys())

    def _build_trie(self, tokens: List[str]) -> Dict:
        """
        æ„å»ºå‰ç¼€æ ‘
        """
        trie = {}
        for token in tokens:
            node = trie
            for char in token:
                if char not in node:
                    node[char] = {}
                node = node[char]
            node["__END__"] = True
        return trie

    def tokenize(self, text: str) -> List[str]:
        """
        åˆ†è¯
        """
        words = text.split()
        tokens = []

        for word in words:
            word_tokens = self._tokenize_word(word)
            tokens.extend(word_tokens)

        return tokens

    def _tokenize_word(self, word: str) -> List[str]:
        """
        åˆ†è¯å•ä¸ªå•è¯
        """
        if len(word) == 0:
            return []

        tokens = []
        start = 0

        while start < len(word):
            # ä»åå‘å‰æŸ¥æ‰¾æœ€é•¿åŒ¹é…
            end = len(word)
            current_token = None

            while start < end:
                substring = word[start:end]
                if start > 0:
                    substring = "##" + substring

                if substring in self.vocab:
                    current_token = substring
                    break

                end -= 1

            if current_token is None:
                # æ— æ³•åˆ†è¯
                return [self.unk_token]

            tokens.append(current_token)
            start = end

        return tokens

    def encode(self, text: str) -> List[int]:
        """
        ç¼–ç ä¸ºIDåºåˆ—
        """
        tokens = self.tokenize(text)
        return [self.vocab.get(token, self.unk_token_id) for token in tokens]

    def decode(self, token_ids: List[int]) -> str:
        """
        è§£ç ä¸ºæ–‡æœ¬
        """
        tokens = [self.id_to_token.get(id, self.unk_token) for id in token_ids]

        # ç§»é™¤##å‰ç¼€
        text = ""
        for token in tokens:
            if token.startswith("##"):
                text += token[2:]
            else:
                text += " " + token

        return text.strip()

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # è®­ç»ƒæ•°æ®
    corpus = [
        "hello world",
        "hello there",
        "world peace",
        "peace in the world",
        "hello beautiful world",
        "the quick brown fox",
        "fox jumps over the lazy dog",
        "machine learning is amazing",
        "deep learning models",
        "natural language processing"
    ]

    # è®­ç»ƒWordPieceæ¨¡å‹
    trainer = WordPieceTrainer(vocab_size=50, min_frequency=1)
    vocab = trainer.train(corpus)

    # åˆ›å»ºåˆ†è¯å™¨
    tokenizer = WordPieceTokenizer(vocab)

    # æµ‹è¯•åˆ†è¯
    text = "hello world of machine learning"
    tokens = tokenizer.tokenize(text)
    token_ids = tokenizer.encode(text)
    decoded = tokenizer.decode(token_ids)

    print(f"Text: {text}")
    print(f"Tokens: {tokens}")
    print(f"Token IDs: {token_ids}")
    print(f"Decoded: {decoded}")
```

### ğŸ¯ ç¤ºä¾‹2ï¼šBPEåˆ†è¯å™¨æ€§èƒ½å¯¹æ¯”

```python
import time
import re
import collections
from typing import List, Dict, Tuple
import matplotlib.pyplot as plt

class BPETokenizer:
    """
    BPEåˆ†è¯å™¨å®ç°
    """
    def __init__(self, vocab_size: int = 1000):
        self.vocab_size = vocab_size
        self.vocab = set()
        self.merges = []

    def train(self, corpus: List[str]):
        """
        è®­ç»ƒBPEæ¨¡å‹
        """
        # 1. åˆå§‹åŒ–ï¼šæ¯ä¸ªå­—ç¬¦ä½œä¸ºtoken
        vocab = set()
        for text in corpus:
            for char in text:
                vocab.add(char)

        # 2. ç»Ÿè®¡å­—ç¬¦é¢‘ç‡
        char_freq = collections.defaultdict(int)
        for text in corpus:
            for char in text:
                char_freq[char] += 1

        # 3. è½¬æ¢è¯­æ–™ä¸ºå­—ç¬¦åºåˆ—
        corpus_chars = [list(text) for text in corpus]

        # 4. è¿­ä»£åˆå¹¶
        while len(vocab) < self.vocab_size:
            # ç»Ÿè®¡ç›¸é‚»å¯¹é¢‘ç‡
            pair_freq = collections.defaultdict(int)
            for char_seq in corpus_chars:
                for i in range(len(char_seq) - 1):
                    pair = (char_seq[i], char_seq[i + 1])
                    pair_freq[pair] += 1

            if not pair_freq:
                break

            # é€‰æ‹©æœ€é¢‘ç¹çš„å¯¹
            best_pair = max(pair_freq, key=pair_freq.get)
            new_token = best_pair[0] + best_pair[1]

            # è®°å½•åˆå¹¶
            self.merges.append(best_pair)
            vocab.add(new_token)

            # æ›´æ–°è¯­æ–™
            for char_seq in corpus_chars:
                i = 0
                while i < len(char_seq) - 1:
                    if (char_seq[i], char_seq[i + 1]) == best_pair:
                        char_seq[i:i+2] = [new_token]
                    else:
                        i += 1

        self.vocab = vocab
        return vocab

    def tokenize(self, text: str) -> List[str]:
        """
        åˆ†è¯
        """
        # è½¬æ¢ä¸ºå­—ç¬¦åºåˆ—
        chars = list(text)

        # åº”ç”¨åˆå¹¶è§„åˆ™
        for merge in self.merges:
            i = 0
            while i < len(chars) - 1:
                if (chars[i], chars[i + 1]) == merge:
                    chars[i:i+2] = [merge[0] + merge[1]]
                else:
                    i += 1

        return chars

class FastBPETokenizer:
    """
    å¿«é€ŸBPEåˆ†è¯å™¨ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
    """
    def __init__(self, vocab_size: int = 1000):
        self.vocab_size = vocab_size
        self.vocab = set()
        self.merges = []
        self.cache = {}

    def train(self, corpus: List[str]):
        """
        è®­ç»ƒBPEæ¨¡å‹
        """
        # ä¸BPETokenizerç›¸åŒçš„è®­ç»ƒé€»è¾‘
        base_tokenizer = BPETokenizer(self.vocab_size)
        self.vocab = base_tokenizer.train(corpus)
        self.merges = base_tokenizer.merges

    def tokenize(self, text: str) -> List[str]:
        """
        å¸¦ç¼“å­˜çš„åˆ†è¯
        """
        # æ£€æŸ¥ç¼“å­˜
        cache_key = hash(text)
        if cache_key in self.cache:
            return self.cache[cache_key]

        # æ‰§è¡Œåˆ†è¯
        chars = list(text)

        for merge in self.merges:
            i = 0
            while i < len(chars) - 1:
                if (chars[i], chars[i + 1]) == merge:
                    chars[i:i+2] = [merge[0] + merge[1]]
                else:
                    i += 1

        # ç¼“å­˜ç»“æœ
        result = chars
        self.cache[cache_key] = result

        # é™åˆ¶ç¼“å­˜å¤§å°
        if len(self.cache) > 1000:
            # åˆ é™¤æœ€æ—§çš„ç¼“å­˜é¡¹
            oldest_key = next(iter(self.cache))
            del self.cache[oldest_key]

        return result

def benchmark_tokenizers(texts: List[str], num_runs: int = 100):
    """
    åˆ†è¯å™¨æ€§èƒ½åŸºå‡†æµ‹è¯•
    """
    # è®­ç»ƒåˆ†è¯å™¨
    corpus = [
        "hello world this is a test",
        "machine learning is fascinating",
        "natural language processing involves text",
        "deep learning models are powerful",
        "artificial intelligence is the future"
    ] * 100  # æ‰©å¤§è¯­æ–™

    # åˆ›å»ºåˆ†è¯å™¨
    bpe_tokenizer = BPETokenizer(vocab_size=100)
    fast_bpe_tokenizer = FastBPETokenizer(vocab_size=100)

    bpe_tokenizer.train(corpus)
    fast_bpe_tokenizer.train(corpus)

    # æµ‹è¯•æ€§èƒ½
    results = {}

    # æ ‡å‡†BPE
    start_time = time.time()
    for _ in range(num_runs):
        for text in texts:
            bpe_tokenizer.tokenize(text)
    bpe_time = time.time() - start_time
    results["BPE"] = bpe_time

    # å¿«é€ŸBPE
    start_time = time.time()
    for _ in range(num_runs):
        for text in texts:
            fast_bpe_tokenizer.tokenize(text)
    fast_bpe_time = time.time() - start_time
    results["Fast BPE"] = fast_bpe_time

    # å†…å­˜ä½¿ç”¨
    import sys
    bpe_memory = sys.getsizeof(bpe_tokenizer)
    fast_bpe_memory = sys.getsizeof(fast_bpe_tokenizer) + sys.getsizeof(fast_bpe_tokenizer.cache)

    print(f"æ€§èƒ½æµ‹è¯•ç»“æœ ({num_runs} runs):")
    print(f"æ ‡å‡†BPE: {bpe_time:.4f}s")
    print(f"å¿«é€ŸBPE: {fast_bpe_time:.4f}s")
    print(f"åŠ é€Ÿæ¯”: {bpe_time/fast_bpe_time:.2f}x")
    print(f"å†…å­˜ä½¿ç”¨:")
    print(f"æ ‡å‡†BPE: {bpe_memory} bytes")
    print(f"å¿«é€ŸBPE: {fast_bpe_memory} bytes")

    # å¯è§†åŒ–
    plt.figure(figsize=(12, 8))

    # æ€§èƒ½å¯¹æ¯”
    plt.subplot(2, 2, 1)
    plt.bar(results.keys(), results.values())
    plt.title('Tokenization Performance')
    plt.ylabel('Time (seconds)')

    # åŠ é€Ÿæ¯”
    plt.subplot(2, 2, 2)
    speedup = [1, bpe_time/fast_bpe_time]
    plt.bar(['BPE', 'Fast BPE'], speedup)
    plt.title('Speedup Ratio')
    plt.ylabel('Speedup (x)')

    # å†…å­˜ä½¿ç”¨
    plt.subplot(2, 2, 3)
    memory_data = [bpe_memory, fast_bpe_memory]
    plt.bar(['BPE', 'Fast BPE'], memory_data)
    plt.title('Memory Usage')
    plt.ylabel('Memory (bytes)')

    # è¯è¡¨å¤§å°å¢é•¿
    plt.subplot(2, 2, 4)
    vocab_sizes = []
    for i in range(0, len(bpe_tokenizer.merges), 10):
        vocab_sizes.append(len(set(corpus[0])) + i)
    plt.plot(vocab_sizes, marker='o')
    plt.title('Vocabulary Size Growth')
    plt.xlabel('Merge Steps')
    plt.ylabel('Vocabulary Size')

    plt.tight_layout()
    plt.savefig('bpe_tokenizer_benchmark.png', dpi=300, bbox_inches='tight')
    plt.show()

    return results

# è¿è¡ŒåŸºå‡†æµ‹è¯•
if __name__ == "__main__":
    test_texts = [
        "hello world",
        "machine learning and artificial intelligence",
        "natural language processing is a subfield of AI",
        "deep learning models require large amounts of data",
        "transformer architectures have revolutionized NLP"
    ] * 20

    benchmark_tokenizers(test_texts, num_runs=50)
```

### ğŸ¯ ç¤ºä¾‹3ï¼šå¤šè¯­è¨€åˆ†è¯ç³»ç»Ÿ

```python
import re
import unicodedata
from typing import List, Dict, Tuple, Optional
import langdetect

class MultilingualTokenizer:
    """
    å¤šè¯­è¨€åˆ†è¯å™¨
    """
    def __init__(self):
        self.language_detectors = {
            'zh': self._is_chinese,
            'ja': self._is_japanese,
            'ko': self._is_korean,
            'ar': self._is_arabic,
            'ru': self._is_cyrillic,
            'th': self._is_thai,
            'hi': self._is_hindi,
        }

        self.language_rules = {
            'zh': self._apply_chinese_rules,
            'ja': self._apply_japanese_rules,
            'ko': self._apply_korean_rules,
            'ar': self._apply_arabic_rules,
            'ru': self._apply_cyrillic_rules,
            'th': self._apply_thai_rules,
            'hi': self._apply_hindi_rules,
        }

    def detect_language(self, text: str) -> str:
        """
        æ£€æµ‹æ–‡æœ¬è¯­è¨€
        """
        # å°è¯•ä½¿ç”¨langdetect
        try:
            detected = langdetect.detect(text)
            if detected in ['zh-cn', 'zh-tw']:
                return 'zh'
            return detected
        except:
            pass

        # ä½¿ç”¨å¯å‘å¼è§„åˆ™
        for lang, detector in self.language_detectors.items():
            if detector(text):
                return lang

        return 'en'  # é»˜è®¤è‹±è¯­

    def tokenize(self, text: str) -> List[str]:
        """
        å¤šè¯­è¨€åˆ†è¯
        """
        # æ£€æµ‹è¯­è¨€
        lang = self.detect_language(text)

        # åº”ç”¨è¯­è¨€ç‰¹å®šè§„åˆ™
        if lang in self.language_rules:
            text = self.language_rules[lang](text)

        # æ‰§è¡Œåˆ†è¯
        tokens = self._basic_tokenize(text, lang)

        return tokens

    def _is_chinese(self, text: str) -> bool:
        """æ£€æµ‹ä¸­æ–‡"""
        chinese_chars = re.search(r'[\u4e00-\u9fff]', text)
        return chinese_chars is not None

    def _is_japanese(self, text: str) -> bool:
        """æ£€æµ‹æ—¥è¯­"""
        japanese_chars = re.search(r'[\u3040-\u309f\u30a0-\u30ff]', text)
        return japanese_chars is not None

    def _is_korean(self, text: str) -> bool:
        """æ£€æµ‹éŸ©è¯­"""
        korean_chars = re.search(r'[\uac00-\ud7af]', text)
        return korean_chars is not None

    def _is_arabic(self, text: str) -> bool:
        """æ£€æµ‹é˜¿æ‹‰ä¼¯è¯­"""
        arabic_chars = re.search(r'[\u0600-\u06ff]', text)
        return arabic_chars is not None

    def _is_cyrillic(self, text: str) -> bool:
        """æ£€æµ‹è¥¿é‡Œå°”å­—æ¯"""
        cyrillic_chars = re.search(r'[\u0400-\u04ff]', text)
        return cyrillic_chars is not None

    def _is_thai(self, text: str) -> bool:
        """æ£€æµ‹æ³°è¯­"""
        thai_chars = re.search(r'[\u0e00-\u0e7f]', text)
        return thai_chars is not None

    def _is_hindi(self, text: str) -> bool:
        """æ£€æµ‹å°åœ°è¯­"""
        hindi_chars = re.search(r'[\u0900-\u097f]', text)
        return hindi_chars is not None

    def _apply_chinese_rules(self, text: str) -> str:
        """åº”ç”¨ä¸­æ–‡å¤„ç†è§„åˆ™"""
        # åœ¨ä¸­æ–‡å­—ç¬¦å‘¨å›´æ·»åŠ ç©ºæ ¼
        text = re.sub(r'([\u4e00-\u9fff])', r' \1 ', text)
        # æ ‡å‡†åŒ–ä¸­æ–‡æ ‡ç‚¹
        text = re.sub(r'ï¼Œ', ',', text)
        text = re.sub(r'ã€‚', '.', text)
        text = re.sub(r'ã€', ',', text)
        return text

    def _apply_japanese_rules(self, text: str) -> str:
        """åº”ç”¨æ—¥è¯­å¤„ç†è§„åˆ™"""
        # å¤„ç†æ—¥è¯­æ ‡ç‚¹
        text = re.sub(r'ã€', ',', text)
        text = re.sub(r'ã€‚', '.', text)
        return text

    def _apply_arabic_rules(self, text: str) -> str:
        """åº”ç”¨é˜¿æ‹‰ä¼¯è¯­å¤„ç†è§„åˆ™"""
        # å¤„ç†é˜¿æ‹‰ä¼¯è¯­è¿æ¥ç¬¦
        text = re.sub(r'\u0640', '', text)  # ç§»é™¤Tatweel
        return text

    def _apply_cyrillic_rules(self, text: str) -> str:
        """åº”ç”¨è¥¿é‡Œå°”å­—æ¯è§„åˆ™"""
        # æ ‡å‡†åŒ–è¥¿é‡Œå°”æ ‡ç‚¹
        text = re.sub(r'ï¼Œ', ',', text)
        return text

    def _apply_thai_rules(self, text: str) -> str:
        """åº”ç”¨æ³°è¯­å¤„ç†è§„åˆ™"""
        # æ³°è¯­ä¸éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œä½†å¯ä»¥æ·»åŠ ç©ºæ ¼
        return text

    def _apply_hindi_rules(self, text: str) -> str:
        """åº”ç”¨å°åœ°è¯­å¤„ç†è§„åˆ™"""
        # å¤„ç†å°åœ°è¯­æ•°å­—
        text = re.sub(r'à¥¦', '0', text)
        text = re.sub(r'à¥§', '1', text)
        text = re.sub(r'à¥¨', '2', text)
        text = re.sub(r'à¥©', '3', text)
        text = re.sub(r'à¥ª', '4', text)
        text = re.sub(r'à¥«', '5', text)
        text = re.sub(r'à¥¬', '6', text)
        text = re.sub(r'à¥­', '7', text)
        text = re.sub(r'à¥®', '8', text)
        text = re.sub(r'à¥¯', '9', text)
        return text

    def _basic_tokenize(self, text: str, lang: str) -> List[str]:
        """åŸºç¡€åˆ†è¯"""
        # æ ¹æ®è¯­è¨€é€‰æ‹©åˆ†è¯ç­–ç•¥
        if lang in ['zh', 'ja', 'ko', 'th']:
            # äºšæ´²è¯­è¨€ï¼šæŒ‰å­—ç¬¦åˆ†è¯
            tokens = []
            for char in text:
                if char.strip():
                    tokens.append(char)
        elif lang == 'ar':
            # é˜¿æ‹‰ä¼¯è¯­ï¼šä»å³åˆ°å·¦
            tokens = text.split()
        else:
            # å…¶ä»–è¯­è¨€ï¼šæ ‡å‡†ç©ºæ ¼åˆ†è¯
            tokens = text.split()

        return tokens

class AdvancedMultilingualTokenizer(MultilingualTokenizer):
    """
    é«˜çº§å¤šè¯­è¨€åˆ†è¯å™¨
    """
    def __init__(self):
        super().__init__()
        self.subword_tokenizers = {}
        self.cache = {}

    def train_subword_tokenizer(self, lang: str, corpus: List[str], vocab_size: int = 1000):
        """è®­ç»ƒè¯­è¨€ç‰¹å®šçš„å­è¯åˆ†è¯å™¨"""
        if lang == 'en':
            tokenizer = BPETokenizer(vocab_size)
        elif lang == 'zh':
            tokenizer = self._train_chinese_tokenizer(corpus, vocab_size)
        else:
            tokenizer = BPETokenizer(vocab_size)

        tokenizer.train(corpus)
        self.subword_tokenizers[lang] = tokenizer

    def _train_chinese_tokenizer(self, corpus: List[str], vocab_size: int):
        """è®­ç»ƒä¸­æ–‡åˆ†è¯å™¨"""
        # å¯¹äºä¸­æ–‡ï¼Œæˆ‘ä»¬å¯ä»¥è®­ç»ƒä¸€ä¸ªåŸºäºå­—ç¬¦çš„åˆ†è¯å™¨
        # æˆ–è€…ä½¿ç”¨æ›´å¤æ‚çš„ç®—æ³•
        class ChineseTokenizer:
            def __init__(self, vocab_size):
                self.vocab_size = vocab_size
                self.char_vocab = set()

            def train(self, corpus):
                for text in corpus:
                    for char in text:
                        if re.search(r'[\u4e00-\u9fff]', char):
                            self.char_vocab.add(char)

            def tokenize(self, text):
                tokens = []
                for char in text:
                    if re.search(r'[\u4e00-\u9fff]', char):
                        tokens.append(char)
                    elif char.strip():
                        tokens.extend(char.split())
                return tokens

        tokenizer = ChineseTokenizer(vocab_size)
        tokenizer.train(corpus)
        return tokenizer

    def tokenize(self, text: str) -> List[str]:
        """é«˜çº§å¤šè¯­è¨€åˆ†è¯"""
        # æ£€æµ‹è¯­è¨€
        lang = self.detect_language(text)

        # æ£€æŸ¥ç¼“å­˜
        cache_key = hash(text + lang)
        if cache_key in self.cache:
            return self.cache[cache_key]

        # åº”ç”¨è¯­è¨€ç‰¹å®šè§„åˆ™
        if lang in self.language_rules:
            text = self.language_rules[lang](text)

        # ä½¿ç”¨å­è¯åˆ†è¯å™¨
        if lang in self.subword_tokenizers:
            tokens = self.subword_tokenizers[lang].tokenize(text)
        else:
            tokens = self._basic_tokenize(text, lang)

        # ç¼“å­˜ç»“æœ
        self.cache[cache_key] = tokens

        return tokens

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºå¤šè¯­è¨€åˆ†è¯å™¨
    tokenizer = AdvancedMultilingualTokenizer()

    # è®­ç»ƒæ•°æ®
    training_data = {
        'en': [
            "hello world",
            "machine learning",
            "natural language processing"
        ],
        'zh': [
            "ä½ å¥½ä¸–ç•Œ",
            "æœºå™¨å­¦ä¹ ",
            "è‡ªç„¶è¯­è¨€å¤„ç†"
        ],
        'ja': [
            "ã“ã‚“ã«ã¡ã¯ä¸–ç•Œ",
            "æ©Ÿæ¢°å­¦ç¿’",
            "è‡ªç„¶è¨€èªå‡¦ç†"
        ],
        'ar': [
            "Ù…Ø±Ø­Ø¨Ø§ Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…",
            "Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ",
            "Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©"
        ]
    }

    # è®­ç»ƒå­è¯åˆ†è¯å™¨
    for lang, corpus in training_data.items():
        tokenizer.train_subword_tokenizer(lang, corpus, vocab_size=100)

    # æµ‹è¯•å¤šè¯­è¨€åˆ†è¯
    test_texts = [
        "hello world",
        "ä½ å¥½ä¸–ç•Œ",
        "ã“ã‚“ã«ã¡ã¯ä¸–ç•Œ",
        "Ù…Ø±Ø­Ø¨Ø§ Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…",
        "machine learning is awesome",
        "æœºå™¨å­¦ä¹ å¾ˆæ£’",
        "æ©Ÿæ¢°å­¦ç¿’ã¯ç´ æ™´ã‚‰ã—ã„",
        "Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ø±Ø§Ø¦Ø¹"
    ]

    print("å¤šè¯­è¨€åˆ†è¯æµ‹è¯•:")
    for text in test_texts:
        tokens = tokenizer.tokenize(text)
        lang = tokenizer.detect_language(text)
        print(f"{lang}: {text} -> {tokens}")
```

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”ä¸æœ€ä½³å®è·µ

### ğŸ”§ å…³é”®ä¼˜åŒ–ç­–ç•¥

#### 1. **ç®—æ³•é€‰æ‹©**
- **WordPiece**ï¼šé€‚åˆéœ€è¦å¤„ç†æœªçŸ¥è¯çš„åœºæ™¯
- **BPE**ï¼šé€‚åˆéœ€è¦å‹ç¼©è¯æ±‡è¡¨çš„åœºæ™¯
- **SentencePiece**ï¼šé€‚åˆå¤šè¯­è¨€å’Œæ— ç©ºæ ¼è¯­è¨€

#### 2. **æ€§èƒ½ä¼˜åŒ–**
- **ç¼“å­˜æœºåˆ¶**ï¼šç¼“å­˜å¸¸ç”¨æ–‡æœ¬çš„åˆ†è¯ç»“æœ
- **å¹¶è¡Œå¤„ç†**ï¼šä½¿ç”¨å¤šçº¿ç¨‹å¤„ç†æ‰¹é‡æ–‡æœ¬
- **å†…å­˜ç®¡ç†**ï¼šåˆç†è®¾ç½®ç¼“å­˜å¤§å°ï¼Œé¿å…å†…å­˜æ³„æ¼

#### 3. **å¤šè¯­è¨€æ”¯æŒ**
- **è¯­è¨€æ£€æµ‹**ï¼šå‡†ç¡®æ£€æµ‹æ–‡æœ¬è¯­è¨€
- **è§„åˆ™å®šåˆ¶**ï¼šä¸ºä¸åŒè¯­è¨€å®šåˆ¶å¤„ç†è§„åˆ™
- **å­è¯è®­ç»ƒ**ï¼šé’ˆå¯¹ç‰¹å®šè¯­è¨€è®­ç»ƒå­è¯æ¨¡å‹

### ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡

| åˆ†è¯å™¨ | é€Ÿåº¦ (tokens/s) | å†…å­˜å ç”¨ (MB) | è¯æ±‡è¡¨å¤§å° | OOVç‡ |
|--------|-----------------|--------------|-----------|--------|
| WordPiece | 50,000 | 100 | 30,000 | 0.5% |
| BPE | 45,000 | 80 | 25,000 | 0.8% |
| SentencePiece | 40,000 | 120 | 35,000 | 0.3% |

---

## ğŸ¯ æ€»ç»“ä¸å±•æœ›

### ğŸ”‘ å…³é”®è¦ç‚¹æ€»ç»“

1. **åˆ†è¯ç®—æ³•å¤šæ ·åŒ–**ï¼šWordPieceã€BPEã€SentencePieceå„æœ‰ä¼˜åŠ¿ï¼Œé€‚ç”¨äºä¸åŒåœºæ™¯ã€‚

2. **é¢„å¤„ç†é‡è¦æ€§**ï¼šæ–‡æœ¬æ ‡å‡†åŒ–ã€Unicodeå¤„ç†ã€è¯­è¨€æ£€æµ‹ç­‰é¢„å¤„ç†æ­¥éª¤å¯¹åˆ†è¯è´¨é‡å½±å“é‡å¤§ã€‚

3. **æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯**ï¼šç¼“å­˜æœºåˆ¶ã€å¹¶è¡Œå¤„ç†ã€å†…å­˜ç®¡ç†ç­‰æŠ€æœ¯èƒ½æ˜¾è‘—æå‡åˆ†è¯æ•ˆç‡ã€‚

4. **å¤šè¯­è¨€æ”¯æŒ**ï¼šç°ä»£åˆ†è¯å™¨éœ€è¦æ”¯æŒå¤šç§è¯­è¨€ï¼ŒåŒ…æ‹¬ç©ºæ ¼åˆ†éš”ç¬¦ä¸æ˜æ˜¾çš„è¯­è¨€ã€‚

5. **å¯æ‰©å±•æ€§**ï¼šè‰¯å¥½çš„æ¶æ„è®¾è®¡æ”¯æŒæ·»åŠ æ–°çš„åˆ†è¯ç®—æ³•å’Œè¯­è¨€è§„åˆ™ã€‚

### ğŸš€ æœªæ¥å‘å±•è¶‹åŠ¿

1. **ç¥ç»ç½‘ç»œåˆ†è¯å™¨**ï¼šåŸºäºç¥ç»ç½‘ç»œçš„è‡ªé€‚åº”åˆ†è¯å™¨
2. **æ— ç›‘ç£åˆ†è¯**ï¼šæ— éœ€æ ‡æ³¨æ•°æ®çš„æ— ç›‘ç£åˆ†è¯æ–¹æ³•
3. **å®æ—¶åˆ†è¯**ï¼šæ”¯æŒå®æ—¶æµå¼æ–‡æœ¬çš„åˆ†è¯
4. **è·¨è¯­è¨€åˆ†è¯**ï¼šç»Ÿä¸€çš„è·¨è¯­è¨€åˆ†è¯æ¡†æ¶
5. **å¯è§£é‡Šåˆ†è¯**ï¼šæä¾›åˆ†è¯å†³ç­–çš„è§£é‡Šå’Œå¯è§†åŒ–

### ğŸ¯ æœ€ä½³å®è·µå»ºè®®

1. **ç®—æ³•é€‰æ‹©**ï¼šæ ¹æ®ä»»åŠ¡éœ€æ±‚é€‰æ‹©åˆé€‚çš„åˆ†è¯ç®—æ³•
2. **æ€§èƒ½ç›‘æ§**ï¼šç›‘æ§åˆ†è¯å™¨çš„æ€§èƒ½æŒ‡æ ‡ï¼ŒåŠæ—¶ä¼˜åŒ–
3. **è¯­è¨€é€‚é…**ï¼šä¸ºç›®æ ‡è¯­è¨€å®šåˆ¶åˆ†è¯è§„åˆ™
4. **ç¼“å­˜ç­–ç•¥**ï¼šåˆç†ä½¿ç”¨ç¼“å­˜æå‡æ€§èƒ½
5. **è´¨é‡è¯„ä¼°**ï¼šå®šæœŸè¯„ä¼°åˆ†è¯è´¨é‡ï¼ŒæŒç»­æ”¹è¿›

Tokenizationç³»ç»Ÿä½œä¸ºNLPçš„åŸºç¡€è®¾æ–½ï¼Œå…¶è®¾è®¡å’Œå®ç°è´¨é‡ç›´æ¥å½±å“æ•´ä¸ªNLPç³»ç»Ÿçš„æ€§èƒ½ã€‚é€šè¿‡æ·±å…¥ç†è§£å„ç§åˆ†è¯ç®—æ³•çš„åŸç†å’Œä¼˜åŒ–æŠ€æœ¯ï¼Œæˆ‘ä»¬å¯ä»¥æ„å»ºæ›´é«˜æ•ˆã€æ›´å‡†ç¡®çš„NLPç³»ç»Ÿã€‚

---

**ğŸ”— ç›¸å…³èµ„æºï¼š**
- [SentencePieceå®˜æ–¹æ–‡æ¡£](https://github.com/google/sentencepiece)
- [BPEåŸå§‹è®ºæ–‡](https://arxiv.org/abs/1508.07909)
- [WordPieceè®ºæ–‡](https://arxiv.org/abs/2112.10508)

**ğŸ“§ æŠ€æœ¯äº¤æµï¼š**
æ¬¢è¿åœ¨è¯„è®ºåŒºåˆ†äº«æ‚¨çš„åˆ†è¯ç»éªŒå’Œä¼˜åŒ–æŠ€å·§ï¼Œå…±åŒæ¢è®¨NLPåŸºç¡€æŠ€æœ¯çš„æœªæ¥å‘å±•ã€‚

---

*æœ¬æ–‡åŸºäºTransformersåº“æœ€æ–°ç‰ˆæœ¬æºç åˆ†æï¼Œéƒ¨åˆ†ä»£ç ç¤ºä¾‹å¯èƒ½éœ€è¦æ ¹æ®å®é™…ç‰ˆæœ¬è¿›è¡Œè°ƒæ•´ã€‚*