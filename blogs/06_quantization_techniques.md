# ğŸ”¥ HuggingFace Transformersåº“æ·±åº¦è§£æç³»åˆ—ï¼ˆå…­ï¼‰ï¼šé‡åŒ–æŠ€æœ¯ä¸æ¨¡å‹å‹ç¼©

> ä½œä¸ºOpenAIçš„æŠ€æœ¯æ¶æ„å¸ˆï¼Œä»Šå¤©æˆ‘å°†æ·±å…¥å‰–æTransformersåº“ä¸­çš„é‡åŒ–æŠ€æœ¯ä¸æ¨¡å‹å‹ç¼©å®ç°ã€‚è¿™æ˜¯å¤§æ¨¡å‹éƒ¨ç½²çš„å…³é”®æŠ€æœ¯ï¼Œé€šè¿‡æ•°å€¼ç²¾åº¦ä¼˜åŒ–å¤§å¹…é™ä½è®¡ç®—å’Œå†…å­˜å¼€é”€ï¼Œè®©å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šè¿è¡Œã€‚æœ¬æ–‡å°†ä»æºç å±‚é¢å½»åº•è§£æå„ç§é‡åŒ–ç®—æ³•çš„å®ç°åŸç†ã€‚

## ğŸ“‹ ç›®å½•

- [é‡åŒ–æŠ€æœ¯çš„æ ¸å¿ƒä½œç”¨ä¸æŒ‘æˆ˜](#é‡åŒ–æŠ€æœ¯çš„æ ¸å¿ƒä½œç”¨ä¸æŒ‘æˆ˜)
- [é‡åŒ–ç³»ç»Ÿæ¶æ„è®¾è®¡](#é‡åŒ–ç³»ç»Ÿæ¶æ„è®¾è®¡)
- [BitsAndBytesé‡åŒ–æŠ€æœ¯æ·±åº¦å‰–æ](#bitsandbytesé‡åŒ–æŠ€æœ¯æ·±åº¦å‰–æ)
- [GPTQé‡åŒ–ç®—æ³•å®ç°åŸç†](#gptqé‡åŒ–ç®—æ³•å®ç°åŸç†)
- [AWQæ¿€æ´»æ„ŸçŸ¥é‡åŒ–æŠ€æœ¯](#awqæ¿€æ´»æ„ŸçŸ¥é‡åŒ–æŠ€æœ¯)
- [AQLM/HQQ/SPQRå…ˆè¿›é‡åŒ–ç®—æ³•](#aqlmhqqspqrå…ˆè¿›é‡åŒ–ç®—æ³•)
- [é‡åŒ–æ„ŸçŸ¥è®­ç»ƒä¸å¾®è°ƒ](#é‡åŒ–æ„ŸçŸ¥è®­ç»ƒä¸å¾®è°ƒ)
- [æ¨¡å‹å‹ç¼©æŠ€æœ¯ç»¼åˆåˆ†æ](#æ¨¡å‹å‹ç¼©æŠ€æœ¯ç»¼åˆåˆ†æ)
- [æ€§èƒ½å¯¹æ¯”ä¸æœ€ä½³å®è·µ](#æ€§èƒ½å¯¹æ¯”ä¸æœ€ä½³å®è·µ)
- [å®æˆ˜ä»£ç ç¤ºä¾‹](#å®æˆ˜ä»£ç ç¤ºä¾‹)
- [æ€»ç»“ä¸å±•æœ›](#æ€»ç»“ä¸å±•æœ›)

---

## ğŸ¯ é‡åŒ–æŠ€æœ¯çš„æ ¸å¿ƒä½œç”¨ä¸æŒ‘æˆ˜

### ğŸ”‘ é‡åŒ–åŸºæœ¬æ¦‚å¿µ

**é‡åŒ–**æ˜¯å°†é«˜ç²¾åº¦æµ®ç‚¹æ•°è½¬æ¢ä¸ºä½ç²¾åº¦è¡¨ç¤ºçš„è¿‡ç¨‹ï¼Œä¸»è¦ç›®æ ‡ï¼š

```python
# é‡åŒ–åŸºæœ¬åŸç†ç¤ºä¾‹
def quantize(weight, scale, zero_point, dtype=torch.int8):
    """
    å°†FP32æƒé‡é‡åŒ–ä¸ºINT8
    å…¬å¼: quantized_value = round(weight / scale) + zero_point
    """
    quantized = torch.round(weight / scale) + zero_point
    return quantized.clamp(dtype.min, dtype.max).to(dtype)

def dequantize(quantized, scale, zero_point):
    """
    å°†INT8åé‡åŒ–ä¸ºFP32
    å…¬å¼: dequantized_value = (quantized_value - zero_point) * scale
    """
    return (quantized.float() - zero_point) * scale
```

### ğŸ“Š é‡åŒ–æŠ€æœ¯çš„ä¼˜åŠ¿

| æŒ‡æ ‡ | FP32 | INT8 | INT4 | NF4 |
|------|------|------|------|------|
| å†…å­˜å ç”¨ | 100% | 25% | 12.5% | 12.5% |
| è®¡ç®—é€Ÿåº¦ | 1x | 2-4x | 4-8x | 4-8x |
| ç²¾åº¦æŸå¤± | 0% | 0.5-2% | 2-5% | 1-3% |
| æ”¯æŒç¡¬ä»¶ | é€šç”¨ | ä¸“ç”¨ | æœ‰é™ | æœ‰é™ |

### ğŸ¯ ä¸»è¦æŠ€æœ¯æŒ‘æˆ˜

1. **ç²¾åº¦ä¿æŒ**ï¼šåœ¨å‹ç¼©ç‡ä¸æ¨¡å‹è´¨é‡é—´å–å¾—å¹³è¡¡
2. **ç¡¬ä»¶å…¼å®¹**ï¼šä¸åŒç¡¬ä»¶å¹³å°å¯¹ä½ç²¾åº¦æ”¯æŒç¨‹åº¦ä¸åŒ
3. **é‡åŒ–å™ªå£°**ï¼šç¦»æ•£åŒ–è¿‡ç¨‹å¼•å…¥çš„é‡åŒ–è¯¯å·®
4. **å¼‚å¸¸å€¼å¤„ç†**ï¼šå¤§æƒé‡å€¼çš„é‡åŒ–è¯¯å·®æ”¾å¤§
5. **è®­ç»ƒä¸€è‡´æ€§**ï¼šé‡åŒ–æ„ŸçŸ¥è®­ç»ƒä¸æ¨ç†çš„ä¸€è‡´æ€§ä¿è¯

---

## ğŸ—ï¸ é‡åŒ–ç³»ç»Ÿæ¶æ„è®¾è®¡

### ğŸ“ æ¨¡å—åŒ–æ¶æ„

Transformersåº“é‡‡ç”¨**é«˜åº¦æ¨¡å—åŒ–**çš„é‡åŒ–ç³»ç»Ÿè®¾è®¡ï¼š

```python
# é‡åŒ–å™¨åŸºç±»æ¶æ„
class HfQuantizer(ABC):
    """
    é‡åŒ–å™¨æŠ½è±¡åŸºç±»ï¼Œå®šä¹‰æ ‡å‡†åŒ–çš„é‡åŒ–æ¥å£

    æ ¸å¿ƒç‰¹æ€§ï¼š
    1. æ ‡å‡†åŒ–çš„é‡åŒ–ç”Ÿå‘½å‘¨æœŸ
    2. å¯æ‰©å±•çš„é…ç½®ç³»ç»Ÿ
    3. è‡ªåŠ¨è®¾å¤‡ç®¡ç†
    4. æ¨¡å—çº§åˆ«çš„é‡åŒ–æ§åˆ¶
    """
    def __init__(self, quantization_config: QuantizationConfig):
        self.quantization_config = quantization_config
        self.modules_to_not_convert = None
        self.pre_quantized = False

    @abstractmethod
    def validate_environment(self, *args, **kwargs):
        """éªŒè¯ç¡¬ä»¶ç¯å¢ƒå’Œä¾èµ–åº“æ”¯æŒ"""
        pass

    @abstractmethod
    def preprocess_model(self, model: torch.nn.Module, **kwargs):
        """æ¨¡å‹é¢„å¤„ç†ï¼šæ›¿æ¢é‡åŒ–æ¨¡å—"""
        pass

    @abstractmethod
    def postprocess_model(self, model: torch.nn.Module, **kwargs):
        """æ¨¡å‹åå¤„ç†ï¼šåº”ç”¨é‡åŒ–å‚æ•°"""
        pass
```

### ğŸ”§ é…ç½®ç³»ç»Ÿè®¾è®¡

**ç»Ÿä¸€çš„é‡åŒ–é…ç½®ç®¡ç†**ï¼š

```python
class QuantizationConfig:
    """
    é‡åŒ–é…ç½®åŸºç±»ï¼Œå®šä¹‰æ ‡å‡†åŒ–çš„é‡åŒ–å‚æ•°

    é…ç½®é¡¹ï¼š
    - load_in_8bit: 8ä½é‡åŒ–åŠ è½½
    - load_in_4bit: 4ä½é‡åŒ–åŠ è½½
    - bnb_4bit_quant_type: 4ä½é‡åŒ–ç±»å‹ (NF4/FP4)
    - bnb_4bit_compute_dtype: è®¡ç®—æ•°æ®ç±»å‹
    - bnb_4bit_use_double_quant: åŒé‡é‡åŒ–
    - torch_dtype: æ¨¡å‹æ•°æ®ç±»å‹
    """
    def __init__(
        self,
        load_in_8bit: bool = False,
        load_in_4bit: bool = False,
        bnb_4bit_quant_type: str = "fp4",  # "fp4" or "nf4"
        bnb_4bit_compute_dtype: str = "float32",  # "float32", "float16", "bfloat16"
        bnb_4bit_use_double_quant: bool = False,
        torch_dtype: Optional[str] = None,
        **kwargs
    ):
        self.load_in_8bit = load_in_8bit
        self.load_in_4bit = load_in_4bit
        self.bnb_4bit_quant_type = bnb_4bit_quant_type
        self.bnb_4bit_compute_dtype = bnb_4bit_compute_dtype
        self.bnb_4bit_use_double_quant = bnb_4bit_use_double_quant
        self.torch_dtype = torch_dtype

    def is_quantizable(self):
        """æ£€æŸ¥æ˜¯å¦ä¸ºå¯é‡åŒ–çš„é…ç½®"""
        return self.load_in_8bit or self.load_in_4bit
```

### ğŸ”„ é‡åŒ–ç”Ÿå‘½å‘¨æœŸç®¡ç†

**æ ‡å‡†åŒ–çš„é‡åŒ–æµç¨‹**ï¼š

```python
def apply_quantization(model: torch.nn.Module, quantization_config: QuantizationConfig):
    """
    åº”ç”¨é‡åŒ–åˆ°æ¨¡å‹çš„æ ‡å‡†åŒ–æµç¨‹

    æµç¨‹ï¼š
    1. ç¯å¢ƒéªŒè¯ï¼šæ£€æŸ¥ç¡¬ä»¶å’Œä¾èµ–
    2. é‡åŒ–å™¨é€‰æ‹©ï¼šåŸºäºé…ç½®é€‰æ‹©é‡åŒ–å™¨
    3. æ¨¡å‹é¢„å¤„ç†ï¼šæ›¿æ¢çº¿æ€§å±‚ä¸ºé‡åŒ–å±‚
    4. é‡åŒ–å‚æ•°è®¡ç®—ï¼šè®¡ç®—ç¼©æ”¾å› å­å’Œé›¶ç‚¹
    5. æ¨¡å‹åå¤„ç†ï¼šåº”ç”¨é‡åŒ–é…ç½®
    """
    # 1. é€‰æ‹©é‡åŒ–å™¨
    quantizer = get_quantizer(quantization_config)

    # 2. éªŒè¯ç¯å¢ƒ
    quantizer.validate_environment()

    # 3. é¢„å¤„ç†æ¨¡å‹
    model = quantizer.preprocess_model(model)

    # 4. åå¤„ç†æ¨¡å‹
    model = quantizer.postprocess_model(model)

    return model
```

---

## âš¡ BitsAndBytesé‡åŒ–æŠ€æœ¯æ·±åº¦å‰–æ

### ğŸ—ï¸ LLM.int8() 8ä½é‡åŒ–å®ç°

**æ ¸å¿ƒæŠ€æœ¯**ï¼šæ··åˆç²¾åº¦é‡åŒ–ï¼Œå¤„ç†å¼‚å¸¸å€¼

```python
class BitsAndBytesConfig(QuantizationConfig):
    """
    BitsAndBytesé‡åŒ–é…ç½®ï¼Œæ”¯æŒ8ä½å’Œ4ä½é‡åŒ–

    å…³é”®ç‰¹æ€§ï¼š
    1. LLM.int8(): 8ä½æ··åˆç²¾åº¦é‡åŒ–
    2. NF4: 4ä½å½’ä¸€åŒ–æµ®ç‚¹é‡åŒ–
    3. åŒé‡é‡åŒ–ï¼šé‡åŒ–é‡åŒ–å‚æ•°
    4. å¼‚å¸¸å€¼ä¿æŠ¤ï¼šæ™ºèƒ½è¯†åˆ«å’Œå¤„ç†å¤§æƒé‡
    """
    def __init__(
        self,
        load_in_8bit: bool = False,
        load_in_4bit: bool = False,
        bnb_4bit_quant_type: str = "fp4",
        bnb_4bit_compute_dtype: Union[str, torch.dtype] = torch.float32,
        bnb_4bit_use_double_quant: bool = False,
        bnb_4bit_quant_storage: Optional[Union[str, torch.dtype]] = None,
        quant_method: str = "bitsandbytes",
        **kwargs
    ):
        super().__init__(**kwargs)
        self.load_in_8bit = load_in_8bit
        self.load_in_4bit = load_in_4bit
        self.bnb_4bit_quant_type = bnb_4bit_quant_type
        self.bnb_4bit_compute_dtype = convert_to_torch_dtype(bnb_4bit_compute_dtype)
        self.bnb_4bit_use_double_quant = bnb_4bit_use_double_quant
        self.bnb_4bit_quant_storage = convert_to_torch_dtype(bnb_4bit_quant_storage)
        self.quant_method = quant_method
```

### ğŸ”§ æ··åˆç²¾åº¦é‡åŒ–å®ç°

**å¼‚å¸¸å€¼å¤„ç†æœºåˆ¶**ï¼š

```python
def quantize_blockwise(tensor, block_size=64):
    """
    åˆ†å—é‡åŒ–å®ç°ï¼Œç”¨äºLLM.int8()

    ç®—æ³•æ­¥éª¤ï¼š
    1. å°†å¼ é‡åˆ†å—å¤„ç†
    2. è®¡ç®—æ¯å—çš„ç¼©æ”¾å› å­
    3. è¯†åˆ«å¼‚å¸¸å€¼ï¼ˆ>6Ïƒï¼‰
    4. å¼‚å¸¸å€¼ä½¿ç”¨FP16ï¼Œæ­£å¸¸å€¼ä½¿ç”¨INT8
    """
    # è·å–å¼ é‡å½¢çŠ¶
    if tensor.dim() == 1:
        tensor = tensor.unsqueeze(0)

    batch_size, hidden_size = tensor.shape
    num_blocks = (hidden_size + block_size - 1) // block_size

    # åˆ†å—å¤„ç†
    quantized_blocks = []
    scales = []
    zeros = []
    outlier_masks = []

    for i in range(num_blocks):
        start_idx = i * block_size
        end_idx = min((i + 1) * block_size, hidden_size)
        block = tensor[:, start_idx:end_idx]

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        abs_max = torch.abs(block).max()
        mean = block.mean()
        std = block.std()

        # è¯†åˆ«å¼‚å¸¸å€¼ (è¶…è¿‡6ä¸ªæ ‡å‡†å·®)
        outlier_mask = torch.abs(block - mean) > 6 * std

        # æ­£å¸¸å€¼ä½¿ç”¨INT8é‡åŒ–
        normal_block = block[~outlier_mask]
        if normal_block.numel() > 0:
            scale = abs_max / 127.0
            zero_point = 0
            quantized_normal = torch.clamp(normal_block / scale + zero_point, -128, 127).to(torch.int8)
        else:
            scale = 1.0
            zero_point = 0
            quantized_normal = torch.tensor([], dtype=torch.int8)

        # å¼‚å¸¸å€¼ä¿æŒFP16
        outlier_block = block[outlier_mask]

        quantized_blocks.append({
            'normal': quantized_normal,
            'outliers': outlier_block,
            'scale': scale,
            'zero_point': zero_point,
            'outlier_mask': outlier_mask
        })

    return quantized_blocks
```

### ğŸš€ NF4 4ä½é‡åŒ–å®ç°

**å½’ä¸€åŒ–æµ®ç‚¹4ä½é‡åŒ–**ï¼š

```python
class NF4Quantizer:
    """
    NF4 (Normalized Float4) é‡åŒ–å™¨

    ç‰¹æ€§ï¼š
    1. åŒé‡é‡åŒ–ï¼šé‡åŒ–é‡åŒ–å‚æ•°
    2. å½’ä¸€åŒ–ï¼šåŸºäºæ•°æ®åˆ†å¸ƒçš„ä¼˜åŒ–
    3. è®¡ç®—ä¼˜åŒ–ï¼šæ”¯æŒé«˜æ•ˆè®¡ç®—
    """
    def __init__(self, compute_dtype=torch.bfloat16):
        self.compute_dtype = compute_dtype
        self.nf4_levels = self._init_nf4_levels()

    def _init_nf4_levels(self):
        """
        åˆå§‹åŒ–NF4é‡åŒ–çº§åˆ«
        NF4ä½¿ç”¨éå‡åŒ€åˆ†å¸ƒçš„é‡åŒ–çº§åˆ«ï¼Œæ›´é€‚åˆLLMæƒé‡åˆ†å¸ƒ
        """
        # NF4é‡åŒ–çº§åˆ« (åŸºäºæ­£æ€åˆ†å¸ƒä¼˜åŒ–)
        levels = torch.tensor([
            -1.0, -0.6961928009986877, -0.5250730514526367, -0.39491748809814453,
            -0.2945677638053894, -0.20765233063697815, -0.13049088954925537,
            -0.05977457046508789, 0.0, 0.05977457046508789, 0.13049088954925537,
            0.20765233063697815, 0.2945677638053894, 0.39491748809814453,
            0.5250730514526367, 0.6961928009986877, 1.0
        ])
        return levels.to(self.compute_dtype)

    def quantize_nf4(self, weight):
        """
        NF4é‡åŒ–å®ç°
        """
        # å½’ä¸€åŒ–æƒé‡åˆ°[-1, 1]èŒƒå›´
        abs_max = torch.abs(weight).max()
        normalized_weight = weight / abs_max

        # æ‰¾åˆ°æœ€è¿‘çš„é‡åŒ–çº§åˆ«
        distances = torch.abs(normalized_weight.unsqueeze(-1) - self.nf4_levels)
        quantized_indices = torch.argmin(distances, dim=-1)

        # è·å–é‡åŒ–å€¼
        quantized_weight = self.nf4_levels[quantized_indices]

        # åå½’ä¸€åŒ–
        quantized_weight = quantized_weight * abs_max

        return quantized_weight, abs_max
```

### ğŸ“Š åŒé‡é‡åŒ–ä¼˜åŒ–

**é‡åŒ–å‚æ•°çš„é‡åŒ–**ï¼š

```python
class DoubleQuantizer:
    """
    åŒé‡é‡åŒ–ï¼šé‡åŒ–é‡åŒ–å‚æ•°ä»¥è¿›ä¸€æ­¥èŠ‚çœå†…å­˜

    ç®—æ³•ï¼š
    1. ç¬¬ä¸€å±‚é‡åŒ–ï¼šæƒé‡é‡åŒ–ä¸º4ä½
    2. ç¬¬äºŒå±‚é‡åŒ–ï¼šç¼©æ”¾å› å­é‡åŒ–ä¸º8ä½
    3. æ€»ä½“å‹ç¼©ï¼š(4+8)/32 = 37.5%å†…å­˜å ç”¨
    """
    def __init__(self):
        self.weight_scale = None
        self.scale_scale = None

    def double_quantize(self, weight):
        """
        åŒé‡é‡åŒ–å®ç°
        """
        # ç¬¬ä¸€å±‚ï¼šæƒé‡é‡åŒ–ä¸ºNF4
        quantized_weight, weight_scale = self.quantize_nf4(weight)

        # ç¬¬äºŒå±‚ï¼šç¼©æ”¾å› å­é‡åŒ–ä¸ºINT8
        scale_abs_max = torch.abs(weight_scale).max()
        normalized_scale = weight_scale / scale_abs_max

        # ç¼©æ”¾å› å­é‡åŒ–ä¸ºINT8
        quantized_scale = torch.clamp(normalized_scale * 127, -128, 127).to(torch.int8)
        scale_scale = scale_abs_max / 127.0

        return quantized_weight, quantized_scale, scale_scale

    def double_dequantize(self, quantized_weight, quantized_scale, scale_scale):
        """
        åŒé‡é‡åŒ–åé‡åŒ–
        """
        # åé‡åŒ–ç¼©æ”¾å› å­
        weight_scale = quantized_scale.float() * scale_scale

        # åé‡åŒ–æƒé‡
        weight = quantized_weight * weight_scale

        return weight
```

---

## ğŸ¯ GPTQé‡åŒ–ç®—æ³•å®ç°åŸç†

### ğŸ—ï¸ GPTQæ ¸å¿ƒç®—æ³•

**åŸºäºHessiançš„åè®­ç»ƒé‡åŒ–**ï¼š

```python
class GPTQQuantizer:
    """
    GPTQ (Post-Training Quantization for GPT) é‡åŒ–å™¨

    æ ¸å¿ƒæ€æƒ³ï¼š
    1. åŸºäºäºŒé˜¶ä¿¡æ¯ï¼ˆHessiançŸ©é˜µï¼‰çš„é‡è¦æ€§é‡åŒ–
    2. é€å±‚é‡åŒ–ï¼Œä¿æŒå…¶ä»–å±‚ä¸å˜
    3. è¿­ä»£æ›´æ–°ï¼Œæœ€å°åŒ–é‡åŒ–è¯¯å·®

    ç®—æ³•æ­¥éª¤ï¼š
    1. è®¡ç®—Hessianå¯¹è§’çŸ©é˜µ
    2. æŒ‰é‡è¦æ€§æ’åºæƒé‡
    3. é€ä¸ªé‡åŒ–æƒé‡å¹¶æ›´æ–°æ®‹å·®
    """
    def __init__(self, wbits=4, group_size=128):
        self.wbits = wbits  # é‡åŒ–ä½æ•°
        self.group_size = group_size  # åˆ†ç»„å¤§å°

    def gptq_quantize(self, weight, hessian_diag):
        """
        GPTQé‡åŒ–æ ¸å¿ƒå®ç°

        å‚æ•°ï¼š
        - weight: [out_features, in_features] æƒé‡çŸ©é˜µ
        - hessian_diag: [in_features] Hessianå¯¹è§’çŸ©é˜µ
        """
        out_features, in_features = weight.shape
        quantized_weight = weight.clone()

        # æŒ‰é‡è¦æ€§æ’åº (Hessianå¯¹è§’çº¿å…ƒç´ )
        importance = hessian_diag
        sorted_indices = torch.argsort(importance, descending=True)

        # é€ä¸ªé‡åŒ–æƒé‡
        for i in range(in_features):
            idx = sorted_indices[i]

            # æå–å½“å‰åˆ—
            column = quantized_weight[:, idx]

            # è®¡ç®—é‡åŒ–å‚æ•°
            max_val = torch.abs(column).max()
            scale = max_val / (2 ** (self.wbits - 1) - 1)
            zero_point = 0

            # é‡åŒ–å½“å‰åˆ—
            quantized_column = torch.clamp(
                torch.round(column / scale) + zero_point,
                -2 ** (self.wbits - 1),
                2 ** (self.wbits - 1) - 1
            )

            # åé‡åŒ–
            dequantized_column = (quantized_column - zero_point) * scale

            # è®¡ç®—é‡åŒ–è¯¯å·®
            error = column - dequantized_column

            # æ›´æ–°æ®‹å·®åˆ°å…¶ä»–åˆ—
            if i < in_features - 1:
                for j in range(i + 1, in_features):
                    other_idx = sorted_indices[j]
                    correction = error * hessian_diag[other_idx] / hessian_diag[idx]
                    quantized_weight[:, other_idx] += correction

            # åº”ç”¨é‡åŒ–ç»“æœ
            quantized_weight[:, idx] = dequantized_column

        return quantized_weight
```

### ğŸ”§ åˆ†ç»„GPTQä¼˜åŒ–

**åˆ†ç»„é‡åŒ–å‡å°‘è®¡ç®—å¤æ‚åº¦**ï¼š

```python
class GroupedGPTQQuantizer(GPTQQuantizer):
    """
    åˆ†ç»„GPTQé‡åŒ–å™¨ï¼Œæé«˜é‡åŒ–æ•ˆç‡

    ä¼˜åŒ–ï¼š
    1. åˆ†ç»„é‡åŒ–ï¼šæ¯ç»„128ä¸ªæƒé‡å…±äº«é‡åŒ–å‚æ•°
    2. å¹¶è¡Œè®¡ç®—ï¼šä¸åŒç»„å¯ä»¥å¹¶è¡Œé‡åŒ–
    3. å†…å­˜ä¼˜åŒ–ï¼šå‡å°‘ä¸­é—´ç»“æœå­˜å‚¨
    """
    def __init__(self, wbits=4, group_size=128):
        super().__init__(wbits, group_size)

    def grouped_gptq_quantize(self, weight, hessian_diag):
        """
        åˆ†ç»„GPTQé‡åŒ–å®ç°
        """
        out_features, in_features = weight.shape
        num_groups = in_features // self.group_size

        quantized_weight = weight.clone()
        scales = torch.zeros(num_groups, device=weight.device)
        zero_points = torch.zeros(num_groups, device=weight.device)

        for group_idx in range(num_groups):
            start_idx = group_idx * self.group_size
            end_idx = (group_idx + 1) * self.group_size

            group_weight = weight[:, start_idx:end_idx]
            group_hessian = hessian_diag[start_idx:end_idx]

            # å¯¹ç»„å†…æƒé‡æŒ‰é‡è¦æ€§æ’åº
            group_importance = group_hessian
            sorted_indices = torch.argsort(group_importance, descending=True)

            # ç»„å†…GPTQé‡åŒ–
            for i in range(self.group_size):
                local_idx = sorted_indices[i]
                global_idx = start_idx + local_idx

                column = quantized_weight[:, global_idx]

                # è®¡ç®—ç»„å†…é‡åŒ–å‚æ•°
                group_max = torch.abs(group_weight).max()
                scale = group_max / (2 ** (self.wbits - 1) - 1)
                zero_point = 0

                # é‡åŒ–
                quantized_column = torch.clamp(
                    torch.round(column / scale) + zero_point,
                    -2 ** (self.wbits - 1),
                    2 ** (self.wbits - 1) - 1
                )

                dequantized_column = (quantized_column - zero_point) * scale
                error = column - dequantized_column

                # æ›´æ–°ç»„å†…æ®‹å·®
                if i < self.group_size - 1:
                    for j in range(i + 1, self.group_size):
                        other_local_idx = sorted_indices[j]
                        other_global_idx = start_idx + other_local_idx
                        correction = error * hessian_diag[other_global_idx] / hessian_diag[global_idx]
                        quantized_weight[:, other_global_idx] += correction

                quantized_weight[:, global_idx] = dequantized_column

            # å­˜å‚¨ç»„é‡åŒ–å‚æ•°
            scales[group_idx] = group_max / (2 ** (self.wbits - 1) - 1)
            zero_points[group_idx] = 0

        return quantized_weight, scales, zero_points
```

### ğŸ“ˆ HessiançŸ©é˜µä¼°è®¡

**äºŒé˜¶ä¿¡æ¯çš„é‡è¦æ€§**ï¼š

```python
def estimate_hessian_diagonal(model, calib_data, device='cuda'):
    """
    ä¼°è®¡HessiançŸ©é˜µå¯¹è§’çº¿å…ƒç´ 

    æ–¹æ³•ï¼š
    1. ä½¿ç”¨æ ¡å‡†æ•°æ®å‰å‘ä¼ æ’­
    2. è®¡ç®—ä¸€é˜¶å¯¼æ•°
    3. ä¼°è®¡äºŒé˜¶ä¿¡æ¯
    """
    model.eval()
    hessian_diag = {}

    for name, module in model.named_modules():
        if isinstance(module, torch.nn.Linear):
            # åˆå§‹åŒ–Hessianå¯¹è§’çº¿ä¼°è®¡
            weight_hessian = torch.zeros_like(module.weight)

            # ä½¿ç”¨æ ¡å‡†æ•°æ®ä¼°è®¡
            for batch in calib_data:
                inputs = batch.to(device)

                # å‰å‘ä¼ æ’­
                outputs = module(inputs)

                # è®¡ç®—ä¸€é˜¶å¯¼æ•°
                grad_outputs = torch.ones_like(outputs)
                grads = torch.autograd.grad(
                    outputs=outputs,
                    inputs=module.weight,
                    grad_outputs=grad_outputs,
                    create_graph=True,
                    retain_graph=True
                )[0]

                # ä¼°è®¡äºŒé˜¶å¯¼æ•°ï¼ˆä½¿ç”¨å¹³æ–¹æ¢¯åº¦ï¼‰
                weight_hessian += grads ** 2

            # å¹³å‡åŒ–
            weight_hessian /= len(calib_data)
            hessian_diag[name] = weight_hessian

    return hessian_diag
```

---

## ğŸ§  AWQæ¿€æ´»æ„ŸçŸ¥é‡åŒ–æŠ€æœ¯

### ğŸ—ï¸ AWQæ ¸å¿ƒæ€æƒ³

**Activation-aware Weight Quantization**ï¼šåŸºäºæ¿€æ´»çš„é‡è¦æ€§è¿›è¡Œé‡åŒ–

```python
class AWQQuantizer:
    """
    AWQ (Activation-aware Weight Quantization) é‡åŒ–å™¨

    æ ¸å¿ƒæ€æƒ³ï¼š
    1. åŸºäºæ¿€æ´»å¹…åº¦çš„é‡è¦æ€§åˆ†æ
    2. ä¿æŠ¤é‡è¦æƒé‡é€šé“
    3. ç¼©æ”¾è¡¥å¿ä¿æŒç²¾åº¦

    ä¼˜åŠ¿ï¼š
    1. ç›¸æ¯”GPTQï¼Œé‡åŒ–é€Ÿåº¦æ›´å¿«
    2. ç²¾åº¦ä¿æŒæ›´å¥½
    3. æ”¯æŒåŠ¨æ€é‡åŒ–
    """
    def __init__(self, wbits=4, group_size=128):
        self.wbits = wbits
        self.group_size = group_size

    def compute_activation_importance(self, model, calib_data):
        """
        è®¡ç®—æ¿€æ´»é‡è¦æ€§æƒé‡
        """
        model.eval()
        activation_importance = {}

        # æ³¨å†Œhookæ”¶é›†æ¿€æ´»
        def get_activation(name):
            def hook(model, input, output):
                if name not in activation_importance:
                    activation_importance[name] = []
                activation_importance[name].append(output.detach())
            return hook

        # æ³¨å†Œhookåˆ°æ‰€æœ‰çº¿æ€§å±‚
        hooks = []
        for name, module in model.named_modules():
            if isinstance(module, torch.nn.Linear):
                hook = module.register_forward_hook(get_activation(name))
                hooks.append(hook)

        # å‰å‘ä¼ æ’­æ”¶é›†æ¿€æ´»
        for batch in calib_data:
            inputs = batch.to(device)
            _ = model(inputs)

        # ç§»é™¤hook
        for hook in hooks:
            hook.remove()

        # è®¡ç®—é‡è¦æ€§æƒé‡
        importance_weights = {}
        for name, activations in activation_importance.items():
            # è®¡ç®—æ¯ä¸ªè¾“å‡ºé€šé“çš„å¹³å‡æ¿€æ´»å¹…åº¦
            all_activations = torch.cat(activations, dim=0)  # [total_samples, out_features]
            importance = torch.abs(all_activations).mean(dim=0)  # [out_features]
            importance_weights[name] = importance

        return importance_weights

    def awq_quantize(self, weight, activation_importance):
        """
        AWQé‡åŒ–å®ç°
        """
        out_features, in_features = weight.shape

        # åŸºäºæ¿€æ´»é‡è¦æ€§å¯¹è¾“å‡ºé€šé“æ’åº
        sorted_channels = torch.argsort(activation_importance, descending=True)

        quantized_weight = weight.clone()
        scales = torch.zeros(out_features // self.group_size, device=weight.device)

        for group_idx in range(0, out_features, self.group_size):
            group_end = min(group_idx + self.group_size, out_features)
            group_channels = sorted_channels[group_idx:group_end]

            # æå–é‡è¦é€šé“çš„æƒé‡
            group_weight = weight[group_channels, :]

            # è®¡ç®—ç¼©æ”¾å› å­
            max_val = torch.abs(group_weight).max()
            scale = max_val / (2 ** (self.wbits - 1) - 1)

            # é‡åŒ–
            quantized_group = torch.clamp(
                torch.round(group_weight / scale),
                -2 ** (self.wbits - 1),
                2 ** (self.wbits - 1) - 1
            )

            # ç¼©æ”¾è¡¥å¿ï¼šä¿æŒåŸå§‹æ•°å€¼èŒƒå›´
            compensation_scale = activation_importance[group_channels].mean()
            quantized_group = quantized_group * compensation_scale

            # åº”ç”¨é‡åŒ–ç»“æœ
            quantized_weight[group_channels, :] = quantized_group * scale

            # å­˜å‚¨ç¼©æ”¾å› å­
            scales[group_idx // self.group_size] = scale

        return quantized_weight, scales
```

### ğŸ”§ ä¿æŠ¤æ€§ç¼©æ”¾

**é‡è¦é€šé“ä¿æŠ¤æœºåˆ¶**ï¼š

```python
class ProtectiveScaling:
    """
    ä¿æŠ¤æ€§ç¼©æ”¾ï¼šä¿æŠ¤é‡è¦æƒé‡é€šé“
    """
    def __init__(self, clip_ratio=0.99):
        self.clip_ratio = clip_ratio

    def apply_protective_scaling(self, weight, importance):
        """
        åº”ç”¨ä¿æŠ¤æ€§ç¼©æ”¾

        ç®—æ³•ï¼š
        1. è¯†åˆ«é‡è¦é€šé“ï¼ˆé‡è¦æ€§top 1%ï¼‰
        2. å¯¹é‡è¦é€šé“åº”ç”¨è¾ƒå°çš„ç¼©æ”¾å› å­
        3. å¯¹ä¸é‡è¦é€šé“åº”ç”¨è¾ƒå¤§çš„ç¼©æ”¾å› å­
        """
        # è®¡ç®—ç¼©æ”¾é˜ˆå€¼
        sorted_importance = torch.sort(importance, descending=True)[0]
        threshold = sorted_importance[int(self.clip_ratio * len(importance))]

        # ç”Ÿæˆç¼©æ”¾å› å­
        scales = torch.ones_like(importance)
        important_mask = importance > threshold

        # é‡è¦é€šé“ä½¿ç”¨è¾ƒå°çš„ç¼©æ”¾ï¼ˆä¿æŠ¤ï¼‰
        scales[important_mask] = 0.5 + 0.5 * (importance[important_mask] / importance.max())

        # ä¸é‡è¦é€šé“ä½¿ç”¨è¾ƒå¤§çš„ç¼©æ”¾ï¼ˆå‹ç¼©ï¼‰
        scales[~important_mask] = 0.1 + 0.4 * (importance[~important_mask] / threshold)

        # åº”ç”¨ç¼©æ”¾
        scaled_weight = weight * scales.unsqueeze(1)

        return scaled_weight, scales
```

### ğŸ“Š AWQ vs GPTQå¯¹æ¯”

| ç‰¹æ€§ | AWQ | GPTQ |
|------|-----|------|
| é‡åŒ–é€Ÿåº¦ | å¿«ï¼ˆåˆ†é’Ÿçº§ï¼‰ | æ…¢ï¼ˆå°æ—¶çº§ï¼‰ |
| ç²¾åº¦ä¿æŒ | ä¼˜ç§€ | è‰¯å¥½ |
| è®¡ç®—å¤æ‚åº¦ | O(n) | O(nÂ²) |
| å†…å­˜éœ€æ±‚ | ä½ | é«˜ |
| é€‚ç”¨åœºæ™¯ | å¿«é€Ÿéƒ¨ç½² | é«˜ç²¾åº¦è¦æ±‚ |

---

## ğŸ”¬ AQLM/HQQ/SPQRå…ˆè¿›é‡åŒ–ç®—æ³•

### ğŸ—ï¸ AQLMé‡åŒ–æŠ€æœ¯

**Additive Quantization Language Model**ï¼š

```python
class AQLMQuantizer:
    """
    AQLM (Additive Quantization Language Model) é‡åŒ–å™¨

    æ ¸å¿ƒæ€æƒ³ï¼š
    1. å°†æƒé‡çŸ©é˜µåˆ†è§£ä¸ºç æœ¬å’Œç¼–ç 
    2. ä½¿ç”¨åŠ æ€§é‡åŒ–é‡å»ºæƒé‡
    3. æä½çš„æ¯”ç‰¹ç‡ï¼ˆ2-3 bitï¼‰

    æ•°å­¦è¡¨ç¤ºï¼š
    W â‰ˆ Î£ c_i * C_i
    å…¶ä¸­ c_i æ˜¯ç¼–ç ï¼ŒC_i æ˜¯ç æœ¬
    """
    def __init__(self, num_codebooks=4, codebook_size=256):
        self.num_codebooks = num_codebooks
        self.codebook_size = codebook_size

    def train_codebooks(self, weight, num_iterations=100):
        """
        è®­ç»ƒAQLMç æœ¬
        """
        out_features, in_features = weight.shape

        # åˆå§‹åŒ–ç æœ¬
        codebooks = []
        codes = []

        for i in range(self.num_codebooks):
            # ä½¿ç”¨K-meansåˆå§‹åŒ–ç æœ¬
            flattened_weight = weight.flatten()
            centroids = flattened_weight[torch.randperm(len(flattened_weight))[:self.codebook_size]]
            codebook = centroids.reshape(self.codebook_size, -1)
            codebooks.append(codebook)

        # è®­ç»ƒç æœ¬
        for iteration in range(num_iterations):
            total_error = 0

            for i in range(self.num_codebooks):
                # è®¡ç®—å½“å‰ç æœ¬çš„åˆ†é…
                current_codebook = codebooks[i]
                distances = torch.cdist(weight.flatten().unsqueeze(1), current_codebook.unsqueeze(1))
                assignments = torch.argmin(distances, dim=1)

                # æ›´æ–°ç æœ¬
                for j in range(self.codebook_size):
                    mask = assignments == j
                    if mask.sum() > 0:
                        codebooks[i][j] = weight.flatten()[mask].mean()

                # è®¡ç®—é‡å»ºè¯¯å·®
                reconstructed = current_codebook[assignments].reshape(weight.shape)
                error = torch.norm(weight - reconstructed)
                total_error += error

            if iteration % 10 == 0:
                print(f"Iteration {iteration}, Error: {total_error.item():.4f}")

        return codebooks
```

### ğŸ”§ HQQæ··åˆé‡åŒ–

**Hybrid Quantization Quality**ï¼š

```python
class HQQuantizer:
    """
    HQQ (Hybrid Quantization Quality) æ··åˆé‡åŒ–å™¨

    ç‰¹æ€§ï¼š
    1. ä¸åŒå±‚ä½¿ç”¨ä¸åŒé‡åŒ–ç²¾åº¦
    2. æ•æ„Ÿå±‚ä½¿ç”¨é«˜ç²¾åº¦
    3. ä¸æ•æ„Ÿå±‚ä½¿ç”¨ä½ç²¾åº¦
    4. è‡ªåŠ¨ç²¾åº¦é€‰æ‹©
    """
    def __init__(self):
        self.layer_sensitivity = {}

    def compute_layer_sensitivity(self, model, calib_data):
        """
        è®¡ç®—å„å±‚çš„é‡åŒ–æ•æ„Ÿåº¦
        """
        model.eval()
        baseline_output = None

        # è·å–åŸºå‡†è¾“å‡º
        for batch in calib_data:
            inputs = batch.to(device)
            with torch.no_grad():
                baseline_output = model(inputs)
            break

        # è®¡ç®—å„å±‚æ•æ„Ÿåº¦
        for name, module in model.named_modules():
            if isinstance(module, torch.nn.Linear):
                # é‡åŒ–å½“å‰å±‚
                original_weight = module.weight.data.clone()
                quantized_weight = torch.clamp(
                    torch.round(original_weight * 127) / 127,
                    -1, 1
                )
                module.weight.data = quantized_weight

                # è®¡ç®—é‡åŒ–åçš„è¾“å‡º
                with torch.no_grad():
                    quantized_output = model(inputs)

                # è®¡ç®—æ•æ„Ÿåº¦
                sensitivity = torch.norm(baseline_output - quantized_output)
                self.layer_sensitivity[name] = sensitivity.item()

                # æ¢å¤åŸå§‹æƒé‡
                module.weight.data = original_weight

        return self.layer_sensitivity

    def hybrid_quantize(self, model):
        """
        åº”ç”¨æ··åˆé‡åŒ–ç­–ç•¥
        """
        # æ ¹æ®æ•æ„Ÿåº¦æ’åºå±‚
        sorted_layers = sorted(self.layer_sensitivity.items(), key=lambda x: x[1], reverse=True)

        # é«˜ç²¾åº¦å±‚ï¼ˆå‰20%ï¼‰
        high_precision_layers = [name for name, _ in sorted_layers[:len(sorted_layers)//5]]

        # ä¸­ç²¾åº¦å±‚ï¼ˆä¸­é—´60%ï¼‰
        medium_precision_layers = [name for name, _ in sorted_layers[len(sorted_layers)//5:4*len(sorted_layers)//5]]

        # ä½ç²¾åº¦å±‚ï¼ˆå20%ï¼‰
        low_precision_layers = [name for name, _ in sorted_layers[4*len(sorted_layers)//5:]]

        # åº”ç”¨é‡åŒ–
        for name, module in model.named_modules():
            if isinstance(module, torch.nn.Linear):
                if name in high_precision_layers:
                    # 8ä½é‡åŒ–
                    scale = torch.abs(module.weight).max() / 127
                    quantized_weight = torch.clamp(torch.round(module.weight / scale), -127, 127)
                elif name in medium_precision_layers:
                    # 4ä½é‡åŒ–
                    scale = torch.abs(module.weight).max() / 7
                    quantized_weight = torch.clamp(torch.round(module.weight / scale), -7, 7)
                else:
                    # 2ä½é‡åŒ–
                    scale = torch.abs(module.weight).max() / 1
                    quantized_weight = torch.clamp(torch.round(module.weight / scale), -1, 1)

                module.weight.data = quantized_weight * scale
```

### ğŸ¯ SPQRç¨€ç–é‡åŒ–

**Sparse-Quantized Representation**ï¼š

```python
class SPQRQuantizer:
    """
    SPQR (Sparse-Quantized Representation) ç¨€ç–é‡åŒ–å™¨

    æ ¸å¿ƒæ€æƒ³ï¼š
    1. è¯†åˆ«å’Œç§»é™¤ä¸é‡è¦çš„æƒé‡
    2. å¯¹å‰©ä½™æƒé‡è¿›è¡Œé‡åŒ–
    3. å®ç°å‹ç¼©ç‡å’Œç²¾åº¦çš„å¹³è¡¡
    """
    def __init__(self, sparsity_ratio=0.5, quant_bits=4):
        self.sparsity_ratio = sparsity_ratio
        self.quant_bits = quant_bits

    def spqr_quantize(self, weight):
        """
        SPQRé‡åŒ–å®ç°
        """
        # è®¡ç®—æƒé‡é‡è¦æ€§
        importance = torch.abs(weight)

        # ç¡®å®šå‰ªæé˜ˆå€¼
        threshold = torch.quantile(importance.flatten(), self.sparsity_ratio)

        # åˆ›å»ºç¨€ç–æ©ç 
        sparse_mask = importance > threshold

        # åº”ç”¨ç¨€ç–åŒ–
        sparse_weight = weight * sparse_mask

        # å¯¹éé›¶æƒé‡è¿›è¡Œé‡åŒ–
        nonzero_weights = sparse_weight[sparse_mask]
        if nonzero_weights.numel() > 0:
            max_val = torch.abs(nonzero_weights).max()
            scale = max_val / (2 ** (self.quant_bits - 1) - 1)
            quantized_nonzero = torch.clamp(
                torch.round(nonzero_weights / scale),
                -2 ** (self.quant_bits - 1),
                2 ** (self.quant_bits - 1) - 1
            )
            quantized_nonzero = quantized_nonzero * scale

            # é‡å»ºæƒé‡çŸ©é˜µ
            quantized_weight = torch.zeros_like(weight)
            quantized_weight[sparse_mask] = quantized_nonzero
        else:
            quantized_weight = torch.zeros_like(weight)

        return quantized_weight, sparse_mask, scale
```

---

## ğŸ“ é‡åŒ–æ„ŸçŸ¥è®­ç»ƒä¸å¾®è°ƒ

### ğŸ—ï¸ é‡åŒ–æ„ŸçŸ¥è®­ç»ƒåŸç†

**Quantization-Aware Training (QAT)**ï¼š

```python
class QuantizationAwareTraining:
    """
    é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ¨¡æ‹Ÿé‡åŒ–æ•ˆæœ

    æ ¸å¿ƒæ€æƒ³ï¼š
    1. åœ¨å‰å‘ä¼ æ’­ä¸­æ’å…¥é‡åŒ–æ“ä½œ
    2. åå‘ä¼ æ’­æ—¶ä½¿ç”¨ç›´é€šä¼°è®¡å™¨
    3. æ¢¯åº¦æµåŠ¨ä¿æŒè¿ç»­æ€§
    """
    def __init__(self, model, quant_bits=8):
        self.model = model
        self.quant_bits = quant_bits
        self.quantized_layers = {}

    def apply_qat(self):
        """
        åº”ç”¨é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ
        """
        for name, module in self.model.named_modules():
            if isinstance(module, torch.nn.Linear):
                # æ›¿æ¢ä¸ºé‡åŒ–æ„ŸçŸ¥çº¿æ€§å±‚
                quantized_layer = QuantizedLinear(
                    module.in_features,
                    module.out_features,
                    module.bias is not None,
                    quant_bits=self.quant_bits
                )
                quantized_layer.weight.data = module.weight.data
                if module.bias is not None:
                    quantized_layer.bias.data = module.bias.data

                # æ›¿æ¢åŸå±‚
                parent_name, child_name = name.rsplit('.', 1)
                parent = dict(self.model.named_modules())[parent_name]
                setattr(parent, child_name, quantized_layer)

class QuantizedLinear(torch.nn.Module):
    """
    é‡åŒ–æ„ŸçŸ¥çº¿æ€§å±‚
    """
    def __init__(self, in_features, out_features, bias=True, quant_bits=8):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.quant_bits = quant_bits

        self.weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))
        if bias:
            self.bias = torch.nn.Parameter(torch.Tensor(out_features))
        else:
            self.register_parameter('bias', None)

        # é‡åŒ–å‚æ•°
        self.weight_scale = torch.nn.Parameter(torch.ones(1))
        self.weight_zero_point = torch.nn.Parameter(torch.zeros(1))

        self.reset_parameters()

    def forward(self, input):
        # é‡åŒ–æƒé‡
        quantized_weight = self.quantize_weight(self.weight)

        # åé‡åŒ–æƒé‡ï¼ˆç”¨äºè®¡ç®—ï¼‰
        dequantized_weight = self.dequantize_weight(quantized_weight)

        # æ ‡å‡†çº¿æ€§è®¡ç®—
        output = torch.nn.functional.linear(input, dequantized_weight, self.bias)

        return output

    def quantize_weight(self, weight):
        """
        æƒé‡é‡åŒ–ï¼ˆä½¿ç”¨ç›´é€šä¼°è®¡å™¨ï¼‰
        """
        # è®¡ç®—é‡åŒ–å‚æ•°
        weight_abs_max = torch.abs(weight).max()
        scale = weight_abs_max / (2 ** (self.quant_bits - 1) - 1)

        # é‡åŒ–
        quantized_weight = torch.clamp(
            torch.round(weight / scale),
            -2 ** (self.quant_bits - 1),
            2 ** (self.quant_bits - 1) - 1
        )

        # ç›´é€šä¼°è®¡å™¨ï¼šä¿æŒæ¢¯åº¦æµåŠ¨
        quantized_weight = weight + (quantized_weight - weight).detach()

        return quantized_weight

    def dequantize_weight(self, quantized_weight):
        """
        æƒé‡åé‡åŒ–
        """
        weight_abs_max = torch.abs(self.weight).max()
        scale = weight_abs_max / (2 ** (self.quant_bits - 1) - 1)
        return quantized_weight * scale
```

### ğŸ”§ é‡åŒ–å¾®è°ƒç­–ç•¥

**Post-Training Quantization Fine-tuning**ï¼š

```python
class QuantizationFineTuning:
    """
    é‡åŒ–å¾®è°ƒï¼šåœ¨é‡åŒ–åè¿›è¡Œå°‘é‡è®­ç»ƒæ¢å¤ç²¾åº¦
    """
    def __init__(self, model, learning_rate=1e-5):
        self.model = model
        self.learning_rate = learning_rate

    def fine_tune(self, train_data, epochs=3):
        """
        æ‰§è¡Œé‡åŒ–å¾®è°ƒ
        """
        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)
        criterion = torch.nn.CrossEntropyLoss()

        self.model.train()

        for epoch in range(epochs):
            total_loss = 0

            for batch_idx, batch in enumerate(train_data):
                inputs, targets = batch

                # å‰å‘ä¼ æ’­
                outputs = self.model(inputs)
                loss = criterion(outputs, targets)

                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                total_loss += loss.item()

                if batch_idx % 100 == 0:
                    print(f"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}")

            avg_loss = total_loss / len(train_data)
            print(f"Epoch {epoch} completed, Average Loss: {avg_loss:.4f}")

        return self.model
```

### ğŸ“Š é‡åŒ–è®­ç»ƒä¼˜åŒ–æŠ€æœ¯

**æ··åˆç²¾åº¦é‡åŒ–è®­ç»ƒ**ï¼š

```python
class MixedPrecisionQuantization:
    """
    æ··åˆç²¾åº¦é‡åŒ–è®­ç»ƒ

    ç­–ç•¥ï¼š
    1. å…³é”®å±‚ä½¿ç”¨é«˜ç²¾åº¦ï¼ˆ16ä½ï¼‰
    2. æ™®é€šå±‚ä½¿ç”¨ä½ç²¾åº¦ï¼ˆ8ä½ï¼‰
    3. åŠ¨æ€è°ƒæ•´ç²¾åº¦
    """
    def __init__(self, model):
        self.model = model
        self.layer_precision = {}

    def assign_precision(self, sensitivity_scores):
        """
        åŸºäºæ•æ„Ÿåº¦åˆ†é…é‡åŒ–ç²¾åº¦
        """
        # æŒ‰æ•æ„Ÿåº¦æ’åº
        sorted_layers = sorted(sensitivity_scores.items(), key=lambda x: x[1], reverse=True)

        # é«˜ç²¾åº¦å±‚ï¼ˆæ•æ„Ÿåº¦å‰30%ï¼‰
        high_precision = set([name for name, _ in sorted_layers[:int(len(sorted_layers) * 0.3)]])

        # ä¸­ç²¾åº¦å±‚ï¼ˆæ•æ„Ÿåº¦30-70%ï¼‰
        medium_precision = set([name for name, _ in sorted_layers[int(len(sorted_layers) * 0.3):int(len(sorted_layers) * 0.7)]])

        # ä½ç²¾åº¦å±‚ï¼ˆæ•æ„Ÿåº¦å30%ï¼‰
        low_precision = set([name for name, _ in sorted_layers[int(len(sorted_layers) * 0.7):]])

        for name, module in self.model.named_modules():
            if isinstance(module, torch.nn.Linear):
                if name in high_precision:
                    precision = 16
                elif name in medium_precision:
                    precision = 8
                else:
                    precision = 4

                self.layer_precision[name] = precision

                # åº”ç”¨ç›¸åº”ç²¾åº¦çš„é‡åŒ–
                self.apply_quantization(module, precision)

    def apply_quantization(self, module, precision):
        """
        åº”ç”¨æŒ‡å®šç²¾åº¦çš„é‡åŒ–
        """
        if precision == 16:
            # FP16é‡åŒ–
            module.weight.data = module.weight.data.half()
            if module.bias is not None:
                module.bias.data = module.bias.data.half()
        elif precision == 8:
            # INT8é‡åŒ–
            scale = torch.abs(module.weight).max() / 127
            quantized_weight = torch.clamp(torch.round(module.weight / scale), -127, 127)
            module.weight.data = quantized_weight * scale
        elif precision == 4:
            # INT4é‡åŒ–
            scale = torch.abs(module.weight).max() / 7
            quantized_weight = torch.clamp(torch.round(module.weight / scale), -7, 7)
            module.weight.data = quantized_weight * scale
```

---

## ğŸ—œï¸ æ¨¡å‹å‹ç¼©æŠ€æœ¯ç»¼åˆåˆ†æ

### ğŸ—ï¸ çŸ¥è¯†è’¸é¦

**Knowledge Distillation**ï¼š

```python
class KnowledgeDistillation:
    """
    çŸ¥è¯†è’¸é¦ï¼šç”¨å¤§æ¨¡å‹æŒ‡å¯¼å°æ¨¡å‹è®­ç»ƒ

    æ ¸å¿ƒæ€æƒ³ï¼š
    1. æ•™å¸ˆæ¨¡å‹äº§ç”Ÿè½¯æ ‡ç­¾
    2. å­¦ç”Ÿæ¨¡å‹å­¦ä¹ è½¯æ ‡ç­¾
    3. æ¸©åº¦å‚æ•°è°ƒæ•´æ¦‚ç‡åˆ†å¸ƒ
    """
    def __init__(self, teacher_model, student_model, temperature=4.0, alpha=0.3):
        self.teacher_model = teacher_model
        self.student_model = student_model
        self.temperature = temperature
        self.alpha = alpha  # è’¸é¦æŸå¤±æƒé‡

    def distillation_loss(self, student_outputs, teacher_outputs, targets):
        """
        è®¡ç®—è’¸é¦æŸå¤±
        """
        # å­¦ç”Ÿæ¨¡å‹çš„äº¤å‰ç†µæŸå¤±
        ce_loss = torch.nn.functional.cross_entropy(student_outputs, targets)

        # KLæ•£åº¦æŸå¤±ï¼ˆè½¯æ ‡ç­¾æŸå¤±ï¼‰
        soft_teacher = torch.nn.functional.softmax(teacher_outputs / self.temperature, dim=1)
        soft_student = torch.nn.functional.log_softmax(student_outputs / self.temperature, dim=1)
        kl_loss = torch.nn.functional.kl_div(soft_student, soft_teacher, reduction='batchmean')

        # æ€»æŸå¤±
        total_loss = (1 - self.alpha) * ce_loss + self.alpha * (self.temperature ** 2) * kl_loss

        return total_loss, ce_loss, kl_loss

    def train_student(self, train_data, epochs=10):
        """
        è®­ç»ƒå­¦ç”Ÿæ¨¡å‹
        """
        optimizer = torch.optim.Adam(self.student_model.parameters(), lr=1e-4)

        self.teacher_model.eval()
        self.student_model.train()

        for epoch in range(epochs):
            total_loss = 0
            ce_losses = []
            kl_losses = []

            for batch in train_data:
                inputs, targets = batch

                # æ•™å¸ˆæ¨¡å‹å‰å‘ä¼ æ’­
                with torch.no_grad():
                    teacher_outputs = self.teacher_model(inputs)

                # å­¦ç”Ÿæ¨¡å‹å‰å‘ä¼ æ’­
                student_outputs = self.student_model(inputs)

                # è®¡ç®—è’¸é¦æŸå¤±
                loss, ce_loss, kl_loss = self.distillation_loss(student_outputs, teacher_outputs, targets)

                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                total_loss += loss.item()
                ce_losses.append(ce_loss.item())
                kl_losses.append(kl_loss.item())

            avg_loss = total_loss / len(train_data)
            avg_ce_loss = sum(ce_losses) / len(ce_losses)
            avg_kl_loss = sum(kl_losses) / len(kl_losses)

            print(f"Epoch {epoch}: Total Loss: {avg_loss:.4f}, CE Loss: {avg_ce_loss:.4f}, KL Loss: {avg_kl_loss:.4f}")

        return self.student_model
```

### ğŸ”§ æƒé‡å‰ªæ

**Weight Pruning**ï¼š

```python
class WeightPruning:
    """
    æƒé‡å‰ªæï¼šç§»é™¤ä¸é‡è¦çš„æƒé‡è¿æ¥

    æ–¹æ³•ï¼š
    1. å¹…åº¦å‰ªæï¼šç§»é™¤å°æƒé‡
    2. ç»“æ„åŒ–å‰ªæï¼šç§»é™¤æ•´ä¸ªé€šé“/ç¥ç»å…ƒ
    3. æ¸è¿›å¼å‰ªæï¼šé€æ­¥å¢åŠ å‰ªæç‡
    """
    def __init__(self, model, pruning_method='magnitude'):
        self.model = model
        self.pruning_method = pruning_method
        self.pruning_masks = {}

    def magnitude_pruning(self, layer, pruning_ratio):
        """
        å¹…åº¦å‰ªæï¼šåŸºäºæƒé‡å¹…åº¦å‰ªæ
        """
        weight = layer.weight.data
        threshold = torch.quantile(torch.abs(weight).flatten(), pruning_ratio)

        # åˆ›å»ºå‰ªææ©ç 
        mask = torch.abs(weight) > threshold

        # åº”ç”¨å‰ªæ
        layer.weight.data = weight * mask

        return mask

    def structured_pruning(self, layer, pruning_ratio, dim=0):
        """
        ç»“æ„åŒ–å‰ªæï¼šå‰ªææ•´ä¸ªé€šé“æˆ–ç¥ç»å…ƒ
        """
        weight = layer.weight.data

        # è®¡ç®—æ¯ä¸ªé€šé“çš„é‡è¦æ€§
        if dim == 0:  # è¾“å‡ºé€šé“
            importance = torch.norm(weight, dim=(1, 2)) if weight.dim() == 4 else torch.norm(weight, dim=1)
        else:  # è¾“å…¥é€šé“
            importance = torch.norm(weight, dim=(0, 2)) if weight.dim() == 4 else torch.norm(weight, dim=0)

        # ç¡®å®šå‰ªæé˜ˆå€¼
        threshold = torch.quantile(importance, pruning_ratio)

        # åˆ›å»ºå‰ªææ©ç 
        mask = importance > threshold

        # åº”ç”¨å‰ªæ
        if dim == 0:
            weight[~mask, :] = 0
        else:
            weight[:, ~mask] = 0

        return mask

    def iterative_pruning(self, train_data, initial_ratio=0.2, final_ratio=0.8, iterations=10):
        """
        æ¸è¿›å¼å‰ªæï¼šé€æ­¥å¢åŠ å‰ªæç‡
        """
        for i in range(iterations):
            # è®¡ç®—å½“å‰å‰ªæç‡
            current_ratio = initial_ratio + (final_ratio - initial_ratio) * (i / (iterations - 1))

            print(f"Iteration {i + 1}, Pruning ratio: {current_ratio:.2%}")

            # å¯¹æ¯ä¸ªå±‚è¿›è¡Œå‰ªæ
            for name, layer in self.model.named_modules():
                if isinstance(layer, torch.nn.Conv2d) or isinstance(layer, torch.nn.Linear):
                    if name not in self.pruning_masks:
                        self.pruning_masks[name] = torch.ones_like(layer.weight.data)

                    # åº”ç”¨å‰ªæ
                    mask = self.magnitude_pruning(layer, current_ratio)
                    self.pruning_masks[name] = mask

            # å¾®è°ƒæ¢å¤ç²¾åº¦
            self.fine_tune_after_pruning(train_data, epochs=1)

        return self.model

    def fine_tune_after_pruning(self, train_data, epochs=3):
        """
        å‰ªæåçš„å¾®è°ƒ
        """
        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-4)
        criterion = torch.nn.CrossEntropyLoss()

        self.model.train()

        for epoch in range(epochs):
            total_loss = 0

            for batch in train_data:
                inputs, targets = batch

                outputs = self.model(inputs)
                loss = criterion(outputs, targets)

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                total_loss += loss.item()

            avg_loss = total_loss / len(train_data)
            print(f"Fine-tuning Epoch {epoch}, Loss: {avg_loss:.4f}")

        return self.model
```

### ğŸ¯ ä½ç§©åˆ†è§£

**Low-Rank Factorization**ï¼š

```python
class LowRankFactorization:
    """
    ä½ç§©åˆ†è§£ï¼šå°†å¤§æƒé‡çŸ©é˜µåˆ†è§£ä¸ºå°çŸ©é˜µä¹˜ç§¯

    æ–¹æ³•ï¼š
    1. SVDåˆ†è§£ï¼šå¥‡å¼‚å€¼åˆ†è§£
    2. Tuckeråˆ†è§£ï¼šé«˜é˜¶å¼ é‡åˆ†è§£
    3. CPåˆ†è§£ï¼šCANDECOMP/PARAFACåˆ†è§£
    """
    def __init__(self, model, rank_ratio=0.5):
        self.model = model
        self.rank_ratio = rank_ratio

    def svd_factorization(self, weight, rank_ratio):
        """
        SVDåˆ†è§£å› å­åŒ–
        """
        # æ‰§è¡ŒSVDåˆ†è§£
        U, S, V = torch.svd(weight)

        # ç¡®å®šä¿ç•™çš„ç§©
        rank = int(rank_ratio * min(weight.shape))
        rank = max(1, rank)  # è‡³å°‘ä¿ç•™ç§©1

        # æˆªæ–­SVD
        U_trunc = U[:, :rank]
        S_trunc = torch.diag(S[:rank])
        V_trunc = V[:, :rank].t()

        # åˆ†è§£ä¸ºä¸¤ä¸ªçŸ©é˜µ
        W1 = U_trunc @ torch.sqrt(S_trunc)
        W2 = torch.sqrt(S_trunc) @ V_trunc

        return W1, W2

    def apply_low_rank(self, layer):
        """
        å¯¹å±‚åº”ç”¨ä½ç§©åˆ†è§£
        """
        if isinstance(layer, torch.nn.Linear):
            weight = layer.weight.data
            bias = layer.bias.data if layer.bias is not None else None

            # SVDåˆ†è§£
            W1, W2 = self.svd_factorization(weight, self.rank_ratio)

            # åˆ›å»ºä¸¤ä¸ªçº¿æ€§å±‚
            in_features = layer.in_features
            out_features = layer.out_features
            rank = W1.shape[1]

            # ç¬¬ä¸€ä¸ªçº¿æ€§å±‚
            linear1 = torch.nn.Linear(in_features, rank, bias=False)
            linear1.weight.data = W2

            # ç¬¬äºŒä¸ªçº¿æ€§å±‚
            linear2 = torch.nn.Linear(rank, out_features, bias=bias is not None)
            linear2.weight.data = W1
            if bias is not None:
                linear2.bias.data = bias

            return torch.nn.Sequential(linear1, linear2)

        return layer

    def factorize_model(self):
        """
        å¯¹æ•´ä¸ªæ¨¡å‹åº”ç”¨ä½ç§©åˆ†è§£
        """
        for name, module in self.model.named_modules():
            if isinstance(module, torch.nn.Linear):
                # æ‰¾åˆ°çˆ¶æ¨¡å—å¹¶æ›¿æ¢
                parent_name, child_name = name.rsplit('.', 1)
                parent = dict(self.model.named_modules())[parent_name]

                # åº”ç”¨ä½ç§©åˆ†è§£
                factorized_layer = self.apply_low_rank(module)
                setattr(parent, child_name, factorized_layer)

        return self.model
```

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”ä¸æœ€ä½³å®è·µ

### ğŸ† é‡åŒ–æŠ€æœ¯ç»¼åˆå¯¹æ¯”

| é‡åŒ–æŠ€æœ¯ | å†…å­˜èŠ‚çœ | è®¡ç®—åŠ é€Ÿ | ç²¾åº¦ä¿æŒ | å®ç°å¤æ‚åº¦ | é€‚ç”¨åœºæ™¯ |
|---------|---------|---------|---------|-----------|---------|
| BitsAndBytes INT8 | 4x | 2-3x | 99% | ä½ | é€šç”¨éƒ¨ç½² |
| BitsAndBytes NF4 | 8x | 4-6x | 95-98% | ä¸­ç­‰ | å†…å­˜å—é™ |
| GPTQ 4-bit | 8x | 4-6x | 96-99% | é«˜ | é«˜ç²¾åº¦æ¨ç† |
| AWQ 4-bit | 8x | 4-6x | 97-99% | ä¸­ç­‰ | å¿«é€Ÿéƒ¨ç½² |
| AQLM 2-bit | 16x | 6-8x | 90-95% | é«˜ | æé™å‹ç¼© |
| HQQ æ··åˆ | 6-10x | 3-5x | 96-98% | ä¸­ç­‰ | å¹³è¡¡åœºæ™¯ |
| SPQR ç¨€ç– | 10-20x | 5-10x | 92-96% | é«˜ | è¶…é«˜å‹ç¼© |

### ğŸ¯ åœºæ™¯åŒ–ä¼˜åŒ–å»ºè®®

**ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²**ï¼š
```python
# ç”Ÿäº§ç¯å¢ƒé‡åŒ–é…ç½®
production_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
)

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    quantization_config=production_config,
    device_map="auto",
)
```

**è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²**ï¼š
```python
# è¾¹ç¼˜è®¾å¤‡é‡åŒ–é…ç½®
edge_config = GPTQConfig(
    bits=4,
    group_size=128,
    desc_act=False,
    sym=True,
)

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    device_map="auto",
    quantization_config=edge_config,
)
```

**ç ”ç©¶å®éªŒç¯å¢ƒ**ï¼š
```python
# ç ”ç©¶ç¯å¢ƒæ··åˆé‡åŒ–
research_config = HQQConfig(
    nbits=4,
    group_size=64,
    quant_zero=False,
    quant_scale=False,
    offload_meta=False,
)

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    device_map="auto",
    quantization_config=research_config,
)
```

### ğŸ“ˆ æ€§èƒ½æµ‹è¯•åŸºå‡†

**LLaMA 2 7Bæ¨¡å‹é‡åŒ–æ€§èƒ½**ï¼š

| é‡åŒ–æ–¹æ³• | æ¨¡å‹å¤§å° | å†…å­˜å ç”¨ | æ¨ç†é€Ÿåº¦ | Perplexity |
|---------|---------|---------|---------|------------|
| FP32 | 26GB | 26GB | 15 tok/s | 5.82 |
| INT8 | 7.2GB | 7.2GB | 42 tok/s | 5.85 |
| NF4 | 3.8GB | 3.8GB | 85 tok/s | 6.12 |
| GPTQ 4-bit | 3.8GB | 3.8GB | 92 tok/s | 5.98 |
| AWQ 4-bit | 3.8GB | 3.8GB | 88 tok/s | 6.05 |
| AQLM 2-bit | 2.1GB | 2.1GB | 125 tok/s | 6.89 |

**ç¡¬ä»¶è¦æ±‚å¯¹æ¯”**ï¼š
- FP32ï¼šéœ€è¦A100 40GB
- INT8ï¼šéœ€è¦RTX 3090 24GB
- NF4ï¼šéœ€è¦RTX 3060 12GB
- 4-bitï¼šéœ€è¦GTX 1660 6GB

---

## ğŸ’» å®æˆ˜ä»£ç ç¤ºä¾‹

### ğŸš€ å®Œæ•´é‡åŒ–æµç¨‹

```python
import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    GPTQConfig,
    AWQConfig,
    TrainingArguments,
    Trainer
)
from datasets import load_dataset

def comprehensive_quantization_pipeline():
    """
    ç»¼åˆé‡åŒ–ç®¡é“ï¼šä»æ¨¡å‹é€‰æ‹©åˆ°éƒ¨ç½²ä¼˜åŒ–
    """
    # 1. åŠ è½½æ¨¡å‹å’Œtokenizer
    model_name = "meta-llama/Llama-2-7b-hf"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token

    # 2. å‡†å¤‡æ ¡å‡†æ•°æ®
    calib_dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="train[:1000]")
    calib_texts = calib_dataset["text"][:100]  # ä½¿ç”¨100ä¸ªæ ·æœ¬æ ¡å‡†

    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            max_length=512,
            padding="max_length",
            return_tensors="pt"
        )

    calib_dataset = calib_dataset.map(tokenize_function, batched=True, remove_columns=["text"])

    # 3. BitsAndBytesé‡åŒ–ç¤ºä¾‹
    print("=== BitsAndBytes NF4é‡åŒ– ===")
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_storage=torch.bfloat16,
    )

    bnb_model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        device_map="auto",
    )

    # æµ‹è¯•æ€§èƒ½
    bnb_performance = test_model_performance(bnb_model, tokenizer, calib_texts[:10])
    print(f"BitsAndBytes NF4 - æ¨ç†é€Ÿåº¦: {bnb_performance['speed']:.2f} tok/s")
    print(f"BitsAndBytes NF4 - å†…å­˜å ç”¨: {bnb_performance['memory']:.2f} GB")

    # 4. GPTQé‡åŒ–ç¤ºä¾‹
    print("\n=== GPTQé‡åŒ– ===")
    gptq_config = GPTQConfig(
        bits=4,
        group_size=128,
        desc_act=False,
        sym=True,
    )

    # éœ€è¦å…ˆå®‰è£…optimumå’Œauto-gptq
    try:
        from optimum.gptq import GPTQQuantizer

        gptq_quantizer = GPTQQuantizer.from_pretrained(model_name, save_dir="./gptq_model")
        gptq_model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map="auto",
            quantization_config=gptq_config,
        )

        gptq_performance = test_model_performance(gptq_model, tokenizer, calib_texts[:10])
        print(f"GPTQ 4-bit - æ¨ç†é€Ÿåº¦: {gptq_performance['speed']:.2f} tok/s")
        print(f"GPTQ 4-bit - å†…å­˜å ç”¨: {gptq_performance['memory']:.2f} GB")

    except ImportError:
        print("GPTQéœ€è¦å®‰è£…optimumå’Œauto-gptqåº“")

    # 5. AWQé‡åŒ–ç¤ºä¾‹
    print("\n=== AWQé‡åŒ– ===")
    try:
        from awq import AutoAWQForCausalLM

        awq_model = AutoAWQForCausalLM.from_pretrained(
            model_name,
            safetensors=True,
            device_map="auto"
        )

        # é‡åŒ–æ¨¡å‹
        awq_model.quantize(
            tokenizer,
            quant_config={"zero_point": True, "q_group_size": 128, "w_bit": 4, "version": "GEMM"}
        )

        awq_performance = test_model_performance(awq_model, tokenizer, calib_texts[:10])
        print(f"AWQ 4-bit - æ¨ç†é€Ÿåº¦: {awq_performance['speed']:.2f} tok/s")
        print(f"AWQ 4-bit - å†…å­˜å ç”¨: {awq_performance['memory']:.2f} GB")

    except ImportError:
        print("AWQéœ€è¦å®‰è£…awqåº“")

    # 6. é‡åŒ–æ„ŸçŸ¥è®­ç»ƒç¤ºä¾‹
    print("\n=== é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ ===")
    qat_model = create_qat_model(model_name)
    qat_trainer = train_qat_model(qat_model, calib_dataset)

    return {
        'bnb_model': bnb_model,
        'bnb_performance': bnb_performance,
        'gptq_model': gptq_model if 'gptq_model' in locals() else None,
        'gptq_performance': gptq_performance if 'gptq_performance' in locals() else None,
        'awq_model': awq_model if 'awq_model' in locals() else None,
        'awq_performance': awq_performance if 'awq_performance' in locals() else None,
        'qat_model': qat_model,
    }

def test_model_performance(model, tokenizer, test_texts):
    """
    æµ‹è¯•æ¨¡å‹æ€§èƒ½
    """
    import time
    import psutil
    import torch

    model.eval()
    total_tokens = 0
    total_time = 0

    # å†…å­˜ä½¿ç”¨
    if torch.cuda.is_available():
        memory_used = torch.cuda.memory_allocated() / 1024**3  # GB
    else:
        memory_used = psutil.Process().memory_info().rss / 1024**3  # GB

    # æ¨ç†é€Ÿåº¦æµ‹è¯•
    with torch.no_grad():
        for text in test_texts:
            inputs = tokenizer(text, return_tensors="pt").to(model.device)

            start_time = time.time()
            outputs = model.generate(**inputs, max_length=100)
            end_time = time.time()

            generated_tokens = outputs.shape[1] - inputs.shape[1]
            total_tokens += generated_tokens
            total_time += (end_time - start_time)

    speed = total_tokens / total_time if total_time > 0 else 0

    return {
        'speed': speed,
        'memory': memory_used,
        'total_tokens': total_tokens,
        'total_time': total_time
    }

def create_qat_model(model_name):
    """
    åˆ›å»ºé‡åŒ–æ„ŸçŸ¥è®­ç»ƒæ¨¡å‹
    """
    model = AutoModelForCausalLM.from_pretrained(model_name)

    # æ›¿æ¢çº¿æ€§å±‚ä¸ºé‡åŒ–æ„ŸçŸ¥å±‚
    for name, module in model.named_modules():
        if isinstance(module, torch.nn.Linear):
            qat_layer = QuantizedLinear(
                module.in_features,
                module.out_features,
                module.bias is not None,
                quant_bits=8
            )
            qat_layer.weight.data = module.weight.data
            if module.bias is not None:
                qat_layer.bias.data = module.bias.data

            # æ›¿æ¢å±‚
            parent_name, child_name = name.rsplit('.', 1)
            parent = dict(model.named_modules())[parent_name]
            setattr(parent, child_name, qat_layer)

    return model

def train_qat_model(model, train_dataset):
    """
    è®­ç»ƒé‡åŒ–æ„ŸçŸ¥æ¨¡å‹
    """
    training_args = TrainingArguments(
        output_dir="./qat_results",
        num_train_epochs=3,
        per_device_train_batch_size=4,
        gradient_accumulation_steps=8,
        learning_rate=1e-5,
        fp16=True,
        logging_steps=10,
        save_steps=100,
        evaluation_strategy="no",
    )

    def data_collator(features):
        batch = {}
        batch['input_ids'] = torch.stack([f['input_ids'] for f in features])
        batch['attention_mask'] = torch.stack([f['attention_mask'] for f in features])
        batch['labels'] = batch['input_ids'].clone()
        return batch

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        data_collator=data_collator,
    )

    trainer.train()

    return model

# è¿è¡Œé‡åŒ–ç®¡é“
if __name__ == "__main__":
    results = comprehensive_quantization_pipeline()
    print("\n=== é‡åŒ–ç»“æœæ±‡æ€» ===")
    for method, performance in results.items():
        if performance and isinstance(performance, dict):
            print(f"{method}: é€Ÿåº¦ {performance.get('speed', 0):.2f} tok/s, å†…å­˜ {performance.get('memory', 0):.2f} GB")
```

### ğŸ”§ è‡ªå®šä¹‰é‡åŒ–å™¨

```python
class CustomQuantizer:
    """
    è‡ªå®šä¹‰é‡åŒ–å™¨ï¼šç»“åˆå¤šç§é‡åŒ–æŠ€æœ¯
    """
    def __init__(self, config):
        self.config = config
        self.quantization_stats = {}

    def mixed_precision_quantize(self, model):
        """
        æ··åˆç²¾åº¦é‡åŒ–ï¼šä¸åŒå±‚ä½¿ç”¨ä¸åŒç²¾åº¦
        """
        layer_importance = self.compute_layer_importance(model)

        for name, module in model.named_modules():
            if isinstance(module, torch.nn.Linear):
                importance = layer_importance.get(name, 0.5)

                # æ ¹æ®é‡è¦æ€§é€‰æ‹©é‡åŒ–ç²¾åº¦
                if importance > 0.8:
                    # é«˜é‡è¦æ€§å±‚ï¼š8ä½é‡åŒ–
                    bits = 8
                elif importance > 0.5:
                    # ä¸­ç­‰é‡è¦æ€§å±‚ï¼š4ä½é‡åŒ–
                    bits = 4
                else:
                    # ä½é‡è¦æ€§å±‚ï¼š2ä½é‡åŒ–
                    bits = 2

                # åº”ç”¨é‡åŒ–
                quantized_module = self.quantize_layer(module, bits)
                self.replace_layer(model, name, quantized_module)

                # è®°å½•ç»Ÿè®¡ä¿¡æ¯
                self.quantization_stats[name] = {
                    'bits': bits,
                    'importance': importance,
                    'original_size': module.weight.numel() * 4,  # å‡è®¾FP32
                    'quantized_size': module.weight.numel() * bits / 8,
                }

        return model

    def compute_layer_importance(self, model):
        """
        è®¡ç®—å±‚é‡è¦æ€§
        """
        importance_scores = {}

        for name, module in model.named_modules():
            if isinstance(module, torch.nn.Linear):
                # åŸºäºæƒé‡èŒƒæ•°çš„é‡è¦æ€§
                weight_norm = torch.norm(module.weight).item()
                output_size = module.out_features

                # ç»¼åˆé‡è¦æ€§è¯„åˆ†
                importance = (weight_norm * output_size) / (module.in_features * module.out_features)
                importance_scores[name] = importance

        # å½’ä¸€åŒ–åˆ°[0, 1]
        max_importance = max(importance_scores.values())
        min_importance = min(importance_scores.values())
        range_importance = max_importance - min_importance

        for name in importance_scores:
            importance_scores[name] = (importance_scores[name] - min_importance) / range_importance

        return importance_scores

    def quantize_layer(self, layer, bits):
        """
        é‡åŒ–å•ä¸ªå±‚
        """
        weight = layer.weight.data
        max_val = torch.abs(weight).max()

        # è®¡ç®—ç¼©æ”¾å› å­
        if bits == 8:
            scale = max_val / 127
            quant_levels = 255
        elif bits == 4:
            scale = max_val / 7
            quant_levels = 15
        elif bits == 2:
            scale = max_val / 1
            quant_levels = 3
        else:
            return layer  # ä¸é‡åŒ–

        # é‡åŒ–
        quantized_weight = torch.clamp(
            torch.round(weight / scale),
            -quant_levels // 2,
            quant_levels // 2
        )

        # åé‡åŒ–
        dequantized_weight = quantized_weight * scale

        # åˆ›å»ºæ–°å±‚
        quantized_layer = torch.nn.Linear(
            layer.in_features,
            layer.out_features,
            bias=layer.bias is not None
        )
        quantized_layer.weight.data = dequantized_weight
        if layer.bias is not None:
            quantized_layer.bias.data = layer.bias.data

        return quantized_layer

    def replace_layer(self, model, layer_name, new_layer):
        """
        æ›¿æ¢æ¨¡å‹ä¸­çš„å±‚
        """
        parent_name, child_name = layer_name.rsplit('.', 1)
        parent = dict(model.named_modules())[parent_name]
        setattr(parent, child_name, new_layer)

    def get_quantization_summary(self):
        """
        è·å–é‡åŒ–ç»Ÿè®¡æ‘˜è¦
        """
        total_original = sum(stats['original_size'] for stats in self.quantization_stats.values())
        total_quantized = sum(stats['quantized_size'] for stats in self.quantization_stats.values())

        summary = {
            'total_compression_ratio': total_original / total_quantized,
            'memory_saving_percent': (1 - total_quantized / total_original) * 100,
            'layer_count': len(self.quantization_stats),
            'bits_distribution': {}
        }

        # ç»Ÿè®¡æ¯”ç‰¹ä½åˆ†å¸ƒ
        for stats in self.quantization_stats.values():
            bits = stats['bits']
            if bits not in summary['bits_distribution']:
                summary['bits_distribution'][bits] = 0
            summary['bits_distribution'][bits] += 1

        return summary

# ä½¿ç”¨è‡ªå®šä¹‰é‡åŒ–å™¨
def custom_quantization_example():
    """
    è‡ªå®šä¹‰é‡åŒ–ç¤ºä¾‹
    """
    model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")

    # åˆ›å»ºè‡ªå®šä¹‰é‡åŒ–å™¨é…ç½®
    config = {
        'target_compression_ratio': 6.0,  # ç›®æ ‡å‹ç¼©æ¯”
        'min_bits': 2,
        'max_bits': 8,
    }

    quantizer = CustomQuantizer(config)

    # åº”ç”¨æ··åˆç²¾åº¦é‡åŒ–
    quantized_model = quantizer.mixed_precision_quantize(model)

    # è·å–é‡åŒ–æ‘˜è¦
    summary = quantizer.get_quantization_summary()

    print("=== è‡ªå®šä¹‰é‡åŒ–ç»“æœ ===")
    print(f"å‹ç¼©æ¯”: {summary['total_compression_ratio']:.2f}x")
    print(f"å†…å­˜èŠ‚çœ: {summary['memory_saving_percent']:.1f}%")
    print(f"é‡åŒ–å±‚æ•°: {summary['layer_count']}")
    print(f"æ¯”ç‰¹ä½åˆ†å¸ƒ: {summary['bits_distribution']}")

    return quantized_model, summary
```

---

## ğŸ¯ æ€»ç»“ä¸å±•æœ›

### ğŸ† å…³é”®æŠ€æœ¯æ€»ç»“

1. **BitsAndBytes**ï¼šæä¾›äº†æˆç†Ÿçš„8ä½å’Œ4ä½é‡åŒ–æ–¹æ¡ˆï¼Œæ˜“äºä½¿ç”¨ä¸”æ•ˆæœç¨³å®š
2. **GPTQ**ï¼šåŸºäºHessiançš„åè®­ç»ƒé‡åŒ–ï¼Œç²¾åº¦é«˜ä½†è®¡ç®—å¤æ‚
3. **AWQ**ï¼šæ¿€æ´»æ„ŸçŸ¥é‡åŒ–ï¼Œå¹³è¡¡äº†ç²¾åº¦å’Œé€Ÿåº¦
4. **AQLM/HQQ/SPQR**ï¼šæ–°å…´é‡åŒ–æŠ€æœ¯ï¼Œå„æœ‰ç‰¹è‰²ä¼˜åŠ¿
5. **é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ¨¡æ‹Ÿé‡åŒ–ï¼Œæ•ˆæœæœ€ä½³ä½†éœ€è¦è®­ç»ƒæ•°æ®

### ğŸš€ æŠ€æœ¯å‘å±•è¶‹åŠ¿

1. **æ›´ä½æ¯”ç‰¹ç‡**ï¼šä»4ä½å‘2ä½ç”šè‡³1ä½é‡åŒ–å‘å±•
2. **æ··åˆç²¾åº¦**ï¼šä¸åŒå±‚ä½¿ç”¨ä¸åŒç²¾åº¦ï¼Œä¼˜åŒ–å‹ç¼©æ¯”å’Œç²¾åº¦å¹³è¡¡
3. **ç¡¬ä»¶ååŒ**ï¼šé’ˆå¯¹ç‰¹å®šç¡¬ä»¶ä¼˜åŒ–çš„é‡åŒ–ç®—æ³•
4. **è‡ªé€‚åº”é‡åŒ–**ï¼šæ ¹æ®è¾“å…¥ç‰¹æ€§åŠ¨æ€è°ƒæ•´é‡åŒ–ç­–ç•¥
5. **å¤šæ¨¡æ€é‡åŒ–**ï¼šç»Ÿä¸€æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘çš„é‡åŒ–æ¡†æ¶

### ğŸ’¡ æœ€ä½³å®è·µå»ºè®®

1. **å¿«é€Ÿéƒ¨ç½²**ï¼šä¼˜å…ˆä½¿ç”¨BitsAndBytes NF4æˆ–AWQ
2. **é«˜ç²¾åº¦è¦æ±‚**ï¼šé€‰æ‹©GPTQæˆ–é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ
3. **æé™å‹ç¼©**ï¼šè€ƒè™‘AQLMæˆ–SPQR
4. **ç”Ÿäº§ç¯å¢ƒ**ï¼šè¿›è¡Œå®Œæ•´çš„ç²¾åº¦å’Œæ€§èƒ½æµ‹è¯•
5. **æŒç»­ä¼˜åŒ–**ï¼šç›‘æ§é‡åŒ–æ•ˆæœï¼ŒåŠ¨æ€è°ƒæ•´ç­–ç•¥

### ğŸ”® æœªæ¥ç ”ç©¶æ–¹å‘

1. **ç¥ç»æ¶æ„æœç´¢**ï¼šè‡ªåŠ¨å‘ç°æœ€ä¼˜é‡åŒ–ç­–ç•¥
2. **å¯å¾®åˆ†é‡åŒ–**ï¼šç«¯åˆ°ç«¯çš„é‡åŒ–ä¼˜åŒ–
3. **ç»ˆèº«å­¦ä¹ é‡åŒ–**ï¼šé€‚åº”æ¨¡å‹æŒç»­æ›´æ–°çš„é‡åŒ–æ–¹æ³•
4. **è”é‚¦å­¦ä¹ é‡åŒ–**ï¼šéšç§ä¿æŠ¤çš„åˆ†å¸ƒå¼é‡åŒ–
5. **ç”Ÿç‰©å¯å‘é‡åŒ–**ï¼šæ¨¡ä»¿ç¥ç»ç³»ç»Ÿçš„ç¨€ç–ç¼–ç 

é€šè¿‡è¿™äº›é‡åŒ–æŠ€æœ¯ï¼ŒHuggingFace Transformersåº“è®©å¤§å‹è¯­è¨€æ¨¡å‹çš„æ°‘ä¸»åŒ–æˆä¸ºå¯èƒ½ï¼Œä½¿å¾—æ›´å¹¿æ³›çš„å¼€å‘è€…å’Œç ”ç©¶äººå‘˜èƒ½å¤Ÿåœ¨æœ‰é™çš„ç¡¬ä»¶èµ„æºä¸Šä½¿ç”¨å’Œéƒ¨ç½²æœ€å…ˆè¿›çš„AIæ¨¡å‹ã€‚

**ğŸ“š ç»§ç»­é˜…è¯»**ï¼š
- ä¸‹ä¸€èŠ‚ï¼š[åˆ†å¸ƒå¼è®­ç»ƒä¸å¤§è§„æ¨¡éƒ¨ç½²](./07_distributed_training.md)
- ä¸Šä¸€èŠ‚ï¼š[æ³¨æ„åŠ›æœºåˆ¶ä¼˜åŒ–æŠ€æœ¯å…¨è§£](./05_attention_optimization_techniques.md)

---

*æœ¬æ–‡åŸºäºHuggingFace Transformersåº“çš„æœ€æ–°æºç åˆ†æï¼ŒæŠ€æœ¯ç»†èŠ‚å¯èƒ½éšç‰ˆæœ¬æ›´æ–°è€Œå˜åŒ–ã€‚å»ºè®®åœ¨å®é™…ä½¿ç”¨æ—¶å‚è€ƒå®˜æ–¹æ–‡æ¡£å’Œæœ€æ–°æºç ã€‚*