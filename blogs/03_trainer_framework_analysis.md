# ğŸ”¥ HuggingFace Transformersåº“æ·±åº¦è§£æç³»åˆ—ï¼ˆä¸‰ï¼‰ï¼šTrainerè®­ç»ƒæ¡†æ¶æºç æ·±åº¦å‰–æ

> ä½œä¸ºOpenAIçš„æŠ€æœ¯æ¶æ„å¸ˆï¼Œä»Šå¤©æˆ‘å°†æ·±å…¥å‰–æTransformersåº“çš„Trainerè®­ç»ƒæ¡†æ¶ã€‚è¿™æ˜¯ç°ä»£æ·±åº¦å­¦ä¹ è®­ç»ƒç³»ç»Ÿçš„å…¸èŒƒï¼Œé›†æˆäº†åˆ†å¸ƒå¼è®­ç»ƒã€æ··åˆç²¾åº¦ã€æ¨¡å‹ä¿å­˜ç­‰ä¼ä¸šçº§åŠŸèƒ½ã€‚æœ¬æ–‡å°†ä»æºç å±‚é¢å½»åº•è§£æå…¶è®¾è®¡ç²¾é«“ã€‚

## ğŸ“‹ ç›®å½•

- [Traineræ¡†æ¶çš„æ ¸å¿ƒè®¾è®¡ç†å¿µ](#traineræ¡†æ¶çš„æ ¸å¿ƒè®¾è®¡ç†å¿µ)
- [æ•´ä½“æ¶æ„ä¸ç»„ä»¶å…³ç³»](#æ•´ä½“æ¶æ„ä¸ç»„ä»¶å…³ç³»)
- [è®­ç»ƒå¾ªç¯çš„å®Œæ•´å®ç°](#è®­ç»ƒå¾ªç¯çš„å®Œæ•´å®ç°)
- [åˆ†å¸ƒå¼è®­ç»ƒæœºåˆ¶æ·±åº¦å‰–æ](#åˆ†å¸ƒå¼è®­ç»ƒæœºåˆ¶æ·±åº¦å‰–æ)
- [æ··åˆç²¾åº¦è®­ç»ƒå®ç°](#æ··åˆç²¾åº¦è®­ç»ƒå®ç°)
- [æ¢¯åº¦ç´¯ç§¯ä¸ä¼˜åŒ–å™¨é›†æˆ](#æ¢¯åº¦ç´¯ç§¯ä¸ä¼˜åŒ–å™¨é›†æˆ)
- [æ¨¡å‹ä¿å­˜ä¸åŠ è½½æœºåˆ¶](#æ¨¡å‹ä¿å­˜ä¸åŠ è½½æœºåˆ¶)
- [è¯„ä¼°ä¸éªŒè¯ç³»ç»Ÿ](#è¯„ä¼°ä¸éªŒè¯ç³»ç»Ÿ)
- [å›è°ƒç³»ç»Ÿä¸æ‰©å±•æœºåˆ¶](#å›è°ƒç³»ç»Ÿä¸æ‰©å±•æœºåˆ¶)
- [å†…å­˜ç®¡ç†ä¸ä¼˜åŒ–ç­–ç•¥](#å†…å­˜ç®¡ç†ä¸ä¼˜åŒ–ç­–ç•¥)
- [é”™è¯¯å¤„ç†ä¸æ¢å¤æœºåˆ¶](#é”™è¯¯å¤„ç†ä¸æ¢å¤æœºåˆ¶)
- [å®æˆ˜ä»£ç ç¤ºä¾‹](#å®æˆ˜ä»£ç ç¤ºä¾‹)
- [æ€§èƒ½è°ƒä¼˜æœ€ä½³å®è·µ](#æ€§èƒ½è°ƒä¼˜æœ€ä½³å®è·µ)
- [æ€»ç»“ä¸å±•æœ›](#æ€»ç»“ä¸å±•æœ›)

---

## ğŸ¯ Traineræ¡†æ¶çš„æ ¸å¿ƒè®¾è®¡ç†å¿µ

Traineræ¡†æ¶æ˜¯Transformersåº“çš„**è®­ç»ƒå¼•æ“**ï¼Œå…¶è®¾è®¡ç†å¿µä½“ç°äº†ç°ä»£æ·±åº¦å­¦ä¹ è®­ç»ƒç³»ç»Ÿçš„æœ€ä½³å®è·µã€‚

### ğŸ—ï¸ è®¾è®¡åŸåˆ™

#### 1. **é…ç½®é©±åŠ¨è®¾è®¡**
é€šè¿‡`TrainingArguments`ç»Ÿä¸€ç®¡ç†æ‰€æœ‰è®­ç»ƒå‚æ•°ï¼š
```python
# training_args.py:50-100
@dataclass
class TrainingArguments:
    """
    è®­ç»ƒå‚æ•°é…ç½®ç±»
    """
    output_dir: str = field(
        default=None,
        metadata={"help": "è¾“å‡ºç›®å½•"}
    )
    overwrite_output_dir: bool = field(
        default=False,
        metadata={"help": "è¦†ç›–è¾“å‡ºç›®å½•"}
    )
    do_train: bool = field(
        default=False,
        metadata={"help": "æ˜¯å¦è¿›è¡Œè®­ç»ƒ"}
    )
    do_eval: bool = field(
        default=False,
        metadata={"help": "æ˜¯å¦è¿›è¡Œè¯„ä¼°"}
    )
    do_predict: bool = field(
        default=False,
        metadata={"help": "æ˜¯å¦è¿›è¡Œé¢„æµ‹"}
    )
    # ... æ›´å¤šå‚æ•°
```

#### 2. **æ’ä»¶åŒ–æ¶æ„**
é€šè¿‡å›è°ƒæœºåˆ¶å®ç°åŠŸèƒ½æ‰©å±•ï¼š
```python
# trainer_callback.py:100-150
class TrainerCallback:
    """
    è®­ç»ƒå›è°ƒåŸºç±»
    """
    def on_init_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):
        """
        è®­ç»ƒåˆå§‹åŒ–ç»“æŸæ—¶è°ƒç”¨
        """
        pass

    def on_train_begin(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):
        """
        è®­ç»ƒå¼€å§‹æ—¶è°ƒç”¨
        """
        pass

    def on_train_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):
        """
        è®­ç»ƒç»“æŸæ—¶è°ƒç”¨
        """
        pass

    def on_epoch_begin(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):
        """
        Epochå¼€å§‹æ—¶è°ƒç”¨
        """
        pass

    def on_epoch_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):
        """
        Epochç»“æŸæ—¶è°ƒç”¨
        """
        pass

    def on_step_begin(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):
        """
        è®­ç»ƒæ­¥å¼€å§‹æ—¶è°ƒç”¨
        """
        pass

    def on_step_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):
        """
        è®­ç»ƒæ­¥ç»“æŸæ—¶è°ƒç”¨
        """
        pass
```

#### 3. **çŠ¶æ€ç®¡ç†**
é€šè¿‡`TrainerState`ç»´æŠ¤è®­ç»ƒçŠ¶æ€ï¼š
```python
# trainer_callback.py:200-300
@dataclass
class TrainerState:
    """
    è®­ç»ƒçŠ¶æ€ç®¡ç†ç±»
    """
    epoch: float = field(default=0.0, metadata={"help": "å½“å‰epoch"})
    global_step: int = field(default=0, metadata={"help": "å…¨å±€æ­¥æ•°"})
    max_steps: int = field(default=0, metadata={"help": "æœ€å¤§æ­¥æ•°"})
    num_train_epochs: int = field(default=0, metadata={"help": "è®­ç»ƒepochæ•°"})
    total_flos: float = field(default=0, metadata={"help": "æ€»æµ®ç‚¹è¿ç®—æ•°"})
    log_history: List[Dict[str, float]] = field(
        default_factory=list, metadata={"help": "è®­ç»ƒæ—¥å¿—å†å²"}
    )
    best_metric: Optional[float] = field(
        default=None, metadata={"help": "æœ€ä½³æŒ‡æ ‡"}
    )
    best_model_checkpoint: Optional[str] = field(
        default=None, metadata={"help": "æœ€ä½³æ¨¡å‹æ£€æŸ¥ç‚¹"}
    )
    is_local_process_zero: bool = field(
        default=True, metadata={"help": "æ˜¯å¦ä¸ºä¸»è¿›ç¨‹"}
    )
    is_world_process_zero: bool = field(
        default=True, metadata={"help": "æ˜¯å¦ä¸ºå…¨å±€ä¸»è¿›ç¨‹"}
    )
```

---

## ğŸ—ºï¸ æ•´ä½“æ¶æ„ä¸ç»„ä»¶å…³ç³»

### ğŸ“Š Traineræ¶æ„å›¾

```mermaid
graph TB
    subgraph "ç”¨æˆ·æ¥å£å±‚"
        A[Trainerå®ä¾‹åŒ–]
        B[è®­ç»ƒå‚æ•°é…ç½®]
        C[æ•°æ®é›†å‡†å¤‡]
    end

    subgraph "æ ¸å¿ƒè®­ç»ƒå±‚"
        D[Trainerç±»]
        E[TrainingArguments]
        F[TrainerState]
        G[TrainerControl]
    end

    subgraph "æ•°æ®å¤„ç†å±‚"
        H[DataLoader]
        I[DataCollator]
        J[Dataset]
    end

    subgraph "ä¼˜åŒ–å±‚"
        K[Optimizer]
        L[Scheduler]
        M[Gradient Accumulation]
    end

    subgraph "åˆ†å¸ƒå¼å±‚"
        N[DistributedDataParallel]
        O[DeepSpeed]
        P[FSDP]
    end

    subgraph "ç›‘æ§å±‚"
        Q[Callback System]
        R[Metrics Tracking]
        S[Logging]
    end

    subgraph "æŒä¹…åŒ–å±‚"
        T[Model Checkpointing]
        U[State Saving]
        V[Export Utilities]
    end

    A --> D
    B --> E
    C --> J
    D --> F
    D --> G
    D --> H
    H --> I
    I --> J
    D --> K
    K --> L
    L --> M
    D --> N
    N --> O
    N --> P
    D --> Q
    Q --> R
    R --> S
    D --> T
    T --> U
    U --> V
```

---

## ğŸ”„ è®­ç»ƒå¾ªç¯çš„å®Œæ•´å®ç°

è®©æˆ‘ä»¬æ·±å…¥åˆ†æTrainerçš„æ ¸å¿ƒè®­ç»ƒå¾ªç¯å®ç°ï¼š

### ğŸ“ ä¸»è¦è®­ç»ƒå¾ªç¯

```python
# trainer.py:1500-2000
def train(self, resume_from_checkpoint=None, trial=None, ignore_keys_for_eval=None):
    """
    ä¸»è¦è®­ç»ƒæ–¹æ³•
    """
    # 1. è®­ç»ƒå‰æ£€æŸ¥å’Œåˆå§‹åŒ–
    self._wrap_model(self.model, training=True)

    total_train_batch_size = (
        self.args.train_batch_size
        * self.args.gradient_accumulation_steps
        * self.args.world_size
    )

    # 2. è®¡ç®—æ€»è®­ç»ƒæ­¥æ•°
    num_update_steps_per_epoch = (
        len(self.train_dataset) // total_train_batch_size
    )
    if self.args.max_steps > 0:
        num_training_steps = self.args.max_steps
        num_train_epochs = self.args.max_steps // num_update_steps_per_epoch + int(
            self.args.max_steps % num_update_steps_per_epoch > 0
        )
    else:
        num_training_steps = num_update_steps_per_epoch * self.args.num_train_epochs
        num_train_epochs = self.args.num_train_epochs

    # 3. åˆ›å»ºè¿›åº¦æ¡
    epoch_iterator = self.get_train_dataloader()
    progress_bar = tqdm(
        range(num_training_steps),
        disable=not self.is_local_process_zero(),
        desc="Training",
    )

    # 4. åˆå§‹åŒ–ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    self.create_optimizer_and_scheduler(num_training_steps=num_training_steps)

    # 5. è®­ç»ƒå¾ªç¯
    for epoch in range(num_train_epochs):
        # 5.1 Epochå¼€å§‹å›è°ƒ
        self.control = self.callback_handler.on_epoch_begin(
            self.args, self.state, self.control
        )

        # 5.2 è®­ç»ƒæ•°æ®è¿­ä»£
        for step, inputs in enumerate(epoch_iterator):
            # è·³è¿‡å·²å®Œæˆæ­¥éª¤ï¼ˆç”¨äºæ¢å¤è®­ç»ƒï¼‰
            if self.state.global_step >= num_training_steps:
                return

            # 5.3 è®­ç»ƒæ­¥å¼€å§‹å›è°ƒ
            self.control = self.callback_handler.on_step_begin(
                self.args, self.state, self.control
            )

            # 5.4 æ¢¯åº¦æ¸…é›¶
            self.model.zero_grad()

            # 5.5 æ¢¯åº¦ç´¯ç§¯å¾ªç¯
            for micro_step in range(self.args.gradient_accumulation_steps):
                # å‰å‘ä¼ æ’­
                outputs = self.training_step(model, inputs)

                # è®¡ç®—æŸå¤±
                loss = outputs.loss

                # åå‘ä¼ æ’­
                if self.args.gradient_accumulation_steps > 1:
                    loss = loss / self.args.gradient_accumulation_steps

                if self.use_apex:
                    with amp.scale_loss(loss, self.optimizer) as scaled_loss:
                        scaled_loss.backward()
                else:
                    self.accelerator.backward(loss)

            # 5.6 æ¢¯åº¦è£å‰ª
            if self.args.max_grad_norm is not None:
                if self.use_apex:
                    torch.nn.utils.clip_grad_norm_(
                        amp.master_params(self.optimizer),
                        self.args.max_grad_norm,
                    )
                else:
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(),
                        self.args.max_grad_norm,
                    )

            # 5.7 ä¼˜åŒ–å™¨æ­¥è¿›
            self.optimizer.step()

            # 5.8 å­¦ä¹ ç‡è°ƒåº¦
            if not self.lr_scheduler_called_by_deepspeed:
                self.lr_scheduler.step()

            # 5.9 æ›´æ–°çŠ¶æ€
            self.state.global_step += 1
            self.state.epoch = epoch + step / len(epoch_iterator)

            # 5.10 è®­ç»ƒæ­¥ç»“æŸå›è°ƒ
            self.control = self.callback_handler.on_step_end(
                self.args, self.state, self.control
            )

            # 5.11 æ—¥å¿—è®°å½•
            self._maybe_log_save_evaluate(trial, epoch, ignore_keys_for_eval)

            # 5.12 æ£€æŸ¥æ˜¯å¦éœ€è¦æå‰ç»ˆæ­¢
            if self.control.should_training_stop:
                break

        # 5.13 Epochç»“æŸå›è°ƒ
        self.control = self.callback_handler.on_epoch_end(
            self.args, self.state, self.control
        )

        # 5.14 æ£€æŸ¥æ˜¯å¦éœ€è¦æå‰ç»ˆæ­¢
        if self.control.should_training_stop:
            break

    # 6. è®­ç»ƒç»“æŸå›è°ƒ
    self.callback_handler.on_train_end(
        self.args, self.state, self.control
    )

    return TrainOutput(self.state.global_step, self.state.training_loss)
```

### ğŸ“ è®­ç»ƒæ­¥å®ç°

```python
# trainer.py:1000-1200
def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:
    """
    å•ä¸ªè®­ç»ƒæ­¥çš„å®ç°
    """
    # 1. æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
    inputs = self._prepare_inputs(inputs)

    # 2. è®¾ç½®è®­ç»ƒæ¨¡å¼
    if self.use_amp:
        with autocast():
            loss = self.compute_loss(model, inputs)
    else:
        loss = self.compute_loss(model, inputs)

    # 3. å¤„ç†å¤šGPUæƒ…å†µ
    if self.args.n_gpu > 1:
        loss = loss.mean()

    # 4. æ·±åº¦åŠ é€Ÿå™¨å¤„ç†
    if self.do_grad_scaling:
        self.scaler.scale(loss).backward()
    elif self.use_apex:
        with amp.scale_loss(loss, self.optimizer) as scaled_loss:
            scaled_loss.backward()
    else:
        self.backward(loss)

    return loss.detach()
```

### ğŸ“ æŸå¤±è®¡ç®—å®ç°

```python
# trainer.py:800-900
def compute_loss(self, model, inputs, return_outputs=False):
    """
    è®¡ç®—æŸå¤±å‡½æ•°
    """
    # 1. æå–æ ‡ç­¾
    if "labels" in inputs:
        labels = inputs.pop("labels")
    elif self.model.accepts_loss_kwargs:
        labels = None
    else:
        labels = None

    # 2. å‰å‘ä¼ æ’­
    outputs = model(**inputs)

    # 3. ä¿å­˜è¾“å‡º
    if not return_outputs:
        self.save_outputs(outputs)

    # 4. è®¡ç®—æŸå¤±
    if labels is not None:
        loss = self.label_smoother(outputs, labels)
    else:
        # å¦‚æœæ²¡æœ‰æ ‡ç­¾ï¼Œä»è¾“å‡ºä¸­è·å–æŸå¤±
        if isinstance(outputs, dict):
            loss = outputs.get("loss", None)
        elif isinstance(outputs, ModelOutput):
            loss = outputs.loss
        else:
            loss = None

    return (loss, outputs) if return_outputs else loss
```

---

## ğŸŒ åˆ†å¸ƒå¼è®­ç»ƒæœºåˆ¶æ·±åº¦å‰–æ

Traineræ”¯æŒå¤šç§åˆ†å¸ƒå¼è®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬Data Parallelã€DeepSpeedã€FSDPç­‰ã€‚

### ğŸ¯ Data Parallelå®ç°

```python
# trainer.py:2000-2100
def _wrap_model(self, model, training=True):
    """
    åŒ…è£…æ¨¡å‹ä»¥æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒ
    """
    # 1. æ£€æŸ¥æ˜¯å¦ä½¿ç”¨æ·±åº¦åŠ é€Ÿå™¨
    if self.is_deepspeed_enabled:
        # DeepSpeedé›†æˆ
        import deepspeed
        model, optimizer, _, lr_scheduler = deepspeed.initialize(
            model=model,
            optimizer=self.optimizer,
            args=self.args,
            lr_scheduler=self.lr_scheduler,
            dist_init_required=True,
        )
        self.model = model
        self.optimizer = optimizer
        self.lr_scheduler = lr_scheduler

    # 2. æ£€æŸ¥æ˜¯å¦ä½¿ç”¨FSDP
    elif self.is_fsdp_enabled:
        # FSDPé›†æˆ
        from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
        from torch.distributed.fsdp import MixedPrecision
        from torch.distributed.fsdp import BackwardPrefetch

        # FSDPé…ç½®
        fsdp_config = self.args.fsdp_config
        mixed_precision_policy = None
        if fsdp_config.get("mixed_precision", None):
            mixed_precision_policy = MixedPrecision(
                param_dtype=getattr(torch, fsdp_config["mixed_precision"]["param_dtype"]),
                reduce_dtype=getattr(torch, fsdp_config["mixed_precision"]["reduce_dtype"]),
                buffer_dtype=getattr(torch, fsdp_config["mixed_precision"]["buffer_dtype"]),
            )

        # FSDPåŒ…è£…
        self.model = FSDP(
            model,
            process_group=self.process_group,
            mixed_precision=mixed_precision_policy,
            backward_prefetch=BackwardPrefetch.BACKWARD_PRE,
            device_id=torch.cuda.current_device(),
        )

    # 3. æ ‡å‡†Data Parallel
    elif self.args.n_gpu > 1:
        model = torch.nn.DataParallel(model)

    # 4. æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡
    if not self.is_deepspeed_enabled and not self.is_fsdp_enabled:
        model = model.to(self.args.device)

    return model
```

### ğŸ¯ DeepSpeedé›†æˆ

```python
# integrations/deepspeed/__init__.py:100-200
def deepspeed_init(self, num_training_steps: int, **kwargs):
    """
    åˆå§‹åŒ–DeepSpeedé›†æˆ
    """
    if not is_deepspeed_available():
        raise ImportError("DeepSpeed is not available. Please install deepspeed")

    import deepspeed

    # 1. åˆ›å»ºDeepSpeedé…ç½®
    ds_config = self._get_deepspeed_config(num_training_steps)

    # 2. åˆå§‹åŒ–DeepSpeedå¼•æ“
    self.deepspeed_engine, optimizer, _, lr_scheduler = deepspeed.initialize(
        model=self.model,
        optimizer=self.optimizer,
        args=self.args,
        lr_scheduler=self.lr_scheduler,
        config_params=ds_config,
        dist_init_required=True,
    )

    # 3. æ›´æ–°ç»„ä»¶å¼•ç”¨
    self.model = self.deepspeed_engine
    self.optimizer = optimizer
    self.lr_scheduler = lr_scheduler

    return self.deepspeed_engine

def _get_deepspeed_config(self, num_training_steps: int) -> Dict[str, Any]:
    """
    ç”ŸæˆDeepSpeedé…ç½®
    """
    # åŸºç¡€é…ç½®
    ds_config = {
        "train_batch_size": self.args.train_batch_size * self.args.world_size,
        "train_micro_batch_size_per_gpu": self.args.train_batch_size,
        "steps_per_print": 100,
        "zero_optimization": {
            "stage": self.args.deepspeed_zero_stage,
        },
        "fp16": {
            "enabled": self.args.fp16,
        },
    }

    # Stage 2ä¼˜åŒ–
    if self.args.deepspeed_zero_stage >= 2:
        ds_config["zero_optimization"].update({
            "allgather_partitions": True,
            "allgather_bucket_size": 2e8,
            "overlap_comm": True,
            "reduce_scatter": True,
            "reduce_bucket_size": 2e8,
            "contiguous_gradients": True,
        })

    # Stage 3ä¼˜åŒ–
    if self.args.deepspeed_zero_stage >= 3:
        ds_config["zero_optimization"].update({
            "stage3_max_live_parameters": 1e9,
            "stage3_max_reuse_distance": 1e9,
            "stage3_param_persistence_threshold": 1e5,
            "stage3_gather_16bit_weights_on_model_save": True,
        })

    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    if self.lr_scheduler is not None:
        ds_config["scheduler"] = {
            "type": "WarmupLR",
            "params": {
                "warmup_min_lr": 0,
                "warmup_max_lr": self.args.learning_rate,
                "warmup_num_steps": self.args.warmup_steps,
            }
        }

    # ä¼˜åŒ–å™¨é…ç½®
    if self.optimizer is not None:
        ds_config["optimizer"] = {
            "type": "Adam",
            "params": {
                "lr": self.args.learning_rate,
                "betas": [self.args.adam_beta1, self.args.adam_beta2],
                "eps": self.args.adam_epsilon,
                "weight_decay": self.args.weight_decay,
            }
        }

    return ds_config
```

---

## âš¡ æ··åˆç²¾åº¦è®­ç»ƒå®ç°

### ğŸ¯ AMP (Automatic Mixed Precision) å®ç°

```python
# trainer.py:2500-2600
def _setup_amp(self):
    """
    è®¾ç½®è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒ
    """
    if self.args.fp16:
        if not torch.cuda.is_available():
            raise ValueError("FP16 requires CUDA")

        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨Apex
        if self.use_apex:
            from apex import amp

            # åˆå§‹åŒ–Apex AMP
            self.model, self.optimizer = amp.initialize(
                self.model,
                self.optimizer,
                opt_level=self.args.fp16_opt_level,
            )
        else:
            # ä½¿ç”¨PyTorchåŸç”ŸAMP
            self.scaler = torch.cuda.amp.GradScaler()

    elif self.args.bf16:
        if not torch.cuda.is_available() and not torch.backends.mps.is_available():
            raise ValueError("BF16 requires CUDA or MPS")

        # BF16ä¸éœ€è¦ç‰¹æ®Šçš„scaler
        self.use_amp = True
```

### ğŸ¯ æ¢¯åº¦ç¼©æ”¾å®ç°

```python
# trainer.py:2600-2700
def _backward(self, loss):
    """
    åå‘ä¼ æ’­å®ç°
    """
    # 1. å¤„ç†æ¢¯åº¦ç¼©æ”¾
    if self.do_grad_scaling:
        # ä½¿ç”¨GradScalerè¿›è¡Œç¼©æ”¾
        self.scaler.scale(loss).backward()
    elif self.use_apex:
        # ä½¿ç”¨Apexè¿›è¡Œç¼©æ”¾
        with amp.scale_loss(loss, self.optimizer) as scaled_loss:
            scaled_loss.backward()
    else:
        # æ ‡å‡†åå‘ä¼ æ’­
        loss.backward()

def _optimizer_step(self):
    """
    ä¼˜åŒ–å™¨æ­¥è¿›å®ç°
    """
    # 1. å¤„ç†æ¢¯åº¦ç¼©æ”¾
    if self.do_grad_scaling:
        # æ¢¯åº¦åç¼©æ”¾
        self.scaler.unscale_(self.optimizer)

        # æ¢¯åº¦è£å‰ª
        if self.args.max_grad_norm is not None:
            torch.nn.utils.clip_grad_norm_(
                self.model.parameters(),
                self.args.max_grad_norm,
            )

        # ä¼˜åŒ–å™¨æ­¥è¿›
        self.scaler.step(self.optimizer)

        # æ›´æ–°scaler
        self.scaler.update()
    else:
        # æ ‡å‡†ä¼˜åŒ–å™¨æ­¥è¿›
        self.optimizer.step()
```

---

## ğŸ“ˆ æ¢¯åº¦ç´¯ç§¯ä¸ä¼˜åŒ–å™¨é›†æˆ

### ğŸ¯ æ¢¯åº¦ç´¯ç§¯å®ç°

```python
# trainer.py:3000-3100
def gradient_accumulation_steps(self) -> int:
    """
    è·å–æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
    """
    return self.args.gradient_accumulation_steps

def _accumulate_gradients(self, loss):
    """
    æ¢¯åº¦ç´¯ç§¯å®ç°
    """
    # 1. æ ‡å‡†åŒ–æŸå¤±
    loss = loss / self.args.gradient_accumulation_steps

    # 2. åå‘ä¼ æ’­
    if self.use_amp:
        with autocast():
            self.backward(loss)
    else:
        self.backward(loss)

def _should_accumulate(self) -> bool:
    """
    åˆ¤æ–­æ˜¯å¦åº”è¯¥ç´¯ç§¯æ¢¯åº¦
    """
    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç´¯ç§¯æ­¥æ•°
    if self.args.gradient_accumulation_steps > 1:
        return (self.state.global_step + 1) % self.args.gradient_accumulation_steps != 0
    return False
```

### ğŸ¯ ä¼˜åŒ–å™¨åˆ›å»º

```python
# trainer.py:3200-3300
def create_optimizer(self):
    """
    åˆ›å»ºä¼˜åŒ–å™¨
    """
    # 1. è·å–å¯è®­ç»ƒå‚æ•°
    if self.optimizer is None:
        decay_parameters = self.get_parameter_names(self.model, nn.LayerNorm)
        decay_parameters = [name for name in decay_parameters if "bias" not in name]
        optimizer_grouped_parameters = [
            {
                "params": [p for n, p in self.model.named_parameters() if n in decay_parameters],
                "weight_decay": self.args.weight_decay,
            },
            {
                "params": [p for n, p in self.model.named_parameters() if n not in decay_parameters],
                "weight_decay": 0.0,
            },
        ]

        # 2. åˆ›å»ºä¼˜åŒ–å™¨
        if self.args.adafactor:
            from transformers.optimization import Adafactor
            self.optimizer = Adafactor(
                optimizer_grouped_parameters,
                lr=self.args.learning_rate,
                scale_parameter=False,
                relative_step=False,
            )
        else:
            self.optimizer = torch.optim.AdamW(
                optimizer_grouped_parameters,
                lr=self.args.learning_rate,
                betas=(self.args.adam_beta1, self.args.adam_beta2),
                eps=self.args.adam_epsilon,
                weight_decay=self.args.weight_decay,
            )

def create_scheduler(self, num_training_steps: int):
    """
    åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    """
    if self.lr_scheduler is None:
        # 1. åˆ›å»ºè°ƒåº¦å™¨
        if self.args.lr_scheduler_type == "linear":
            from transformers.optimization import get_linear_schedule_with_warmup
            self.lr_scheduler = get_linear_schedule_with_warmup(
                self.optimizer,
                num_warmup_steps=self.args.warmup_steps,
                num_training_steps=num_training_steps,
            )
        elif self.args.lr_scheduler_type == "cosine":
            from transformers.optimization import get_cosine_schedule_with_warmup
            self.lr_scheduler = get_cosine_schedule_with_warmup(
                self.optimizer,
                num_warmup_steps=self.args.warmup_steps,
                num_training_steps=num_training_steps,
            )
        elif self.args.lr_scheduler_type == "polynomial":
            from transformers.optimization import get_polynomial_decay_schedule_with_warmup
            self.lr_scheduler = get_polynomial_decay_schedule_with_warmup(
                self.optimizer,
                num_warmup_steps=self.args.warmup_steps,
                num_training_steps=num_training_steps,
                lr_end=self.args.learning_rate_end,
                power=self.args.power,
            )
```

---

## ğŸ’¾ æ¨¡å‹ä¿å­˜ä¸åŠ è½½æœºåˆ¶

### ğŸ¯ æ¨¡å‹ä¿å­˜å®ç°

```python
# trainer.py:3500-3600
def _save_checkpoint(self, model, trial, metrics=None):
    """
    ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹
    """
    # 1. æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿å­˜
    if not self.is_world_process_zero():
        return

    # 2. åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = self.args.output_dir
    os.makedirs(output_dir, exist_ok=True)

    # 3. ä¿å­˜æ¨¡å‹
    if self.is_deepspeed_enabled:
        # DeepSpeedæ¨¡å‹ä¿å­˜
        self.deepspeed_engine.save_checkpoint(output_dir)
    else:
        # æ ‡å‡†æ¨¡å‹ä¿å­˜
        self.model.save_pretrained(output_dir)

    # 4. ä¿å­˜é…ç½®
    if self.args.should_save:
        torch.save(self.args, os.path.join(output_dir, "training_args.bin"))
        torch.save(self.optimizer.state_dict(), os.path.join(output_dir, "optimizer.pt"))
        torch.save(self.lr_scheduler.state_dict(), os.path.join(output_dir, "scheduler.pt"))

    # 5. ä¿å­˜è®­ç»ƒçŠ¶æ€
    if self.is_world_process_zero():
        state = {
            "epoch": self.state.epoch,
            "global_step": self.state.global_step,
            "best_metric": self.state.best_metric,
            "best_model_checkpoint": self.state.best_model_checkpoint,
        }
        torch.save(state, os.path.join(output_dir, "trainer_state.pt"))

def save_model(self, output_dir: Optional[str] = None, _internal_call: bool = False):
    """
    ä¿å­˜æ¨¡å‹
    """
    if output_dir is None:
        output_dir = self.args.output_dir

    # 1. æ£€æŸ¥æ˜¯å¦ä¸ºåˆ†å¸ƒå¼ä¸»è¿›ç¨‹
    if not self.is_world_process_zero():
        return

    # 2. ä¿å­˜æ¨¡å‹
    self.model.save_pretrained(output_dir)

    # 3. ä¿å­˜tokenizer
    if self.tokenizer is not None:
        self.tokenizer.save_pretrained(output_dir)

    # 4. ä¿å­˜é…ç½®
    if self.args.should_save and not _internal_call:
        torch.save(self.args, os.path.join(output_dir, "training_args.bin"))
```

### ğŸ¯ æ¨¡å‹åŠ è½½å®ç°

```python
# trainer.py:3700-3800
def train(resume_from_checkpoint=None, trial=None, ignore_keys_for_eval=None):
    """
    æ”¯æŒä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
    """
    # 1. å¤„ç†æ£€æŸ¥ç‚¹æ¢å¤
    if resume_from_checkpoint is not None:
        if os.path.isdir(resume_from_checkpoint):
            # ä»ç›®å½•åŠ è½½
            self._load_from_checkpoint(resume_from_checkpoint)
        elif os.path.isfile(resume_from_checkpoint):
            # ä»æ–‡ä»¶åŠ è½½
            self._load_from_file(resume_from_checkpoint)
        else:
            # ä»hubåŠ è½½
            self._load_from_hub(resume_from_checkpoint)

def _load_from_checkpoint(self, checkpoint_dir: str):
    """
    ä»æ£€æŸ¥ç‚¹ç›®å½•åŠ è½½
    """
    # 1. åŠ è½½æ¨¡å‹
    if self.is_deepspeed_enabled:
        import deepspeed
        self.deepspeed_engine.load_checkpoint(checkpoint_dir)
    else:
        # åŠ è½½æ¨¡å‹æƒé‡
        self.model.load_state_dict(
            torch.load(os.path.join(checkpoint_dir, "pytorch_model.bin"), map_location="cpu")
        )

    # 2. åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
    if os.path.exists(os.path.join(checkpoint_dir, "optimizer.pt")):
        self.optimizer.load_state_dict(
            torch.load(os.path.join(checkpoint_dir, "optimizer.pt"), map_location="cpu")
        )

    # 3. åŠ è½½è°ƒåº¦å™¨çŠ¶æ€
    if os.path.exists(os.path.join(checkpoint_dir, "scheduler.pt")):
        self.lr_scheduler.load_state_dict(
            torch.load(os.path.join(checkpoint_dir, "scheduler.pt"), map_location="cpu")
        )

    # 4. åŠ è½½è®­ç»ƒçŠ¶æ€
    if os.path.exists(os.path.join(checkpoint_dir, "trainer_state.pt")):
        state = torch.load(os.path.join(checkpoint_dir, "trainer_state.pt"), map_location="cpu")
        self.state.epoch = state["epoch"]
        self.state.global_step = state["global_step"]
        self.state.best_metric = state.get("best_metric", None)
        self.state.best_model_checkpoint = state.get("best_model_checkpoint", None)
```

---

## ğŸ“Š è¯„ä¼°ä¸éªŒè¯ç³»ç»Ÿ

### ğŸ¯ è¯„ä¼°å¾ªç¯å®ç°

```python
# trainer.py:4000-4100
def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix="eval"):
    """
    è¯„ä¼°æ¨¡å‹æ€§èƒ½
    """
    # 1. å‡†å¤‡è¯„ä¼°æ•°æ®é›†
    if eval_dataset is None:
        eval_dataset = self.eval_dataset

    # 2. åˆ›å»ºè¯„ä¼°æ•°æ®åŠ è½½å™¨
    eval_dataloader = self.get_eval_dataloader(eval_dataset)

    # 3. è¯„ä¼°å‰å›è°ƒ
    self.control = self.callback_handler.on_evaluate(
        self.args, self.state, self.control
    )

    # 4. è¯„ä¼°å¾ªç¯
    for step, inputs in enumerate(eval_dataloader):
        # å‰å‘ä¼ æ’­
        outputs = self.evaluation_step(model, inputs)

        # æ”¶é›†è¾“å‡º
        all_outputs.append(outputs)

    # 5. è®¡ç®—æŒ‡æ ‡
    metrics = self.compute_metrics(
        EvalPrediction(
            predictions=all_predictions,
            label_ids=all_labels,
        )
    )

    # 6. è®°å½•æŒ‡æ ‡
    self.log(metrics)

    # 7. è¯„ä¼°åå›è°ƒ
    self.control = self.callback_handler.on_evaluate(
        self.args, self.state, self.control
    )

    return metrics

def evaluation_step(self, model, inputs):
    """
    è¯„ä¼°æ­¥å®ç°
    """
    # 1. è®¾ç½®è¯„ä¼°æ¨¡å¼
    model.eval()

    # 2. å‡†å¤‡è¾“å…¥
    inputs = self._prepare_inputs(inputs)

    # 3. ç¦ç”¨æ¢¯åº¦è®¡ç®—
    with torch.no_grad():
        # 4. å‰å‘ä¼ æ’­
        if self.use_amp:
            with autocast():
                outputs = model(**inputs)
        else:
            outputs = model(**inputs)

    return outputs
```

### ğŸ¯ æŒ‡æ ‡è®¡ç®—å®ç°

```python
# trainer.py:4200-4300
def compute_metrics(self, eval_pred):
    """
    è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    """
    # 1. æå–é¢„æµ‹å’Œæ ‡ç­¾
    predictions, labels = eval_pred

    # 2. å¤„ç†ä¸åŒä»»åŠ¡çš„æŒ‡æ ‡
    if self.task_name == "classification":
        # åˆ†ç±»ä»»åŠ¡
        from sklearn.metrics import accuracy_score, f1_score
        preds = np.argmax(predictions, axis=1)
        return {
            "accuracy": accuracy_score(labels, preds),
            "f1": f1_score(labels, preds, average="weighted"),
        }
    elif self.task_name == "regression":
        # å›å½’ä»»åŠ¡
        from sklearn.metrics import mean_squared_error, mean_absolute_error
        return {
            "mse": mean_squared_error(labels, predictions),
            "mae": mean_absolute_error(labels, predictions),
        }
    elif self.task_name == "ner":
        # å‘½åå®ä½“è¯†åˆ«
        from seqeval.metrics import classification_report
        preds = np.argmax(predictions, axis=2)
        return classification_report(labels, preds, output_dict=True)
    else:
        return {}
```

---

## ğŸ”„ å›è°ƒç³»ç»Ÿä¸æ‰©å±•æœºåˆ¶

### ğŸ¯ å†…ç½®å›è°ƒå®ç°

```python
# trainer_callback.py:300-400
class PrinterCallback(TrainerCallback):
    """
    æ‰“å°å›è°ƒ
    """
    def on_log(self, args, state, control, logs=None, **kwargs):
        if state.is_local_process_zero:
            print(logs)

class ProgressCallback(TrainerCallback):
    """
    è¿›åº¦æ¡å›è°ƒ
    """
    def __init__(self):
        self.training_bar = None
        self.prediction_bar = None

    def on_train_begin(self, args, state, control, **kwargs):
        if state.is_local_process_zero:
            self.training_bar = tqdm(total=state.max_steps)

    def on_step_end(self, args, state, control, **kwargs):
        if state.is_local_process_zero:
            self.training_bar.update(1)

    def on_train_end(self, args, state, control, **kwargs):
        if state.is_local_process_zero:
            self.training_bar.close()

class EarlyStoppingCallback(TrainerCallback):
    """
    æ—©åœå›è°ƒ
    """
    def __init__(self, early_stopping_patience: int = 1, early_stopping_threshold: float = 0.0):
        self.early_stopping_patience = early_stopping_patience
        self.early_stopping_threshold = early_stopping_threshold
        self.early_stopping_patience_counter = 0

    def on_evaluate(self, args, state, control, metrics, **kwargs):
        metric = args.metric_for_best_model
        if metric is not None:
            if not hasattr(self, "best_metric"):
                self.best_metric = metrics.get(metric)

            # æ£€æŸ¥æ˜¯å¦æ”¹è¿›
            if metrics.get(metric, self.best_metric) - self.best_metric > self.early_stopping_threshold:
                self.best_metric = metrics.get(metric)
                self.early_stopping_patience_counter = 0
            else:
                self.early_stopping_patience_counter += 1

            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ—©åœ
            if self.early_stopping_patience_counter >= self.early_stopping_patience:
                control.should_training_stop = True
```

### ğŸ¯ å›è°ƒå¤„ç†å™¨å®ç°

```python
# trainer.py:4400-4500
class CallbackHandler:
    """
    å›è°ƒå¤„ç†å™¨
    """
    def __init__(self, callbacks, model, optimizer, lr_scheduler):
        self.callbacks = callbacks
        self.model = model
        self.optimizer = optimizer
        self.lr_scheduler = lr_scheduler

        # äº‹ä»¶å¤„ç†å‡½æ•°æ˜ å°„
        self.event_handlers = {
            "on_init_end": [],
            "on_train_begin": [],
            "on_train_end": [],
            "on_epoch_begin": [],
            "on_epoch_end": [],
            "on_step_begin": [],
            "on_step_end": [],
            "on_evaluate": [],
            "on_predict": [],
            "on_save": [],
            "on_log": [],
            "on_train_batch_begin": [],
            "on_train_batch_end": [],
            "on_evaluate_batch_begin": [],
            "on_evaluate_batch_end": [],
            "on_predict_batch_begin": [],
            "on_predict_batch_end": [],
        }

        # æ³¨å†Œå›è°ƒ
        for callback in self.callbacks:
            self.register_callback(callback)

    def register_callback(self, callback):
        """
        æ³¨å†Œå›è°ƒ
        """
        for event_name in self.event_handlers.keys():
            if hasattr(callback, event_name):
                self.event_handlers[event_name].append(getattr(callback, event_name))

    def fire_event(self, event_name, *args, **kwargs):
        """
        è§¦å‘äº‹ä»¶
        """
        for handler in self.event_handlers.get(event_name, []):
            handler(*args, **kwargs)
```

---

## ğŸ’» å®æˆ˜ä»£ç ç¤ºä¾‹

### ğŸ¯ ç¤ºä¾‹1ï¼šè‡ªå®šä¹‰è®­ç»ƒå¾ªç¯

```python
import torch
import torch.nn as nn
from transformers import TrainingArguments, Trainer, TrainerCallback
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from datasets import load_dataset
import numpy as np
from sklearn.metrics import accuracy_score

# 1. åŠ è½½æ•°æ®é›†
dataset = load_dataset("imdb")
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

def preprocess_function(examples):
    return tokenizer(examples["text"], truncation=True, padding=True, max_length=512)

encoded_dataset = dataset.map(preprocess_function, batched=True)

# 2. åŠ è½½æ¨¡å‹
model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=2
)

# 3. è‡ªå®šä¹‰å›è°ƒ
class CustomCallback(TrainerCallback):
    def __init__(self):
        self.train_losses = []
        self.eval_accuracies = []

    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs is not None:
            if "loss" in logs:
                self.train_losses.append(logs["loss"])
            if "eval_accuracy" in logs:
                self.eval_accuracies.append(logs["eval_accuracy"])

    def on_train_end(self, args, state, control, **kwargs):
        print(f"Training completed. Final accuracy: {self.eval_accuracies[-1]:.4f}")

# 4. è‡ªå®šä¹‰æŒ‡æ ‡å‡½æ•°
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return {
        "accuracy": accuracy_score(labels, predictions),
        "f1": f1_score(labels, predictions, average="weighted")
    }

# 5. åˆ›å»ºè®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=10,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    greater_is_better=True,
    fp16=True,  # å¯ç”¨æ··åˆç²¾åº¦
    gradient_accumulation_steps=2,
)

# 6. åˆ›å»ºTrainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=encoded_dataset["train"].shuffle().select(range(1000)),  # ä½¿ç”¨å­é›†è¿›è¡Œæ¼”ç¤º
    eval_dataset=encoded_dataset["test"].shuffle().select(range(200)),
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
    callbacks=[CustomCallback()],
)

# 7. å¼€å§‹è®­ç»ƒ
trainer.train()

# 8. è¯„ä¼°æ¨¡å‹
eval_results = trainer.evaluate()
print(f"Evaluation results: {eval_results}")
```

### ğŸ¯ ç¤ºä¾‹2ï¼šåˆ†å¸ƒå¼è®­ç»ƒé…ç½®

```python
import torch
from transformers import TrainingArguments, Trainer
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
import os

# 1. æ£€æŸ¥åˆ†å¸ƒå¼ç¯å¢ƒ
def setup_distributed_environment():
    """
    è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ
    """
    if torch.cuda.is_available():
        # è®¾ç½®CUDAè®¾å¤‡
        torch.cuda.set_device(int(os.environ.get("LOCAL_RANK", 0)))

        # åˆå§‹åŒ–è¿›ç¨‹ç»„
        if "WORLD_SIZE" in os.environ:
            torch.distributed.init_process_group(backend="nccl")

# 2. DeepSpeedé…ç½®
deepspeed_config = {
    "fp16": {
        "enabled": True,
    },
    "zero_optimization": {
        "stage": 3,
        "offload_param": {
            "device": "cpu",
            "pin_memory": True
        },
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": True
        },
        "stage3_param_persistence_threshold": 1e5,
        "stage3_max_live_parameters": 1e9,
        "stage3_max_reuse_distance": 1e9,
        "gather_16bit_weights_on_model_save": True,
    },
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
    "gradient_accumulation_steps": "auto",
    "gradient_clipping": 1.0,
    "steps_per_print": 100,
    "wall_clock_breakdown": False,
}

# 3. FSDPé…ç½®
fsdp_config = {
    "fsdp": "full_shard",
    "fsdp_config": {
        "min_num_params": 1e6,
        "backward_prefetch": "backward_pre",
        "forward_prefetch": "false",
        "use_orig_params": "false",
    },
}

# 4. åˆ›å»ºè®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./distributed_results",
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    gradient_accumulation_steps=4,
    warmup_steps=100,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    save_strategy="steps",
    fp16=True,
    bf16=False,
    # åˆ†å¸ƒå¼é…ç½®
    local_rank=int(os.environ.get("LOCAL_RANK", 0)),
    deepspeed=deepspeed_config if os.environ.get("USE_DEEPSPEED", "0") == "1" else None,
    fsdp=fsdp_config if os.environ.get("USE_FSDP", "0") == "1" else None,
    # å…¶ä»–é…ç½®
    dataloader_num_workers=4,
    remove_unused_columns=False,
    report_to="tensorboard",
)

# 5. åŠ è½½æ¨¡å‹å’Œæ•°æ®
model = AutoModelForCausalLM.from_pretrained("gpt2")
tokenizer = AutoTokenizer.from_pretrained("gpt2")

# è®¾ç½®pad_token
tokenizer.pad_token = tokenizer.eos_token
model.config.pad_token_id = model.config.eos_token_id

# åŠ è½½æ•°æ®é›†
dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="train")

def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=512)

tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)

# 6. åˆ›å»ºTrainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
)

# 7. å¼€å§‹è®­ç»ƒ
if __name__ == "__main__":
    setup_distributed_environment()
    trainer.train()
```

### ğŸ¯ ç¤ºä¾‹3ï¼šè‡ªå®šä¹‰è®­ç»ƒç­–ç•¥

```python
import torch
import torch.nn as nn
from transformers import TrainingArguments, Trainer, TrainerCallback
from transformers import AdamW, get_linear_schedule_with_warmup
import math

# 1. è‡ªå®šä¹‰ä¼˜åŒ–å™¨
class CustomOptimizer:
    def __init__(self, model, learning_rate, weight_decay=0.01):
        # åˆ†ç¦»æƒé‡è¡°å‡å‚æ•°
        no_decay = ["bias", "LayerNorm.weight"]
        optimizer_grouped_parameters = [
            {
                "params": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],
                "weight_decay": weight_decay,
            },
            {
                "params": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],
                "weight_decay": 0.0,
            },
        ]

        self.optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)

    def step(self):
        self.optimizer.step()
        self.optimizer.zero_grad()

# 2. è‡ªå®šä¹‰å­¦ä¹ ç‡è°ƒåº¦å™¨
class CosineAnnealingWarmRestarts:
    def __init__(self, optimizer, T_0=10, T_mult=2, eta_min=1e-6):
        self.optimizer = optimizer
        self.T_0 = T_0
        self.T_mult = T_mult
        self.eta_min = eta_min
        self.T_cur = 0
        self.cycle = 0

    def step(self):
        if self.T_cur == self.T_0:
            self.cycle += 1
            self.T_0 = self.T_0 * self.T_mult
            self.T_cur = 0

        # ä½™å¼¦é€€ç«
        lr = self.eta_min + (self.optimizer.param_groups[0]["lr"] - self.eta_min) * \
             (1 + math.cos(math.pi * self.T_cur / self.T_0)) / 2

        for param_group in self.optimizer.param_groups:
            param_group["lr"] = lr

        self.T_cur += 1

# 3. è‡ªå®šä¹‰Trainer
class CustomTrainer(Trainer):
    def create_optimizer(self):
        """
        åˆ›å»ºè‡ªå®šä¹‰ä¼˜åŒ–å™¨
        """
        self.custom_optimizer = CustomOptimizer(
            self.model,
            learning_rate=self.args.learning_rate,
            weight_decay=self.args.weight_decay
        )
        self.optimizer = self.custom_optimizer.optimizer

    def create_scheduler(self, num_training_steps):
        """
        åˆ›å»ºè‡ªå®šä¹‰è°ƒåº¦å™¨
        """
        self.custom_scheduler = CosineAnnealingWarmRestarts(
            self.optimizer,
            T_0=10,
            T_mult=2,
            eta_min=self.args.learning_rate * 0.1
        )

    def training_step(self, model, inputs):
        """
        è‡ªå®šä¹‰è®­ç»ƒæ­¥
        """
        model.train()

        inputs = self._prepare_inputs(inputs)

        # è‡ªå®šä¹‰æŸå¤±è®¡ç®—
        outputs = model(**inputs)
        loss = outputs.loss

        if self.args.n_gpu > 1:
            loss = loss.mean()

        if self.args.gradient_accumulation_steps > 1:
            loss = loss / self.args.gradient_accumulation_steps

        # è‡ªå®šä¹‰åå‘ä¼ æ’­
        loss.backward()

        # è‡ªå®šä¹‰ä¼˜åŒ–å™¨æ­¥è¿›
        if self._should_accumulate():
            self.custom_optimizer.step()
            self.custom_scheduler.step()

        return loss.detach()

# 4. ä½¿ç”¨è‡ªå®šä¹‰Trainer
training_args = TrainingArguments(
    output_dir="./custom_results",
    num_train_epochs=5,
    per_device_train_batch_size=16,
    learning_rate=5e-5,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=10,
    save_strategy="epoch",
    evaluation_strategy="epoch",
)

# åˆ›å»ºè‡ªå®šä¹‰Trainerå®ä¾‹
trainer = CustomTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
)

# å¼€å§‹è®­ç»ƒ
trainer.train()
```

---

## ğŸ¯ æ€§èƒ½è°ƒä¼˜æœ€ä½³å®è·µ

### ğŸ”§ å…³é”®ä¼˜åŒ–ç­–ç•¥

#### 1. **å†…å­˜ä¼˜åŒ–**
```python
# å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
model.gradient_checkpointing_enable()

# ä½¿ç”¨æ··åˆç²¾åº¦
training_args = TrainingArguments(
    fp16=True,
    # æˆ–
    bf16=True,
)

# ä½¿ç”¨DeepSpeed ZeRO-3
deepspeed_config = {
    "zero_optimization": {
        "stage": 3,
        "offload_param": {"device": "cpu"},
        "offload_optimizer": {"device": "cpu"},
    }
}
```

#### 2. **è®¡ç®—ä¼˜åŒ–**
```python
# ä¼˜åŒ–æ•°æ®åŠ è½½
training_args = TrainingArguments(
    dataloader_num_workers=4,
    dataloader_pin_memory=True,
    remove_unused_columns=True,
)

# å¯ç”¨Flash Attention
model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    attn_implementation="flash_attention_2"
)
```

#### 3. **é€šä¿¡ä¼˜åŒ–**
```python
# ä¼˜åŒ–åˆ†å¸ƒå¼é€šä¿¡
training_args = TrainingArguments(
    gradient_accumulation_steps=4,
    per_device_train_batch_size=8,
    fp16=True,
    # å‡å°‘é€šä¿¡é¢‘ç‡
    logging_steps=100,
    save_steps=1000,
)
```

---

## ğŸ¯ æ€»ç»“ä¸å±•æœ›

### ğŸ”‘ å…³é”®è¦ç‚¹æ€»ç»“

1. **æ¨¡å—åŒ–è®¾è®¡**ï¼šTraineré‡‡ç”¨é«˜åº¦æ¨¡å—åŒ–çš„è®¾è®¡ï¼Œä¾¿äºæ‰©å±•å’Œç»´æŠ¤ã€‚

2. **é…ç½®é©±åŠ¨**ï¼šé€šè¿‡TrainingArgumentsç»Ÿä¸€ç®¡ç†æ‰€æœ‰è®­ç»ƒå‚æ•°ï¼Œæä¾›çµæ´»çš„é…ç½®é€‰é¡¹ã€‚

3. **åˆ†å¸ƒå¼æ”¯æŒ**ï¼šé›†æˆData Parallelã€DeepSpeedã€FSDPç­‰å¤šç§åˆ†å¸ƒå¼è®­ç»ƒç­–ç•¥ã€‚

4. **æ··åˆç²¾åº¦**ï¼šæ”¯æŒFP16ã€BF16ç­‰æ··åˆç²¾åº¦è®­ç»ƒï¼Œæå‡è®­ç»ƒæ•ˆç‡ã€‚

5. **å›è°ƒç³»ç»Ÿ**ï¼šé€šè¿‡å›è°ƒæœºåˆ¶å®ç°è®­ç»ƒè¿‡ç¨‹çš„ç›‘æ§å’Œæ‰©å±•ã€‚

6. **çŠ¶æ€ç®¡ç†**ï¼šå®Œå–„çš„è®­ç»ƒçŠ¶æ€ç®¡ç†ï¼Œæ”¯æŒæ£€æŸ¥ç‚¹æ¢å¤å’Œè®­ç»ƒç»§ç»­ã€‚

### ğŸš€ æœªæ¥å‘å±•è¶‹åŠ¿

1. **æ›´é«˜æ•ˆçš„åˆ†å¸ƒå¼ç­–ç•¥**ï¼šæ”¯æŒæ›´ç»†ç²’åº¦çš„å¹¶è¡Œç­–ç•¥å’Œæ›´é«˜æ•ˆçš„é€šä¿¡æœºåˆ¶ã€‚
2. **è‡ªåŠ¨åŒ–ä¼˜åŒ–**ï¼šæ™ºèƒ½åŒ–çš„è¶…å‚æ•°ä¼˜åŒ–å’Œæ¨¡å‹æ¶æ„æœç´¢ã€‚
3. **äº‘åŸç”Ÿæ”¯æŒ**ï¼šæ›´å¥½çš„äº‘å¹³å°é›†æˆå’Œå¼¹æ€§è®­ç»ƒæ”¯æŒã€‚
4. **å¤šæ¨¡æ€è®­ç»ƒ**ï¼šç»Ÿä¸€çš„å¤šæ¨¡æ€è®­ç»ƒæ¡†æ¶ã€‚
5. **ç»¿è‰²AI**ï¼šæ›´ç¯ä¿çš„è®­ç»ƒç­–ç•¥å’Œèµ„æºåˆ©ç”¨ã€‚

### ğŸ¯ æœ€ä½³å®è·µå»ºè®®

1. **åˆç†é€‰æ‹©åˆ†å¸ƒå¼ç­–ç•¥**ï¼šæ ¹æ®æ¨¡å‹è§„æ¨¡å’Œç¡¬ä»¶æ¡ä»¶é€‰æ‹©åˆé€‚çš„åˆ†å¸ƒå¼ç­–ç•¥ã€‚
2. **ä¼˜åŒ–å†…å­˜ä½¿ç”¨**ï¼šåˆç†è®¾ç½®æ¢¯åº¦ç´¯ç§¯ã€æ¢¯åº¦æ£€æŸ¥ç‚¹ç­‰å‚æ•°ã€‚
3. **ç›‘æ§è®­ç»ƒè¿‡ç¨‹**ï¼šä½¿ç”¨å›è°ƒç³»ç»Ÿç›‘æ§è®­ç»ƒçŠ¶æ€å’Œæ€§èƒ½æŒ‡æ ‡ã€‚
4. **å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹**ï¼šé¿å…è®­ç»ƒä¸­æ–­å¯¼è‡´çš„æ•°æ®ä¸¢å¤±ã€‚
5. **æ€§èƒ½è°ƒä¼˜**ï¼šæ ¹æ®å…·ä½“åœºæ™¯è°ƒæ•´æ‰¹å¤§å°ã€å­¦ä¹ ç‡ç­‰è¶…å‚æ•°ã€‚

Traineræ¡†æ¶ä½œä¸ºç°ä»£æ·±åº¦å­¦ä¹ è®­ç»ƒçš„æ ‡æ†ï¼Œå…¶è®¾è®¡ç†å¿µå’Œå®ç°ç»†èŠ‚å¯¹æ„å»ºä¼ä¸šçº§è®­ç»ƒç³»ç»Ÿå…·æœ‰é‡è¦å‚è€ƒä»·å€¼ã€‚é€šè¿‡æ·±å…¥ç†è§£å…¶å®ç°æœºåˆ¶ï¼Œæˆ‘ä»¬å¯ä»¥æ›´å¥½åœ°åº”ç”¨å’Œæ‰©å±•è¿™äº›æŠ€æœ¯ï¼Œæ„å»ºæ›´é«˜æ•ˆçš„æ·±åº¦å­¦ä¹ è®­ç»ƒç³»ç»Ÿã€‚

---

**ğŸ”— ç›¸å…³èµ„æºï¼š**
- [Trainerå®˜æ–¹æ–‡æ¡£](https://huggingface.co/docs/transformers/main_classes/trainer)
- [DeepSpeedå®˜æ–¹æ–‡æ¡£](https://www.deepspeed.ai/)
- [PyTorchåˆ†å¸ƒå¼è®­ç»ƒæ–‡æ¡£](https://pytorch.org/docs/stable/distributed.html)

**ğŸ“§ æŠ€æœ¯äº¤æµï¼š**
æ¬¢è¿åœ¨è¯„è®ºåŒºåˆ†äº«æ‚¨çš„è®­ç»ƒç»éªŒå’Œä¼˜åŒ–æŠ€å·§ï¼Œå…±åŒæ¢è®¨æ·±åº¦å­¦ä¹ è®­ç»ƒçš„æœ€ä½³å®è·µã€‚

---

*æœ¬æ–‡åŸºäºTransformersåº“æœ€æ–°ç‰ˆæœ¬æºç åˆ†æï¼Œéƒ¨åˆ†ä»£ç ç¤ºä¾‹å¯èƒ½éœ€è¦æ ¹æ®å®é™…ç‰ˆæœ¬è¿›è¡Œè°ƒæ•´ã€‚*