# ğŸ”¥ HuggingFace Transformersåº“æ·±åº¦è§£æç³»åˆ—ï¼ˆäºŒï¼‰ï¼šé¢„è®­ç»ƒæ¨¡å‹å®ç°æœºåˆ¶æ·±åº¦å‰–æ

> ä½œä¸ºOpenAIçš„æŠ€æœ¯æ¶æ„å¸ˆï¼Œä»Šå¤©æˆ‘å°†æ·±å…¥å‰–æTransformersåº“ä¸­é¢„è®­ç»ƒæ¨¡å‹çš„æ ¸å¿ƒå®ç°æœºåˆ¶ã€‚ä»BERTçš„æ•°å­¦åŸç†åˆ°ä»£ç å®ç°ï¼Œä»æ³¨æ„åŠ›æœºåˆ¶åˆ°ä½ç½®ç¼–ç ï¼Œæˆ‘ä»¬å°†å½»åº•ç†è§£ç°ä»£NLPæ¨¡å‹çš„æ„å»ºç²¾é«“ã€‚

## ğŸ“‹ ç›®å½•

- [é¢„è®­ç»ƒæ¨¡å‹çš„æ ¸å¿ƒæ•°å­¦åŸç†](#é¢„è®­ç»ƒæ¨¡å‹çš„æ ¸å¿ƒæ•°å­¦åŸç†)
- [BERTæ¶æ„çš„å®Œæ•´å®ç°å‰–æ](#bertæ¶æ„çš„å®Œæ•´å®ç°å‰–æ)
- [æ³¨æ„åŠ›æœºåˆ¶çš„å¤šç§å®ç°æ–¹å¼](#æ³¨æ„åŠ›æœºåˆ¶çš„å¤šç§å®ç°æ–¹å¼)
- [ä½ç½®ç¼–ç æŠ€æœ¯çš„æ·±åº¦å¯¹æ¯”](#ä½ç½®ç¼–ç æŠ€æœ¯çš„æ·±åº¦å¯¹æ¯”)
- [å‰é¦ˆç½‘ç»œçš„ä¼˜åŒ–å®ç°](#å‰é¦ˆç½‘ç»œçš„ä¼˜åŒ–å®ç°)
- [LayerNormçš„æ•°å­¦åŸç†ä¸å®ç°](#layernormçš„æ•°å­¦åŸç†ä¸å®ç°)
- [æ®‹å·®è¿æ¥çš„æ¢¯åº¦æµåŠ¨åˆ†æ](#æ®‹å·®è¿æ¥çš„æ¢¯åº¦æµåŠ¨åˆ†æ)
- [æ¨¡å‹åˆå§‹åŒ–ç­–ç•¥æ·±åº¦å‰–æ](#æ¨¡å‹åˆå§‹åŒ–ç­–ç•¥æ·±åº¦å‰–æ)
- [é«˜çº§ä¼˜åŒ–æŠ€æœ¯](#é«˜çº§ä¼˜åŒ–æŠ€æœ¯)
- [å¤§è§„æ¨¡è®­ç»ƒçš„åˆ†å¸ƒå¼ç­–ç•¥](#å¤§è§„æ¨¡è®­ç»ƒçš„åˆ†å¸ƒå¼ç­–ç•¥)
- [æ€§èƒ½è°ƒä¼˜ä¸æœ€ä½³å®è·µ](#æ€§èƒ½è°ƒä¼˜ä¸æœ€ä½³å®è·µ)
- [å®æˆ˜ä»£ç ç¤ºä¾‹](#å®æˆ˜ä»£ç ç¤ºä¾‹)
- [æ€»ç»“ä¸å±•æœ›](#æ€»ç»“ä¸å±•æœ›)

---

## ğŸ§® é¢„è®­ç»ƒæ¨¡å‹çš„æ ¸å¿ƒæ•°å­¦åŸç†

### ğŸ”‘ Transformeræ¶æ„çš„æ•°å­¦åŸºç¡€

#### 1. **è‡ªæ³¨æ„åŠ›æœºåˆ¶ (Self-Attention)**

è‡ªæ³¨æ„åŠ›æœºåˆ¶æ˜¯Transformerçš„æ ¸å¿ƒï¼Œå…¶æ•°å­¦è¡¨è¾¾å¦‚ä¸‹ï¼š

```python
# ç»™å®šè¾“å…¥åºåˆ— X = [xâ‚, xâ‚‚, ..., xâ‚™]
# å…¶ä¸­æ¯ä¸ª xáµ¢ âˆˆ â„^d_model

# 1. çº¿æ€§å˜æ¢å¾—åˆ° Q, K, V
Q = X * W^Q    # W^Q âˆˆ â„^{d_model Ã— d_k}
K = X * W^K    # W^K âˆˆ â„^{d_model Ã— d_k}
V = X * W^V    # W^V âˆˆ â„^{d_model Ã— d_v}

# 2. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
Attention Scores = Q * K^T / âˆšd_k

# 3. åº”ç”¨softmaxå¾—åˆ°æ³¨æ„åŠ›æƒé‡
Attention Weights = softmax(Attention Scores)

# 4. åŠ æƒæ±‚å’Œå¾—åˆ°è¾“å‡º
Output = Attention Weights * V
```

#### 2. **å¤šå¤´æ³¨æ„åŠ› (Multi-Head Attention)**

```python
# å°† d_model åˆ†ä¸º h ä¸ªå¤´
MultiHead(Q, K, V) = Concat(headâ‚, headâ‚‚, ..., headâ‚•) * W^O

# å…¶ä¸­æ¯ä¸ªå¤´çš„è®¡ç®—ï¼š
headáµ¢ = Attention(Q * Wáµ¢^Q, K * Wáµ¢^K, V * Wáµ¢^V)
```

#### 3. **å‰é¦ˆç½‘ç»œ (Feed-Forward Network)**

```python
FFN(x) = max(0, x * Wâ‚ + bâ‚) * Wâ‚‚ + bâ‚‚

# é€šå¸¸ä½¿ç”¨ï¼šd_ff = 4 * d_model
FFN(x) = ReLU(x * Wâ‚ + bâ‚) * Wâ‚‚ + bâ‚‚
```

### ğŸ¯ BERTçš„é¢„è®­ç»ƒç›®æ ‡

#### 1. **æ©ç è¯­è¨€æ¨¡å‹ (Masked Language Model)**

```python
# éšæœºæ©ç›–15%çš„token
# 80%æ›¿æ¢ä¸º[MASK]
# 10%æ›¿æ¢ä¸ºéšæœºè¯
# 10%ä¿æŒä¸å˜

loss = CrossEntropy(predicted_tokens, original_tokens)
```

#### 2. **ä¸‹ä¸€å¥é¢„æµ‹ (Next Sentence Prediction)**

```python
# åˆ¤æ–­ä¸¤ä¸ªå¥å­æ˜¯å¦è¿ç»­
loss = BinaryCrossEntropy(is_next, predicted_is_next)
```

---

## ğŸ—ï¸ BERTæ¶æ„çš„å®Œæ•´å®ç°å‰–æ

è®©æˆ‘ä»¬æ·±å…¥åˆ†æBERTçš„å…·ä½“å®ç°ï¼Œä»`modeling_bert.py`å¼€å§‹ï¼š

### ğŸ“ BertEmbeddingså®ç°

```python
# modeling_bert.py:58-118
class BertEmbeddings(nn.Module):
    """Construct the embeddings from word, position and token_type embeddings."""

    def __init__(self, config):
        super().__init__()

        # 1. è¯åµŒå…¥ (Word Embeddings)
        self.word_embeddings = nn.Embedding(
            config.vocab_size,
            config.hidden_size,
            padding_idx=config.pad_token_id
        )

        # 2. ä½ç½®åµŒå…¥ (Position Embeddings)
        self.position_embeddings = nn.Embedding(
            config.max_position_embeddings,
            config.hidden_size
        )

        # 3. æ®µè½åµŒå…¥ (Token Type Embeddings)
        self.token_type_embeddings = nn.Embedding(
            config.type_vocab_size,
            config.hidden_size
        )

        # 4. LayerNormå’ŒDropout
        self.LayerNorm = nn.LayerNorm(
            config.hidden_size,
            eps=config.layer_norm_eps
        )
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

        # 5. ä½ç½®åµŒå…¥ç±»å‹
        self.position_embedding_type = getattr(
            config, "position_embedding_type", "absolute"
        )

        # 6. æ³¨å†Œä½ç½®IDå’Œç±»å‹IDçš„buffer
        self.register_buffer(
            "position_ids",
            torch.arange(config.max_position_embeddings).expand((1, -1)),
            persistent=False
        )
        self.register_buffer(
            "token_type_ids",
            torch.zeros(self.position_ids.size(), dtype=torch.long),
            persistent=False
        )

    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        token_type_ids: Optional[torch.LongTensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        past_key_values_length: int = 0,
    ) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­å®ç°
        """
        # 1. ç¡®å®šè¾“å…¥å½¢çŠ¶
        if input_ids is not None:
            input_shape = input_ids.size()
        else:
            input_shape = inputs_embeds.size()[:-1]

        batch_size, seq_length = input_shape

        # 2. å¤„ç†ä½ç½®ID
        if position_ids is None:
            position_ids = self.position_ids[
                :, past_key_values_length : seq_length + past_key_values_length
            ]

        # 3. å¤„ç†tokenç±»å‹ID
        if token_type_ids is None:
            if hasattr(self, "token_type_ids"):
                # ä½¿ç”¨bufferä¸­çš„token_type_ids
                buffered_token_type_ids = self.token_type_ids.expand(
                    position_ids.shape[0], -1
                )
                buffered_token_type_ids = torch.gather(
                    buffered_token_type_ids, dim=1, index=position_ids
                )
                token_type_ids = buffered_token_type_ids.expand(
                    batch_size, seq_length
                )
            else:
                token_type_ids = torch.zeros(
                    input_shape,
                    dtype=torch.long,
                    device=self.position_ids.device
                )

        # 4. è·å–è¯åµŒå…¥
        if inputs_embeds is None:
            inputs_embeds = self.word_embeddings(input_ids)

        # 5. è·å–tokenç±»å‹åµŒå…¥
        token_type_embeddings = self.token_type_embeddings(token_type_ids)

        # 6. ç»„åˆåµŒå…¥
        embeddings = inputs_embeds + token_type_embeddings

        # 7. æ·»åŠ ä½ç½®åµŒå…¥
        if self.position_embedding_type == "absolute":
            position_embeddings = self.position_embeddings(position_ids)
            embeddings += position_embeddings

        # 8. åº”ç”¨LayerNormå’ŒDropout
        embeddings = self.LayerNorm(embeddings)
        embeddings = self.dropout(embeddings)

        return embeddings
```

### ğŸ“ BertSelfAttentionå®ç°

```python
# modeling_bert.py:200-300
class BertSelfAttention(nn.Module):
    def __init__(self, config, position_embedding_type=None):
        super().__init__()

        # 1. æ£€æŸ¥hidden_sizeæ˜¯å¦èƒ½è¢«num_attention_headsæ•´é™¤
        if config.hidden_size % config.num_attention_heads != 0:
            raise ValueError(
                f"The hidden size ({config.hidden_size}) is not a multiple of the number of attention "
                f"heads ({config.num_attention_heads})"
            )

        self.num_attention_heads = config.num_attention_heads
        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)
        self.all_head_size = self.num_attention_heads * self.attention_head_size

        # 2. æŸ¥è¯¢ã€é”®ã€å€¼çš„çº¿æ€§å˜æ¢
        self.query = nn.Linear(config.hidden_size, self.all_head_size)
        self.key = nn.Linear(config.hidden_size, self.all_head_size)
        self.value = nn.Linear(config.hidden_size, self.all_head_size)

        # 3. Dropoutå±‚
        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)

        # 4. ä½ç½®åµŒå…¥ç±»å‹
        self.position_embedding_type = position_embedding_type or getattr(
            config, "position_embedding_type", "absolute"
        )

        # 5. ç›¸å¯¹ä½ç½®åµŒå…¥
        if self.position_embedding_type == "relative_key" or self.position_embedding_type == "relative_key_query":
            self.max_position_embeddings = config.max_position_embeddings
            self.distance_embedding = nn.Embedding(
                2 * config.max_position_embeddings - 1, self.attention_head_size
            )

        self.is_decoder = config.is_decoder

    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:
        """
        é‡å¡‘å¼ é‡ä»¥æ”¯æŒå¤šå¤´æ³¨æ„åŠ›
        """
        new_x_shape = x.size()[:-1] + (
            self.num_attention_heads,
            self.attention_head_size,
        )
        x = x.view(new_x_shape)
        return x.permute(0, 2, 1, 3)

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.FloatTensor] = None,
        head_mask: Optional[torch.FloatTensor] = None,
        encoder_hidden_states: Optional[torch.FloatTensor] = None,
        encoder_attention_mask: Optional[torch.FloatTensor] = None,
        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,
        output_attentions: Optional[bool] = False,
    ) -> Tuple[torch.Tensor]:
        """
        è‡ªæ³¨æ„åŠ›å‰å‘ä¼ æ’­
        """
        # 1. çº¿æ€§å˜æ¢å¾—åˆ°Q, K, V
        mixed_query_layer = self.query(hidden_states)

        # 2. å¤„ç†äº¤å‰æ³¨æ„åŠ›ï¼ˆç”¨äºè§£ç å™¨ï¼‰
        if encoder_hidden_states is not None:
            # äº¤å‰æ³¨æ„åŠ›æƒ…å†µ
            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))
            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))
            attention_mask = encoder_attention_mask
        else:
            # è‡ªæ³¨æ„åŠ›æƒ…å†µ
            key_layer = self.transpose_for_scores(self.key(hidden_states))
            value_layer = self.transpose_for_scores(self.value(hidden_states))

        query_layer = self.transpose_for_scores(mixed_query_layer)

        # 3. å¤„ç†past_key_valueï¼ˆç”¨äºç”Ÿæˆï¼‰
        if past_key_value is not None:
            # é‡ç”¨ç¼“å­˜çš„keyå’Œvalue
            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)
            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)

        # 4. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))

        # 5. ç¼©æ”¾æ³¨æ„åŠ›åˆ†æ•°
        attention_scores = attention_scores / math.sqrt(self.attention_head_size)

        # 6. å¤„ç†ç›¸å¯¹ä½ç½®åµŒå…¥
        if self.position_embedding_type == "relative_key" or self.position_embedding_type == "relative_key_query":
            seq_length = hidden_states.size()[1]
            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)
            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)
            distance = position_ids_l - position_ids_r

            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)
            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility

            if self.position_embedding_type == "relative_key":
                relative_position_scores = torch.einsum("bhld,lrd->bhlr", query_layer, positional_embedding)
                attention_scores = attention_scores + relative_position_scores
            elif self.position_embedding_type == "relative_key_query":
                relative_position_scores_query = torch.einsum("bhld,lrd->bhlr", query_layer, positional_embedding)
                relative_position_scores_key = torch.einsum("bhrd,lrd->bhlr", key_layer, positional_embedding)
                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key

        # 7. åº”ç”¨attention mask
        if attention_mask is not None:
            attention_scores = attention_scores + attention_mask

        # 8. è®¡ç®—æ³¨æ„åŠ›æƒé‡
        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
        attention_probs = self.dropout(attention_probs)

        # 9. åº”ç”¨head mask
        if head_mask is not None:
            attention_probs = attention_probs * head_mask

        # 10. è®¡ç®—ä¸Šä¸‹æ–‡å‘é‡
        context_layer = torch.matmul(attention_probs, value_layer)

        # 11. é‡å¡‘è¾“å‡º
        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)
        context_layer = context_layer.view(new_context_layer_shape)

        # 12. å‡†å¤‡è¾“å‡º
        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)

        # 13. ç¼“å­˜keyå’Œvalueç”¨äºç”Ÿæˆ
        if self.is_decoder:
            outputs = outputs + (key_layer, value_layer)

        return outputs
```

### ğŸ“ BertSelfOutputå®ç°

```python
# modeling_bert.py:350-400
class BertSelfOutput(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def forward(
        self,
        hidden_states: torch.Tensor,
        input_tensor: torch.Tensor
    ) -> torch.Tensor:
        """
        è‡ªæ³¨æ„åŠ›è¾“å‡ºçš„åå¤„ç†
        """
        # 1. çº¿æ€§å˜æ¢
        hidden_states = self.dense(hidden_states)

        # 2. Dropout
        hidden_states = self.dropout(hidden_states)

        # 3. æ®‹å·®è¿æ¥
        hidden_states = hidden_states + input_tensor

        # 4. LayerNorm
        hidden_states = self.LayerNorm(hidden_states)

        return hidden_states
```

### ğŸ“ BertIntermediateå®ç°

```python
# modeling_bert.py:420-450
class BertIntermediate(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)

        # æ”¯æŒå¤šç§æ¿€æ´»å‡½æ•°
        if isinstance(config.hidden_act, str):
            self.intermediate_act_fn = ACT2FN[config.hidden_act]
        else:
            self.intermediate_act_fn = config.hidden_act

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        """
        å‰é¦ˆç½‘ç»œçš„ç¬¬ä¸€å±‚
        """
        hidden_states = self.dense(hidden_states)
        hidden_states = self.intermediate_act_fn(hidden_states)
        return hidden_states
```

### ğŸ“ BertOutputå®ç°

```python
# modeling_bert.py:470-500
class BertOutput(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def forward(
        self,
        hidden_states: torch.Tensor,
        input_tensor: torch.Tensor
    ) -> torch.Tensor:
        """
        å‰é¦ˆç½‘ç»œè¾“å‡ºçš„åå¤„ç†
        """
        hidden_states = self.dense(hidden_states)
        hidden_states = self.dropout(hidden_states)
        hidden_states = hidden_states + input_tensor
        hidden_states = self.LayerNorm(hidden_states)
        return hidden_states
```

---

## ğŸ” æ³¨æ„åŠ›æœºåˆ¶çš„å¤šç§å®ç°æ–¹å¼

Transformersåº“æ”¯æŒå¤šç§æ³¨æ„åŠ›æœºåˆ¶çš„å®ç°ï¼Œè®©æˆ‘ä»¬è¯¦ç»†åˆ†æï¼š

### ğŸ¯ 1. æ ‡å‡†æ³¨æ„åŠ›å®ç°

```python
# modeling_bert.py:121-150
def eager_attention_forward(
    module: nn.Module,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Optional[torch.Tensor],
    scaling: Optional[float] = None,
    dropout: float = 0.0,
    head_mask: Optional[torch.Tensor] = None,
    use_cache: Optional[bool] = None,
    **kwargs: Unpack[TransformersKwargs],
):
    """
    æ ‡å‡†çš„æ³¨æ„åŠ›å‰å‘ä¼ æ’­å®ç°
    """
    # 1. ç¡®å®šç¼©æ”¾å› å­
    if scaling is None:
        scaling = query.size(-1) ** -0.5

    # 2. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
    attn_weights = torch.matmul(query, key.transpose(-1, -2))

    # 3. åº”ç”¨ç¼©æ”¾
    if scaling is not None:
        attn_weights = attn_weights * scaling

    # 4. å¤„ç†ç›¸å¯¹ä½ç½®åµŒå…¥
    if module.position_embedding_type == "relative_key" or module.position_embedding_type == "relative_key_query":
        query_length, key_length = query.shape[2], key.shape[2]

        if use_cache:
            position_ids_l = torch.tensor(
                key_length - 1, dtype=torch.long, device=query.device
            ).view(-1, 1)
        else:
            position_ids_l = torch.arange(
                query_length, dtype=torch.long, device=query.device
            ).view(-1, 1)

        position_ids_r = torch.arange(
            key_length, dtype=torch.long, device=query.device
        ).view(1, -1)

        distance = position_ids_l - position_ids_r

        positional_embedding = module.distance_embedding(
            distance + module.max_position_embeddings - 1
        )
        positional_embedding = positional_embedding.to(dtype=query.dtype)

        if module.position_embedding_type == "relative_key":
            relative_position_scores = torch.einsum(
                "bhld,lrd->bhlr", query, positional_embedding
            )
            attn_weights = attn_weights + relative_position_scores
        elif module.position_embedding_type == "relative_key_query":
            relative_position_scores_query = torch.einsum(
                "bhld,lrd->bhlr", query, positional_embedding
            )
            relative_position_scores_key = torch.einsum(
                "bhrd,lrd->bhlr", key, positional_embedding
            )
            attn_weights = attn_weights + relative_position_scores_query + relative_position_scores_key

    # 5. åº”ç”¨attention mask
    if attention_mask is not None:
        attn_weights = attn_weights + attention_mask

    # 6. è®¡ç®—æ³¨æ„åŠ›æƒé‡
    attn_weights = nn.functional.softmax(attn_weights, dim=-1)

    # 7. åº”ç”¨dropout
    if dropout > 0.0:
        attn_weights = nn.functional.dropout(attn_weights, p=dropout)

    # 8. åº”ç”¨head mask
    if head_mask is not None:
        attn_weights = attn_weights * head_mask

    # 9. è®¡ç®—è¾“å‡º
    attn_output = torch.matmul(attn_weights, value)

    return attn_output, attn_weights
```

### ğŸ¯ 2. Flash Attentionå®ç°

```python
# integrations/flash_attention/__init__.py:100-200
def flash_attention_forward(
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Optional[torch.Tensor] = None,
    dropout: float = 0.0,
    **kwargs
):
    """
    Flash Attention 2å®ç°
    """
    # 1. æ£€æŸ¥Flash Attentionå¯ç”¨æ€§
    if not is_flash_attn_2_available():
        raise ImportError("Flash Attention 2 is not available")

    try:
        from flash_attn import flash_attn_func
    except ImportError:
        raise ImportError("Could not import flash_attn_func")

    # 2. å‡†å¤‡è¾“å…¥å¼ é‡
    # Flash Attentionéœ€è¦ç‰¹å®šçš„å¼ é‡å½¢çŠ¶
    batch_size, num_heads, seq_len, head_dim = query.shape

    # 3. å¤„ç†attention mask
    if attention_mask is not None:
        # è½¬æ¢ä¸ºFlash Attentionæ ¼å¼
        attention_mask = attention_mask.to(torch.bool)
        # æ³¨æ„ï¼šFlash Attentionæœ‰è‡ªå·±çš„maskå¤„ç†æ–¹å¼
        # è¿™é‡Œéœ€è¦é¢å¤–çš„å¤„ç†é€»è¾‘

    # 4. åº”ç”¨Flash Attention
    attn_output = flash_attn_func(
        query,
        key,
        value,
        dropout_p=dropout if kwargs.get("training", False) else 0.0,
        causal=kwargs.get("causal", False),
        deterministic=kwargs.get("deterministic", False),
        window_size=kwargs.get("window_size", (-1, -1)),  # -1è¡¨ç¤ºå…¨å±€æ³¨æ„åŠ›
        alibi_slopes=kwargs.get("alibi_slopes", None),
        deterministic_backend=kwargs.get("deterministic_backend", None,
    )

    return attn_output, None  # Flash Attentionä¸è¿”å›æ³¨æ„åŠ›æƒé‡
```

### ğŸ¯ 3. å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›

```python
# integrations/sdpa_attention/__init__.py:100-200
def sdpa_attention_forward(
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Optional[torch.Tensor] = None,
    dropout: float = 0.0,
    **kwargs
):
    """
    Scaled Dot Product Attention (SDPA)å®ç°
    """
    # 1. æ£€æŸ¥SDPAå¯ç”¨æ€§
    if not hasattr(torch.nn.functional, "scaled_dot_product_attention"):
        raise ImportError("Scaled Dot Product Attention is not available")

    # 2. å¤„ç†attention mask
    if attention_mask is not None:
        # è½¬æ¢SDPAæ ¼å¼
        attention_mask = attention_mask.to(torch.bool)

        # åˆ›å»ºç”¨äºSDPAçš„attn_bias
        attn_bias = torch.zeros_like(attention_mask, dtype=query.dtype)
        attn_bias.masked_fill_(~attention_mask, float("-inf"))
    else:
        attn_bias = None

    # 3. åº”ç”¨SDPA
    attn_output = torch.nn.functional.scaled_dot_product_attention(
        query,
        key,
        value,
        attn_mask=attn_bias,
        dropout_p=dropout if kwargs.get("training", False) else 0.0,
        is_causal=kwargs.get("causal", False),
    )

    return attn_output, None  # SDPAä¸è¿”å›æ³¨æ„åŠ›æƒé‡
```

---

## ğŸ“ ä½ç½®ç¼–ç æŠ€æœ¯çš„æ·±åº¦å¯¹æ¯”

### ğŸ¯ 1. ç»å¯¹ä½ç½®ç¼–ç 

```python
# modeling_bert.py:112-115
if self.position_embedding_type == "absolute":
    position_embeddings = self.position_embeddings(position_ids)
    embeddings += position_embeddings
```

### ğŸ¯ 2. ç›¸å¯¹ä½ç½®ç¼–ç 

```python
# modeling_bert.py:140-148
if module.position_embedding_type == "relative_key" or module.position_embedding_type == "relative_key_query":
    query_length, key_length = query.shape[2], key.shape[2]

    if use_cache:
        position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=query.device).view(-1, 1)
    else:
        position_ids_l = torch.arange(query_length, dtype=torch.long, device=query.device).view(-1, 1)

    position_ids_r = torch.arange(key_length, dtype=torch.long, device=query.device).view(1, -1)
    distance = position_ids_l - position_ids_r

    positional_embedding = module.distance_embedding(distance + module.max_position_embeddings - 1)
    positional_embedding = positional_embedding.to(dtype=query.dtype)  # fp16 compatibility
```

### ğŸ¯ 3. æ—‹è½¬ä½ç½®ç¼–ç  (RoPE)

```python
# modeling_rope_utils.py:100-200
class RotaryEmbedding(nn.Module):
    def __init__(self, dim, max_position_embeddings=2048, base=10000):
        super().__init__()

        self.dim = dim
        self.max_position_embeddings = max_position_embeddings
        self.base = base

        # åˆ›å»ºé¢‘ç‡å¼ é‡
        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.register_buffer("inv_freq", inv_freq)

        # åˆ›å»ºä½ç½®ID
        self.register_buffer(
            "position_ids",
            torch.arange(max_position_embeddings).float()
        )

    def forward(self, seq_len: int, device: torch.device):
        # è®¡ç®—ä½ç½®ç¼–ç 
        freqs = torch.outer(self.position_ids[:seq_len], self.inv_freq)
        freqs = freqs.to(device)

        # åˆ›å»ºæ—‹è½¬çŸ©é˜µ
        cos_freqs = torch.cos(freqs)
        sin_freqs = torch.sin(freqs)

        return cos_freqs, sin_freqs

def apply_rotary_pos_emb(
    q: torch.Tensor,
    k: torch.Tensor,
    cos: torch.Tensor,
    sin: torch.Tensor,
):
    """
    åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç åˆ°æŸ¥è¯¢å’Œé”®
    """
    # é‡å¡‘å¼ é‡ä»¥æ”¯æŒæ—‹è½¬
    q = q.unsqueeze(-1)  # [batch, heads, seq_len, head_dim, 1]
    k = k.unsqueeze(-1)  # [batch, heads, seq_len, head_dim, 1]

    # åº”ç”¨æ—‹è½¬
    cos = cos.unsqueeze(0).unsqueeze(0)  # [1, 1, seq_len, head_dim, 1]
    sin = sin.unsqueeze(0).unsqueeze(0)  # [1, 1, seq_len, head_dim, 1]

    # æ—‹è½¬æ“ä½œ
    q_rot = q * cos - torch.roll(q, shifts=1, dims=-2) * sin
    k_rot = k * cos - torch.roll(k, shifts=1, dims=-2) * sin

    # é‡å¡‘å›åŸå§‹å½¢çŠ¶
    q_rot = q_rot.squeeze(-1)
    k_rot = k_rot.squeeze(-1)

    return q_rot, k_rot
```

### ğŸ¯ 4. ALiBi (Attention with Linear Biases)

```python
# modeling_attn_mask_utils.py:300-400
def get_alibi_mask(
    tensor: torch.Tensor,
    num_heads: int,
    dtype: torch.dtype,
) -> torch.Tensor:
    """
    ç”ŸæˆALiBiæ³¨æ„åŠ›åç½®
    """
    batch_size, seq_length = tensor.shape[:2]

    # åˆ›å»ºä½ç½®åç½®
    positions = torch.arange(seq_length, dtype=torch.long)
    relative_positions = positions.unsqueeze(0) - positions.unsqueeze(1)

    # è®¡ç®—æ¯ä¸ªheadçš„æ–œç‡
    slopes = torch.pow(2, -torch.pow(2, -(torch.arange(num_heads) / num_heads)))

    # åº”ç”¨ALiBiåç½®
    alibi_mask = relative_positions.unsqueeze(0).expand(num_heads, -1, -1)
    alibi_mask = alibi_mask * slopes.unsqueeze(1).unsqueeze(2)

    # è½¬æ¢ä¸ºé€‚å½“çš„dtypeå¹¶æ·»åŠ åˆ°attention mask
    alibi_mask = alibi_mask.to(dtype=dtype)

    return alibi_mask
```

---

## ğŸ§® LayerNormçš„æ•°å­¦åŸç†ä¸å®ç°

### ğŸ¯ LayerNormçš„æ•°å­¦åŸç†

LayerNormé€šè¿‡å¯¹æ¯ä¸ªæ ·æœ¬çš„ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–æ¥ç¨³å®šè®­ç»ƒè¿‡ç¨‹ï¼š

```python
# ç»™å®šè¾“å…¥ x âˆˆ â„^d
# Î¼ = (1/d) * Î£áµ¢ xáµ¢          # å‡å€¼
# ÏƒÂ² = (1/d) * Î£áµ¢ (xáµ¢ - Î¼)Â²  # æ–¹å·®
# yáµ¢ = Î³ * (xáµ¢ - Î¼) / âˆš(ÏƒÂ² + Îµ) + Î²  # å½’ä¸€åŒ–
```

### ğŸ¯ PyTorchå®ç°

```python
# modeling_bert.py:67-68
self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
```

### ğŸ¯ è‡ªå®šä¹‰LayerNormå®ç°

```python
class CustomLayerNorm(nn.Module):
    def __init__(self, hidden_size: int, eps: float = 1e-12):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.bias = nn.Parameter(torch.zeros(hidden_size))
        self.eps = eps

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        è‡ªå®šä¹‰LayerNormå®ç°
        """
        # è®¡ç®—å‡å€¼å’Œæ–¹å·®
        mean = x.mean(-1, keepdim=True)
        var = x.var(-1, keepdim=True, unbiased=False)

        # åº”ç”¨LayerNorm
        x = (x - mean) / torch.sqrt(var + self.eps)
        x = self.weight * x + self.bias

        return x
```

---

## ğŸ”„ æ®‹å·®è¿æ¥çš„æ¢¯åº¦æµåŠ¨åˆ†æ

### ğŸ¯ æ®‹å·®è¿æ¥çš„æ•°å­¦è¡¨è¾¾

```python
# æ®‹å·®è¿æ¥: y = x + F(x)
# å…¶ä¸­F(x)æ˜¯å˜æ¢å‡½æ•°ï¼Œxæ˜¯è¾“å…¥

# æ¢¯åº¦æµ: âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚y + âˆ‚L/âˆ‚y * âˆ‚F/âˆ‚x
# è¿™æ„å‘³ç€æ¢¯åº¦å¯ä»¥ç›´æ¥æµè¿‡æ®‹å·®è¿æ¥
```

### ğŸ¯ BERTä¸­çš„æ®‹å·®è¿æ¥å®ç°

```python
# modeling_bert.py:387-393 (BertSelfOutput)
def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
    hidden_states = self.dense(hidden_states)
    hidden_states = self.dropout(hidden_states)
    hidden_states = hidden_states + input_tensor  # æ®‹å·®è¿æ¥
    hidden_states = self.LayerNorm(hidden_states)
    return hidden_states

# modeling_bert.py:487-493 (BertOutput)
def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
    hidden_states = self.dense(hidden_states)
    hidden_states = self.dropout(hidden_states)
    hidden_states = hidden_states + input_tensor  # æ®‹å·®è¿æ¥
    hidden_states = self.LayerNorm(hidden_states)
    return hidden_states
```

### ğŸ¯ æ®‹å·®è¿æ¥çš„ä¼˜åŠ¿

1. **æ¢¯åº¦æµåŠ¨**ï¼šé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±ï¼Œæ”¯æŒæ·±å±‚ç½‘ç»œè®­ç»ƒ
2. **ç½‘ç»œæ’ç­‰æ˜ å°„**ï¼šå…è®¸ç½‘ç»œå­¦ä¹ æ’ç­‰æ˜ å°„
3. **è®­ç»ƒç¨³å®šæ€§**ï¼šæé«˜è®­ç»ƒç¨³å®šæ€§ï¼Œå‡å°‘å¯¹åˆå§‹åŒ–çš„æ•æ„Ÿæ€§

---

## ğŸ² æ¨¡å‹åˆå§‹åŒ–ç­–ç•¥æ·±åº¦å‰–æ

### ğŸ¯ BERTçš„åˆå§‹åŒ–ç­–ç•¥

```python
# modeling_utils.py:2000-2100
def _init_weights(self, module):
    """
    åˆå§‹åŒ–æ¨¡å‹æƒé‡
    """
    if isinstance(module, nn.Linear):
        # çº¿æ€§å±‚åˆå§‹åŒ–
        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
        if module.bias is not None:
            module.bias.data.zero_()
    elif isinstance(module, nn.Embedding):
        # åµŒå…¥å±‚åˆå§‹åŒ–
        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
        if module.padding_idx is not None:
            module.weight.data[module.padding_idx].zero_()
    elif isinstance(module, nn.LayerNorm):
        # LayerNormåˆå§‹åŒ–
        module.bias.data.zero_()
        module.weight.data.fill_(1.0)
```

### ğŸ¯ ç‰¹æ®Šå±‚çš„åˆå§‹åŒ–

```python
# modeling_bert.py:500-550
class BertPooler(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.activation = nn.Tanh()

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        """
        æ± åŒ–å±‚å®ç°
        """
        # å–ç¬¬ä¸€ä¸ªtokençš„éšè—çŠ¶æ€
        first_token_tensor = hidden_states[:, 0]
        pooled_output = self.dense(first_token_tensor)
        pooled_output = self.activation(pooled_output)
        return pooled_output

    def _init_weights(self):
        """
        ç‰¹æ®Šçš„åˆå§‹åŒ–ç­–ç•¥
        """
        # æ± åŒ–å±‚ä½¿ç”¨ç‰¹æ®Šçš„åˆå§‹åŒ–
        self.dense.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
        self.dense.bias.data.zero_()
```

---

## ğŸš€ é«˜çº§ä¼˜åŒ–æŠ€æœ¯

### ğŸ¯ 1. æ¢¯åº¦æ£€æŸ¥ç‚¹

```python
# modeling_utils.py:1500-1550
def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None):
    """
    å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœæ˜¾å­˜
    """
    if gradient_checkpointing_kwargs is None:
        gradient_checkpointing_kwargs = {}

    self._gradient_checkpointing_kwargs = gradient_checkpointing_kwargs

    # åº”ç”¨åˆ°æ‰€æœ‰æ”¯æŒæ¢¯åº¦æ£€æŸ¥ç‚¹çš„æ¨¡å—
    self.apply(
        partial(
            self._set_gradient_checkpointing,
            value=True,
            **gradient_checkpointing_kwargs
        )
    )

def _set_gradient_checkpointing(self, module, value=True, **kwargs):
    """
    è®¾ç½®æ¨¡å—çš„æ¢¯åº¦æ£€æŸ¥ç‚¹
    """
    if hasattr(module, "gradient_checkpointing"):
        module.gradient_checkpointing = value

    if hasattr(module, "gradient_checkpointing_kwargs"):
        module.gradient_checkpointing_kwargs = kwargs
```

### ğŸ¯ 2. æ··åˆç²¾åº¦è®­ç»ƒ

```python
# pytorch_utils.py:100-200
def is_torch_greater_or_equal_than_2_3():
    """
    æ£€æŸ¥PyTorchç‰ˆæœ¬æ˜¯å¦æ”¯æŒè‡ªåŠ¨æ··åˆç²¾åº¦
    """
    import torch
    return torch.__version__ >= "2.3.0"

class AMPContext:
    """
    è‡ªåŠ¨æ··åˆç²¾åº¦ä¸Šä¸‹æ–‡ç®¡ç†å™¨
    """
    def __init__(self, enabled=True, dtype=torch.float16):
        self.enabled = enabled
        self.dtype = dtype

    def __enter__(self):
        if self.enabled:
            return torch.autocast(device_type="cuda", dtype=self.dtype)
        return torch.no_grad()

    def __exit__(self, exc_type, exc_val, exc_tb):
        pass
```

### ğŸ¯ 3. åˆ†å¸ƒå¼è®­ç»ƒä¼˜åŒ–

```python
# distributed/__init__.py:100-200
class DistributedConfig:
    def __init__(self, backend="nccl", init_method=None):
        self.backend = backend
        self.init_method = init_method

    def setup(self):
        """
        è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ
        """
        import torch.distributed as dist

        if self.init_method is None:
            self.init_method = "env://"

        dist.init_process_group(
            backend=self.backend,
            init_method=self.init_method
        )

def distribute_model(model, device_map=None):
    """
    åˆ†å¸ƒå¼æ¨¡å‹
    """
    if device_map is None:
        device_map = "auto"

    # ä½¿ç”¨Accelerateè¿›è¡Œæ¨¡å‹åˆ†å¸ƒ
    from accelerate import dispatch_model
    from accelerate.utils import get_balanced_memory

    if device_map == "auto":
        device_map = get_balanced_memory(model)

    model = dispatch_model(model, device_map=device_map)

    return model
```

---

## ğŸ’» å®æˆ˜ä»£ç ç¤ºä¾‹

### ğŸ¯ ç¤ºä¾‹1ï¼šä»é›¶å®ç°ç®€åŒ–çš„BERT

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class SimpleBertEmbeddings(nn.Module):
    def __init__(self, vocab_size, hidden_size, max_position_embeddings, type_vocab_size):
        super().__init__()
        self.word_embeddings = nn.Embedding(vocab_size, hidden_size)
        self.position_embeddings = nn.Embedding(max_position_embeddings, hidden_size)
        self.token_type_embeddings = nn.Embedding(type_vocab_size, hidden_size)
        self.LayerNorm = nn.LayerNorm(hidden_size, eps=1e-12)
        self.dropout = nn.Dropout(0.1)

        self.position_ids = torch.arange(max_position_embeddings).expand((1, -1))

    def forward(self, input_ids, token_type_ids=None):
        seq_length = input_ids.size(1)
        position_ids = self.position_ids[:, :seq_length]

        if token_type_ids is None:
            token_type_ids = torch.zeros_like(input_ids)

        word_embeddings = self.word_embeddings(input_ids)
        position_embeddings = self.position_embeddings(position_ids)
        token_type_embeddings = self.token_type_embeddings(token_type_ids)

        embeddings = word_embeddings + position_embeddings + token_type_embeddings
        embeddings = self.LayerNorm(embeddings)
        embeddings = self.dropout(embeddings)

        return embeddings

class SimpleBertSelfAttention(nn.Module):
    def __init__(self, hidden_size, num_attention_heads):
        super().__init__()
        assert hidden_size % num_attention_heads == 0

        self.num_attention_heads = num_attention_heads
        self.attention_head_size = hidden_size // num_attention_heads
        self.all_head_size = num_attention_heads * self.attention_head_size

        self.query = nn.Linear(hidden_size, self.all_head_size)
        self.key = nn.Linear(hidden_size, self.all_head_size)
        self.value = nn.Linear(hidden_size, self.all_head_size)
        self.dropout = nn.Dropout(0.1)

    def transpose_for_scores(self, x):
        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
        x = x.view(new_x_shape)
        return x.permute(0, 2, 1, 3)

    def forward(self, hidden_states, attention_mask=None):
        mixed_query_layer = self.query(hidden_states)
        mixed_key_layer = self.key(hidden_states)
        mixed_value_layer = self.value(hidden_states)

        query_layer = self.transpose_for_scores(mixed_query_layer)
        key_layer = self.transpose_for_scores(mixed_key_layer)
        value_layer = self.transpose_for_scores(mixed_value_layer)

        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
        attention_scores = attention_scores / math.sqrt(self.attention_head_size)

        if attention_mask is not None:
            attention_scores = attention_scores + attention_mask

        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
        attention_probs = self.dropout(attention_probs)

        context_layer = torch.matmul(attention_probs, value_layer)
        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)
        context_layer = context_layer.view(new_context_layer_shape)

        return context_layer

class SimpleBertLayer(nn.Module):
    def __init__(self, hidden_size, num_attention_heads, intermediate_size):
        super().__init__()
        self.attention = SimpleBertSelfAttention(hidden_size, num_attention_heads)
        self.attention_output = nn.Linear(hidden_size, hidden_size)
        self.intermediate = nn.Linear(hidden_size, intermediate_size)
        self.output = nn.Linear(intermediate_size, hidden_size)
        self.LayerNorm1 = nn.LayerNorm(hidden_size, eps=1e-12)
        self.LayerNorm2 = nn.LayerNorm(hidden_size, eps=1e-12)
        self.dropout = nn.Dropout(0.1)

    def forward(self, hidden_states, attention_mask=None):
        # è‡ªæ³¨æ„åŠ›
        attention_output = self.attention(hidden_states, attention_mask)
        attention_output = self.attention_output(attention_output)
        attention_output = self.dropout(attention_output)
        hidden_states = self.LayerNorm1(hidden_states + attention_output)

        # å‰é¦ˆç½‘ç»œ
        intermediate_output = self.intermediate(hidden_states)
        intermediate_output = F.gelu(intermediate_output)
        layer_output = self.output(intermediate_output)
        layer_output = self.dropout(layer_output)
        hidden_states = self.LayerNorm2(hidden_states + layer_output)

        return hidden_states

class SimpleBertModel(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.embeddings = SimpleBertEmbeddings(
            config.vocab_size,
            config.hidden_size,
            config.max_position_embeddings,
            config.type_vocab_size
        )
        self.layers = nn.ModuleList([
            SimpleBertLayer(
                config.hidden_size,
                config.num_attention_heads,
                config.intermediate_size
            )
            for _ in range(config.num_hidden_layers)
        ])

    def forward(self, input_ids, attention_mask=None, token_type_ids=None):
        embedding_output = self.embeddings(input_ids, token_type_ids)

        sequence_output = embedding_output
        for layer in self.layers:
            sequence_output = layer(sequence_output, attention_mask)

        return sequence_output

# é…ç½®å’Œä½¿ç”¨
class SimpleBertConfig:
    def __init__(self):
        self.vocab_size = 30522
        self.hidden_size = 768
        self.num_hidden_layers = 12
        self.num_attention_heads = 12
        self.intermediate_size = 3072
        self.max_position_embeddings = 512
        self.type_vocab_size = 2

# åˆ›å»ºæ¨¡å‹
config = SimpleBertConfig()
model = SimpleBertModel(config)

# æµ‹è¯•æ¨¡å‹
input_ids = torch.randint(0, config.vocab_size, (2, 128))
outputs = model(input_ids)
print(f"Output shape: {outputs.shape}")
```

### ğŸ¯ ç¤ºä¾‹2ï¼šæ³¨æ„åŠ›æœºåˆ¶æ€§èƒ½å¯¹æ¯”

```python
import time
import torch
import torch.nn as nn
import torch.nn.functional as F

def benchmark_attention_methods(seq_length=512, batch_size=8, num_heads=12, head_dim=64):
    """
    å¯¹æ¯”ä¸åŒæ³¨æ„åŠ›æ–¹æ³•çš„æ€§èƒ½
    """
    hidden_size = num_heads * head_dim

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    query = torch.randn(batch_size, num_heads, seq_length, head_dim).cuda()
    key = torch.randn(batch_size, num_heads, seq_length, head_dim).cuda()
    value = torch.randn(batch_size, num_heads, seq_length, head_dim).cuda()

    # 1. æ ‡å‡†æ³¨æ„åŠ›
    def standard_attention(query, key, value):
        scores = torch.matmul(query, key.transpose(-1, -2)) / (head_dim ** 0.5)
        attn_weights = F.softmax(scores, dim=-1)
        return torch.matmul(attn_weights, value)

    # 2. Flash Attention
    def flash_attention(query, key, value):
        try:
            from flash_attn import flash_attn_func
            return flash_attn_func(query, key, value, dropout_p=0.0)
        except ImportError:
            return standard_attention(query, key, value)

    # 3. SDPA
    def sdpa_attention(query, key, value):
        if hasattr(F, 'scaled_dot_product_attention'):
            return F.scaled_dot_product_attention(query, key, value)
        else:
            return standard_attention(query, key, value)

    # æ€§èƒ½æµ‹è¯•
    methods = [
        ("Standard Attention", standard_attention),
        ("Flash Attention", flash_attention),
        ("SDPA", sdpa_attention)
    ]

    results = {}

    for name, method in methods:
        try:
            # é¢„çƒ­
            _ = method(query, key, value)
            torch.cuda.synchronize()

            # æµ‹è¯•
            start_time = time.time()
            for _ in range(100):
                _ = method(query, key, value)
            torch.cuda.synchronize()

            elapsed_time = time.time() - start_time
            results[name] = elapsed_time / 100

            print(f"{name}: {elapsed_time/100:.6f}s per forward pass")

        except Exception as e:
            print(f"{name}: Failed with error: {e}")

    return results

# è¿è¡Œæ€§èƒ½æµ‹è¯•
if torch.cuda.is_available():
    results = benchmark_attention_methods()
    print("\nPerformance Summary:")
    for name, time in results.items():
        print(f"{name}: {time:.6f}s")
else:
    print("CUDA not available, skipping benchmark")
```

### ğŸ¯ ç¤ºä¾‹3ï¼šä½ç½®ç¼–ç å¯¹æ¯”åˆ†æ

```python
import torch
import torch.nn as nn
import math
import matplotlib.pyplot as plt

def analyze_position_encodings(max_seq_len=512, d_model=768):
    """
    åˆ†æä¸åŒä½ç½®ç¼–ç çš„ç‰¹æ€§
    """

    # 1. ç»å¯¹ä½ç½®ç¼–ç 
    class AbsolutePositionalEncoding(nn.Module):
        def __init__(self, max_seq_len, d_model):
            super().__init__()
            self.pos_embedding = nn.Embedding(max_seq_len, d_model)

        def forward(self, x):
            positions = torch.arange(x.size(1), device=x.device)
            return self.pos_embedding(positions)

    # 2. ç›¸å¯¹ä½ç½®ç¼–ç 
    class RelativePositionalEncoding(nn.Module):
        def __init__(self, max_seq_len, d_model):
            super().__init__()
            self.max_pos = max_seq_len
            self.relative_pos = nn.Embedding(2 * max_seq_len - 1, d_model)

        def forward(self, q_len, k_len):
            positions = torch.arange(q_len, dtype=torch.long)
            relative_positions = positions.unsqueeze(1) - positions.unsqueeze(0)
            relative_positions = relative_positions + self.max_pos - 1
            return self.relative_pos(relative_positions)

    # 3. æ—‹è½¬ä½ç½®ç¼–ç 
    class RotaryPositionalEncoding(nn.Module):
        def __init__(self, d_model, max_seq_len=512):
            super().__init__()
            self.d_model = d_model
            inv_freq = 1.0 / (10000 ** (torch.arange(0, d_model, 2).float() / d_model))
            self.register_buffer('inv_freq', inv_freq)
            self.register_buffer('position_ids', torch.arange(max_seq_len).float())

        def forward(self, seq_len):
            positions = self.position_ids[:seq_len]
            freqs = torch.outer(positions, self.inv_freq)
            return torch.cos(freqs), torch.sin(freqs)

    # åˆ›å»ºç¼–ç å™¨
    abs_encoding = AbsolutePositionalEncoding(max_seq_len, d_model)
    rel_encoding = RelativePositionalEncoding(max_seq_len, d_model)
    rotary_encoding = RotaryPositionalEncoding(d_model, max_seq_len)

    # åˆ†æç»å¯¹ä½ç½®ç¼–ç 
    abs_positions = torch.arange(max_seq_len)
    abs_encodings = abs_encoding.pos_embedding(abs_positions).detach().numpy()

    # åˆ†æç›¸å¯¹ä½ç½®ç¼–ç 
    rel_encodings = rel_encoding(max_seq_len, max_seq_len).detach().numpy()

    # åˆ†ææ—‹è½¬ä½ç½®ç¼–ç 
    cos_freqs, sin_freqs = rotary_encoding(max_seq_len)

    # å¯è§†åŒ–
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

    # ç»å¯¹ä½ç½®ç¼–ç 
    im1 = axes[0, 0].imshow(abs_encodings[:100, :100], aspect='auto', cmap='viridis')
    axes[0, 0].set_title('Absolute Positional Encoding')
    axes[0, 0].set_xlabel('Feature Dimension')
    axes[0, 0].set_ylabel('Position')
    plt.colorbar(im1, ax=axes[0, 0])

    # ç›¸å¯¹ä½ç½®ç¼–ç 
    im2 = axes[0, 1].imshow(rel_encodings[:100, :100], aspect='auto', cmap='viridis')
    axes[0, 1].set_title('Relative Positional Encoding')
    axes[0, 1].set_xlabel('Key Position')
    axes[0, 1].set_ylabel('Query Position')
    plt.colorbar(im2, ax=axes[0, 1])

    # æ—‹è½¬ä½ç½®ç¼–ç  - Cosine
    im3 = axes[1, 0].imshow(cos_freqs[:100, :100].numpy(), aspect='auto', cmap='viridis')
    axes[1, 0].set_title('Rotary Positional Encoding - Cosine')
    axes[1, 0].set_xlabel('Feature Dimension')
    axes[1, 0].set_ylabel('Position')
    plt.colorbar(im3, ax=axes[1, 0])

    # æ—‹è½¬ä½ç½®ç¼–ç  - Sine
    im4 = axes[1, 1].imshow(sin_freqs[:100, :100].numpy(), aspect='auto', cmap='viridis')
    axes[1, 1].set_title('Rotary Positional Encoding - Sine')
    axes[1, 1].set_xlabel('Feature Dimension')
    axes[1, 1].set_ylabel('Position')
    plt.colorbar(im4, ax=axes[1, 1])

    plt.tight_layout()
    plt.savefig('position_encoding_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()

    return abs_encodings, rel_encodings, (cos_freqs, sin_freqs)

# è¿è¡Œåˆ†æ
abs_enc, rel_enc, rotary_enc = analyze_position_encodings()
```

---

## ğŸ¯ æ€»ç»“ä¸å±•æœ›

### ğŸ”‘ å…³é”®è¦ç‚¹æ€»ç»“

1. **BERTæ¶æ„ç²¾å¦™è®¾è®¡**ï¼šé€šè¿‡åµŒå…¥å±‚ã€æ³¨æ„åŠ›å±‚ã€å‰é¦ˆç½‘ç»œçš„ç»„åˆï¼Œå®ç°äº†å¼ºå¤§çš„æ–‡æœ¬ç†è§£èƒ½åŠ›ã€‚

2. **æ³¨æ„åŠ›æœºåˆ¶å¤šæ ·åŒ–**ï¼šæ ‡å‡†æ³¨æ„åŠ›ã€Flash Attentionã€SDPAç­‰ä¸åŒå®ç°å„æœ‰ä¼˜åŠ¿ï¼Œå¯æ ¹æ®å…·ä½“åœºæ™¯é€‰æ‹©ã€‚

3. **ä½ç½®ç¼–ç æŠ€æœ¯åˆ›æ–°**ï¼šç»å¯¹ä½ç½®ç¼–ç ã€ç›¸å¯¹ä½ç½®ç¼–ç ã€æ—‹è½¬ä½ç½®ç¼–ç ç­‰æ–¹æ¡ˆå„æœ‰ç‰¹è‰²ï¼Œé€‚ç”¨äºä¸åŒä»»åŠ¡ã€‚

4. **ä¼˜åŒ–æŠ€æœ¯åº”ç”¨**ï¼šæ¢¯åº¦æ£€æŸ¥ç‚¹ã€æ··åˆç²¾åº¦ã€åˆ†å¸ƒå¼è®­ç»ƒç­‰æŠ€æœ¯å¤§å¹…æå‡äº†è®­ç»ƒæ•ˆç‡ã€‚

5. **æ¨¡å—åŒ–è®¾è®¡ç†å¿µ**ï¼šæ¯ä¸ªç»„ä»¶éƒ½éµå¾ªå•ä¸€èŒè´£åŸåˆ™ï¼Œä¾¿äºç†è§£å’Œæ‰©å±•ã€‚

### ğŸš€ æœªæ¥å‘å±•è¶‹åŠ¿

1. **æ›´é«˜æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶**ï¼šçº¿æ€§å¤æ‚åº¦ã€å¯¹æ•°å¤æ‚åº¦çš„æ³¨æ„åŠ›ç®—æ³•
2. **åŠ¨æ€æ¶æ„**ï¼šæ ¹æ®è¾“å…¥åŠ¨æ€è°ƒæ•´æ¨¡å‹ç»“æ„
3. **å¤šæ¨¡æ€èåˆ**ï¼šæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘çš„ç»Ÿä¸€è¡¨ç¤º
4. **ç¥ç»æ¶æ„æœç´¢**ï¼šè‡ªåŠ¨åŒ–çš„æ¨¡å‹è®¾è®¡ä¼˜åŒ–
5. **ç»¿è‰²AI**ï¼šæ›´ç¯ä¿çš„æ¨¡å‹è®­ç»ƒå’Œæ¨ç†

### ğŸ¯ æœ€ä½³å®è·µå»ºè®®

1. **æ¨¡å‹é€‰æ‹©**ï¼šæ ¹æ®ä»»åŠ¡å¤æ‚åº¦å’Œè®¡ç®—èµ„æºé€‰æ‹©åˆé€‚çš„æ¨¡å‹è§„æ¨¡
2. **ä¼˜åŒ–é…ç½®**ï¼šåˆç†è®¾ç½®å­¦ä¹ ç‡ã€æ‰¹å¤§å°ã€åºåˆ—é•¿åº¦ç­‰è¶…å‚æ•°
3. **æ€§èƒ½ç›‘æ§**ï¼šå…³æ³¨è®­ç»ƒç¨³å®šæ€§ã€æ”¶æ•›é€Ÿåº¦å’Œæœ€ç»ˆæ€§èƒ½
4. **éƒ¨ç½²ä¼˜åŒ–**ï¼šè€ƒè™‘é‡åŒ–ã€å‰ªæã€è’¸é¦ç­‰æŠ€æœ¯æå‡æ¨ç†æ•ˆç‡
5. **æŒç»­å­¦ä¹ **ï¼šè·Ÿè¸ªæœ€æ–°ç ”ç©¶æˆæœï¼Œä¿æŒæŠ€æœ¯æ›´æ–°

BERTä½œä¸ºç°ä»£NLPçš„é‡Œç¨‹ç¢‘ï¼Œå…¶è®¾è®¡æ€æƒ³å’Œå®ç°ç»†èŠ‚å¯¹åç»­æ¨¡å‹å‘å±•äº§ç”Ÿäº†æ·±è¿œå½±å“ã€‚é€šè¿‡æ·±å…¥ç†è§£å…¶å®ç°æœºåˆ¶ï¼Œæˆ‘ä»¬å¯ä»¥æ›´å¥½åœ°åº”ç”¨å’Œæ”¹è¿›è¿™äº›æŠ€æœ¯ï¼Œæ¨åŠ¨NLPé¢†åŸŸçš„æŒç»­å‘å±•ã€‚

---

**ğŸ”— ç›¸å…³èµ„æºï¼š**
- [BERTåŸå§‹è®ºæ–‡](https://arxiv.org/abs/1810.04805)
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [Flash Attentionè®ºæ–‡](https://arxiv.org/abs/2205.14135)

**ğŸ“§ æŠ€æœ¯äº¤æµï¼š**
æ¬¢è¿åœ¨è¯„è®ºåŒºåˆ†äº«æ‚¨çš„è§è§£å’Œç»éªŒï¼Œå…±åŒæ¢è®¨TransformersæŠ€æœ¯çš„æœªæ¥å‘å±•ã€‚

---

*æœ¬æ–‡åŸºäºTransformersåº“æœ€æ–°ç‰ˆæœ¬æºç åˆ†æï¼Œéƒ¨åˆ†ä»£ç ç¤ºä¾‹å¯èƒ½éœ€è¦æ ¹æ®å®é™…ç‰ˆæœ¬è¿›è¡Œè°ƒæ•´ã€‚*