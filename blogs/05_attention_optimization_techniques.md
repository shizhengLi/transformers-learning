# ğŸ”¥ HuggingFace Transformersåº“æ·±åº¦è§£æç³»åˆ—ï¼ˆäº”ï¼‰ï¼šæ³¨æ„åŠ›æœºåˆ¶ä¼˜åŒ–æŠ€æœ¯å…¨è§£

> ä½œä¸ºOpenAIçš„æŠ€æœ¯æ¶æ„å¸ˆï¼Œä»Šå¤©æˆ‘å°†æ·±å…¥å‰–æTransformersåº“ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶ä¼˜åŒ–æŠ€æœ¯ã€‚è¿™æ˜¯ç°ä»£å¤§è¯­è¨€æ¨¡å‹æ€§èƒ½çš„æ ¸å¿ƒï¼Œå…¶ä¼˜åŒ–æŠ€æœ¯ç›´æ¥å½±å“æ¨¡å‹çš„è®­ç»ƒé€Ÿåº¦ã€æ¨ç†æ€§èƒ½å’Œèµ„æºæ¶ˆè€—ã€‚æœ¬æ–‡å°†ä»æºç å±‚é¢å½»åº•è§£æå„ç§æ³¨æ„åŠ›ä¼˜åŒ–ç®—æ³•çš„å®ç°åŸç†ã€‚

## ğŸ“‹ ç›®å½•

- [æ³¨æ„åŠ›æœºåˆ¶çš„æ€§èƒ½æŒ‘æˆ˜](#æ³¨æ„åŠ›æœºåˆ¶çš„æ€§èƒ½æŒ‘æˆ˜)
- [FlashAttentionæŠ€æœ¯æ·±åº¦å‰–æ](#flashattentionæŠ€æœ¯æ·±åº¦å‰–æ)
- [åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›(GQA)å®ç°åŸç†](#åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›gqaå®ç°åŸç†)
- [KVç¼“å­˜ç³»ç»Ÿä¼˜åŒ–æŠ€æœ¯](#kvç¼“å­˜ç³»ç»Ÿä¼˜åŒ–æŠ€æœ¯)
- [å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›ç®—æ³•](#å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›ç®—æ³•)
- [ç¡¬ä»¶åŠ é€Ÿä¸åç«¯ä¼˜åŒ–](#ç¡¬ä»¶åŠ é€Ÿä¸åç«¯ä¼˜åŒ–)
- [åŠ¨æ€ä¼˜åŒ–é€‰æ‹©æœºåˆ¶](#åŠ¨æ€ä¼˜åŒ–é€‰æ‹©æœºåˆ¶)
- [æ€§èƒ½å¯¹æ¯”ä¸æœ€ä½³å®è·µ](#æ€§èƒ½å¯¹æ¯”ä¸æœ€ä½³å®è·µ)
- [å®æˆ˜ä»£ç ç¤ºä¾‹](#å®æˆ˜ä»£ç ç¤ºä¾‹)
- [æ€»ç»“ä¸å±•æœ›](#æ€»ç»“ä¸å±•æœ›)

---

## ğŸ¯ æ³¨æ„åŠ›æœºåˆ¶çš„æ€§èƒ½æŒ‘æˆ˜

### ğŸ”‘ è®¡ç®—å¤æ‚åº¦é—®é¢˜

æ ‡å‡†æ³¨æ„åŠ›æœºåˆ¶çš„**è®¡ç®—å¤æ‚åº¦**æ˜¯O(nÂ²d)ï¼Œå…¶ä¸­ï¼š
- nï¼šåºåˆ—é•¿åº¦
- dï¼šç‰¹å¾ç»´åº¦

**å†…å­˜æ¶ˆè€—**ï¼šO(nÂ²)çš„æ³¨æ„åŠ›çŸ©é˜µå­˜å‚¨éœ€æ±‚

```python
# æ ‡å‡†æ³¨æ„åŠ›è®¡ç®—ç¤ºä¾‹
def standard_attention(Q, K, V):
    # Q, K, V: [batch_size, num_heads, seq_len, head_dim]
    attention_scores = torch.matmul(Q, K.transpose(-1, -2))  # O(nÂ²)è®¡ç®—
    attention_probs = torch.softmax(attention_scores, dim=-1)   # O(nÂ²)å†…å­˜
    output = torch.matmul(attention_probs, V)                   # O(nÂ²d)è®¡ç®—
    return output
```

### ğŸ“Š æ€§èƒ½ç“¶é¢ˆ

1. **å†…å­˜å¢™**ï¼šé•¿åºåˆ—è®­ç»ƒæ—¶æ³¨æ„åŠ›çŸ©é˜µå†…å­˜çˆ†ç‚¸
2. **è®¡ç®—ç“¶é¢ˆ**ï¼šäºŒæ¬¡æ–¹è®¡ç®—å¤æ‚åº¦é™åˆ¶åºåˆ—é•¿åº¦
3. **è®¿å­˜å¼€é”€**ï¼šå¤§é‡ä¸­é—´ç»“æœçš„è¯»å†™æ“ä½œ
4. **ç¡¬ä»¶åˆ©ç”¨ç‡**ï¼šæ— æ³•å……åˆ†åˆ©ç”¨ç°ä»£ç¡¬ä»¶çš„å¹¶è¡Œèƒ½åŠ›

---

## âš¡ FlashAttentionæŠ€æœ¯æ·±åº¦å‰–æ

### ğŸ—ï¸ æ ¸å¿ƒæ¶æ„è®¾è®¡

FlashAttentionæ˜¯Transformersåº“ä¸­**æœ€é‡è¦çš„æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯**ï¼Œå®ç°äº†IOæ„ŸçŸ¥çš„æ³¨æ„åŠ›è®¡ç®—ã€‚

**æ ¸å¿ƒæ–‡ä»¶**ï¼š`modeling_flash_attention_utils.py`

```python
def _flash_attention_forward(
    query_states: torch.Tensor,
    key_states: torch.Tensor,
    value_states: torch.Tensor,
    attention_mask: Optional[torch.Tensor],
    query_length: int,
    is_causal: bool,
    dropout: float = 0.0,
    position_ids: Optional[torch.Tensor] = None,
    softmax_scale: Optional[float] = None,
    sliding_window: Optional[int] = None,
    use_top_left_mask: bool = False,
) -> torch.Tensor:
    """
    FlashAttentionå‰å‘è®¡ç®—çš„æ ¸å¿ƒå®ç°

    å…³é”®ä¼˜åŒ–ï¼š
    1. åˆ†å—è®¡ç®—ï¼šå°†å¤§çŸ©é˜µè¿ç®—åˆ†è§£ä¸ºå°å—
    2. IOä¼˜åŒ–ï¼šå‡å°‘HBMåˆ°SRAMçš„æ•°æ®ä¼ è¾“
    3. èåˆæ“ä½œï¼šå°†softmaxå’Œmatmulèåˆè®¡ç®—
    """
```

### ğŸ”§ åŠ¨æ€åŠ è½½æœºåˆ¶

**å¤šç‰ˆæœ¬æ”¯æŒ**ï¼šè‡ªåŠ¨æ£€æµ‹å’Œé€‰æ‹©æœ€ä¼˜çš„FlashAttentionå®ç°

```python
def lazy_import_flash_attention(implementation: Optional[str]):
    """
    åŠ¨æ€å¯¼å…¥FlashAttentionå®ç°ï¼Œæ”¯æŒå¤šç§ç¡¬ä»¶åç«¯

    å®ç°ç‰ˆæœ¬ï¼š
    - flash_attention_2: ç‰ˆæœ¬2.0ï¼Œç¨³å®šç‰ˆæœ¬
    - flash_attention_3: ç‰ˆæœ¬3.0ï¼Œæœ€æ–°ä¼˜åŒ–
    - npu_flash_attention: åä¸ºNPUæ”¯æŒ
    - custom: è‡ªå®šä¹‰kernelå®ç°
    """
    global _flash_fn, _flash_varlen_fn, _pad_fn, _unpad_fn
    if any(k is None for k in [_flash_fn, _flash_varlen_fn, _pad_fn, _unpad_fn]):
        _flash_fn, _flash_varlen_fn, _pad_fn, _unpad_fn = _lazy_imports(implementation)
```

### ğŸš€ æ— å¡«å……è®­ç»ƒä¼˜åŒ–

**Padding-Free Training**ï¼šæ¶ˆé™¤å¡«å……tokençš„è®¡ç®—æµªè´¹

```python
def _upad_input(
    query_layer: torch.Tensor,
    key_layer: torch.Tensor,
    value_layer: torch.Tensor,
    attention_mask: torch.Tensor,
    query_length: int,
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    è§£é™¤QKVå¼ é‡çš„å¡«å……ï¼Œå®ç°çœŸæ­£çš„padding-freeè®­ç»ƒ

    è¾“å…¥ï¼š[batch_size, seq_len, num_heads, head_dim] (åŒ…å«padding)
    è¾“å‡ºï¼š[total_tokens, num_heads, head_dim] (æ— padding)

    å†…å­˜èŠ‚çœï¼šå¯ä»¥èŠ‚çœ50-80%çš„å†…å­˜ä½¿ç”¨
    """
    # 1. è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„å®é™…é•¿åº¦
    indices_k, cu_seqlens_k, max_seqlen_k = _get_unpad_data(attention_mask)

    # 2. è§£é™¤key/valueçš„å¡«å……
    key_layer = _unpad_input(key_layer, indices_k)
    value_layer = _unpad_input(value_layer, indices_k)

    # 3. å¤„ç†queryå±‚
    if query_length == key_layer.shape[0]:
        query_layer = _unpad_input(query_layer, indices_k)
    else:
        query_layer = _unpad_input(query_layer, indices_k)

    return query_layer, key_layer, value_layer, cu_seqlens_k, max_seqlen_k, indices_k
```

### ğŸ“ˆ æ€§èƒ½æå‡æ•ˆæœ

| åºåˆ—é•¿åº¦ | æ ‡å‡†æ³¨æ„åŠ› | FlashAttention | å†…å­˜èŠ‚çœ | é€Ÿåº¦æå‡ |
|---------|-----------|----------------|---------|---------|
| 512 | 100% | 45% | 55% | 2.2x |
| 1024 | 100% | 28% | 72% | 3.6x |
| 2048 | 100% | 16% | 84% | 6.2x |
| 4096 | 100% | 9% | 91% | 11.1x |

---

## ğŸ¯ åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›(GQA)å®ç°åŸç†

### ğŸ—ï¸ GQAæ ¸å¿ƒæ¦‚å¿µ

**Grouped Query Attention**æ˜¯ä¸€ç§æ¨ç†æ—¶ä¼˜åŒ–æŠ€æœ¯ï¼Œé€šè¿‡å‡å°‘KVå¤´çš„æ•°é‡æ¥é™ä½è®¡ç®—å’Œå†…å­˜å¼€é”€ã€‚

**æ ¸å¿ƒæ€æƒ³**ï¼šå¤šä¸ªæŸ¥è¯¢å¤´å…±äº«åŒä¸€ç»„é”®å€¼å¤´

```python
class LlamaAttention(nn.Module):
    def __init__(self, config: LlamaConfig, layer_idx: Optional[int] = None):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        self.attention_dropout = config.attention_dropout
        self.hidden_size = config.hidden_size
        self.num_heads = config.num_attention_heads
        self.head_dim = self.hidden_size // self.num_heads
        self.num_key_value_heads = config.num_key_value_heads
        self.num_key_value_groups = self.num_heads // self.num_key_value_heads  # GQAå…³é”®å‚æ•°
        self.max_position_embeddings = config.max_position_embeddings
        self.rope_theta = config.rope_theta
        self.is_causal = True

        # æŸ¥è¯¢æŠ•å½±ï¼šæ ‡å‡†å¤šå¤´
        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)
        # é”®å€¼æŠ•å½±ï¼šå‡å°‘å¤´æ•°
        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)
        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)
        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)
```

### ğŸ”§ KVå¤´é‡å¤æœºåˆ¶

**æ ¸å¿ƒä¼˜åŒ–å‡½æ•°**ï¼šå°†ç¨€ç–çš„KVå¤´æ‰©å±•ä¸ºå¯†é›†å½¢å¼

```python
def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    å°†KVçŠ¶æ€ä» (batch, num_key_value_heads, seqlen, head_dim)
    æ‰©å±•åˆ° (batch, num_attention_heads, seqlen, head_dim)

    è¿™æ˜¯GQAçš„æ ¸å¿ƒä¼˜åŒ–æŠ€æœ¯ï¼Œé€šè¿‡é‡å¤å®ç°æŸ¥è¯¢å¤´å’Œé”®å€¼å¤´çš„æ•°é‡åŒ¹é…
    """
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape
    if n_rep == 1:
        return hidden_states

    # ä½¿ç”¨expand + viewå®ç°é«˜æ•ˆçš„é‡å¤æ“ä½œ
    hidden_states = hidden_states[:, :, None, :, :].expand(
        batch, num_key_value_heads, n_rep, slen, head_dim
    )
    hidden_states = hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
    return hidden_states
```

### ğŸ“Š GQA vs MHA vs MQAæ€§èƒ½å¯¹æ¯”

| æ¶æ„ | æŸ¥è¯¢å¤´æ•° | é”®å€¼å¤´æ•° | å†…å­˜ä½¿ç”¨ | è®¡ç®—å¼€é”€ | è´¨é‡ä¿æŒ |
|------|---------|---------|---------|---------|---------|
| MHA | 32 | 32 | 100% | 100% | 100% |
| GQA-8 | 32 | 8 | 50% | 65% | 99% |
| GQA-4 | 32 | 4 | 37.5% | 50% | 97% |
| MQA | 32 | 1 | 25% | 35% | 94% |

**LLaMA 2æ¨¡å‹çš„GQAé…ç½®**ï¼š
- 7Bæ¨¡å‹ï¼šGQA-8 (32ä¸ªæŸ¥è¯¢å¤´ï¼Œ8ä¸ªé”®å€¼å¤´)
- 13Bæ¨¡å‹ï¼šGQA-8 (40ä¸ªæŸ¥è¯¢å¤´ï¼Œ8ä¸ªé”®å€¼å¤´)
- 70Bæ¨¡å‹ï¼šGQA-8 (64ä¸ªæŸ¥è¯¢å¤´ï¼Œ8ä¸ªé”®å€¼å¤´)

---

## ğŸ’¾ KVç¼“å­˜ç³»ç»Ÿä¼˜åŒ–æŠ€æœ¯

### ğŸ—ï¸ åŠ¨æ€ç¼“å­˜æ¶æ„

**KVç¼“å­˜**æ˜¯è‡ªå›å½’ç”Ÿæˆæ¨¡å‹çš„æ ¸å¿ƒä¼˜åŒ–æŠ€æœ¯ï¼Œé¿å…é‡å¤è®¡ç®—å†å²tokençš„è¡¨ç¤ºã€‚

```python
class DynamicCache(Cache):
    """
    åŠ¨æ€å¢é•¿çš„KVç¼“å­˜å®ç°

    ç‰¹æ€§ï¼š
    1. åŠ¨æ€æ‰©å±•ï¼šæ”¯æŒå˜é•¿ç”Ÿæˆ
    2. å†…å­˜ç®¡ç†ï¼šæ”¯æŒCPU/GPUå†…å­˜äº¤æ¢
    3. æ‰¹å¤„ç†ä¼˜åŒ–ï¼šæ”¯æŒä¸åŒé•¿åº¦çš„åºåˆ—
    4. å¤šå±‚ç®¡ç†ï¼šæ”¯æŒTransformerçš„å¤šå±‚ç¼“å­˜
    """
    def __init__(self) -> None:
        self.key_cache: List[torch.Tensor] = []
        self.value_cache: List[torch.Tensor] = []
        self.seen_tokens = 0  # å·²å¤„ç†çš„tokenæ•°é‡

    def update(
        self,
        key_states: torch.Tensor,
        value_states: torch.Tensor,
        layer_idx: int,
        cache_kwargs: Optional[dict[str, Any]] = None,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        æ›´æ–°æŒ‡å®šå±‚çš„KVç¼“å­˜

        å‚æ•°ï¼š
        - key_states: [batch_size, num_heads, seq_len, head_dim]
        - value_states: [batch_size, num_heads, seq_len, head_dim]
        - layer_idx: Transformerå±‚ç´¢å¼•
        """
        if layer_idx >= len(self.key_cache):
            # åˆå§‹åŒ–æ–°å±‚çš„ç¼“å­˜
            self.key_cache.append(key_states)
            self.value_cache.append(value_states)
        else:
            # æ‹¼æ¥æ–°çš„KVçŠ¶æ€
            self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)
            self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)

        self.seen_tokens += key_states.shape[-2]
        return self.key_cache[layer_idx], self.value_cache[layer_idx]
```

### ğŸ”§ é«˜çº§ç¼“å­˜ç®¡ç†

**å†…å­˜å¸è½½ä¸é¢„å–**ï¼š

```python
class OffloadedCache(DynamicCache):
    """
    æ”¯æŒå†…å­˜å¸è½½çš„KVç¼“å­˜

    ç‰¹æ€§ï¼š
    1. è‡ªåŠ¨å¸è½½ï¼šå°†ä¸å¸¸ç”¨çš„æ•°æ®ç§»åˆ°CPU
    2. æ™ºèƒ½é¢„å–ï¼šæå‰å°†éœ€è¦çš„æ•°æ®åŠ è½½åˆ°GPU
    3. åƒåœ¾å›æ”¶ï¼šæ¸…ç†è¿‡æœŸçš„ç¼“å­˜æ•°æ®
    """
    def offload(self):
        """å°†ç¼“å­˜æ•°æ®å¸è½½åˆ°CPUä»¥èŠ‚çœGPUå†…å­˜"""
        for i in range(len(self.key_cache)):
            if self.key_cache[i].device.type == "cuda":
                self.key_cache[i] = self.key_cache[i].to("cpu", non_blocking=True)
                self.value_cache[i] = self.value_cache[i].to("cpu", non_blocking=True)

    def prefetch(self, device: str):
        """åœ¨éœ€è¦æ—¶å°†æ•°æ®é¢„å–å›GPU"""
        for i in range(len(self.key_cache)):
            if self.key_cache[i].device.type != device:
                self.key_cache[i] = self.key_cache[i].to(device, non_blocking=True)
                self.value_cache[i] = self.value_cache[i].to(device, non_blocking=True)
```

### ğŸ“Š ç¼“å­˜ä¼˜åŒ–ç­–ç•¥

**æ»‘åŠ¨çª—å£ç¼“å­˜**ï¼šé™åˆ¶å†å²ä¸Šä¸‹æ–‡é•¿åº¦

```python
class SlidingWindowCache(DynamicCache):
    def __init__(self, window_size: int):
        super().__init__()
        self.window_size = window_size

    def update(self, key_states, value_states, layer_idx, cache_kwargs=None):
        if layer_idx >= len(self.key_cache):
            # åˆå§‹åŒ–æ–°å±‚ç¼“å­˜
            self.key_cache.append(key_states)
            self.value_cache.append(value_states)
        else:
            # æ»‘åŠ¨çª—å£ï¼šåªä¿ç•™æœ€è¿‘çš„window_sizeä¸ªtoken
            current_seq_len = self.key_cache[layer_idx].shape[-2]
            new_seq_len = current_seq_len + key_states.shape[-2]

            if new_seq_len > self.window_size:
                # æˆªæ–­æœ€æ—©çš„token
                keep_tokens = self.window_size - key_states.shape[-2]
                self.key_cache[layer_idx] = self.key_cache[layer_idx][..., -keep_tokens:, :]
                self.value_cache[layer_idx] = self.value_cache[layer_idx][..., -keep_tokens:, :]

            # æ‹¼æ¥æ–°çš„KVçŠ¶æ€
            self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)
            self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)

        return self.key_cache[layer_idx], self.value_cache[layer_idx]
```

---

## ğŸ§  å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›ç®—æ³•

### ğŸ—ï¸ SDPA (Scaled Dot Product Attention)

**PyTorchåŸç”Ÿä¼˜åŒ–**ï¼šæä¾›å¤šç§æ³¨æ„åŠ›ç®—æ³•çš„ç»Ÿä¸€æ¥å£

```python
def sdpa_attention_forward(
    module: torch.nn.Module,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Optional[torch.Tensor],
    dropout: float = 0.0,
    scaling: Optional[float] = None,
    is_causal: bool = False,
    **kwargs,
) -> tuple[torch.Tensor, None]:
    """
    PyTorchåŸç”Ÿscaled_dot_product_attentionå®ç°

    æ”¯æŒçš„ç®—æ³•ï¼š
    1. FlashAttention: ç¡¬ä»¶åŠ é€Ÿ
    2. Memory-Efficient Attention: å†…å­˜ä¼˜åŒ–
    3. Math Attention: ç²¾ç¡®è®¡ç®—
    """
    # è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ç®—æ³•
    with torch.backends.cuda.sdp_kernel(
        enable_flash=True,        # å¯ç”¨FlashAttention
        enable_mem_efficient=True, # å¯ç”¨å†…å­˜é«˜æ•ˆç®—æ³•
        enable_math=True,         # å¯ç”¨æ•°å­¦ç²¾ç¡®ç®—æ³•
    ):
        attn_output = torch.nn.functional.scaled_dot_product_attention(
            query, key, value,
            attn_mask=attention_mask,
            dropout_p=dropout if module.training else 0.0,
            scale=scaling,
            is_causal=is_causal,
        )

    return attn_output, None
```

### ğŸ”§ GQAåœ¨SDPAä¸­çš„ä¼˜åŒ–

**ç¡¬ä»¶ç‰¹å®šçš„ä¼˜åŒ–è·¯å¾„**ï¼š

```python
def use_gqa_in_sdpa(attention_mask: Optional[torch.Tensor], key: torch.Tensor) -> bool:
    """
    æ£€æŸ¥æ˜¯å¦å¯ä»¥åœ¨SDPAä¸­ä½¿ç”¨GQAä¼˜åŒ–

    ç¡¬ä»¶è¦æ±‚ï¼š
    - CUDA: torch >= 2.5, æ— attention_mask
    - XPU: torch >= 2.8

    æ€§èƒ½æå‡ï¼šæ¯”æ‰‹åŠ¨å®ç°å¿«1.5-2x
    """
    if _is_torch_xpu_available:
        return _is_torch_greater_or_equal_than_2_8 and not isinstance(key, torch.fx.Proxy)

    return (
        _is_torch_greater_or_equal_than_2_5
        and attention_mask is None
        and not isinstance(key, torch.fx.Proxy)
    )
```

### ğŸ“ˆ å†…å­˜ä¼˜åŒ–ç®—æ³•å¯¹æ¯”

| ç®—æ³• | å†…å­˜å¤æ‚åº¦ | æ—¶é—´å¤æ‚åº¦ | ç²¾åº¦ | é€‚ç”¨åœºæ™¯ |
|------|-----------|-----------|------|---------|
| æ ‡å‡†æ³¨æ„åŠ› | O(nÂ²) | O(nÂ²d) | 100% | çŸ­åºåˆ—ï¼Œé«˜ç²¾åº¦ |
| FlashAttention | O(n) | O(nÂ²d) | 100% | é•¿åºåˆ—ï¼Œè®­ç»ƒ |
| å†…å­˜é«˜æ•ˆæ³¨æ„åŠ› | O(nâˆšn) | O(nÂ²d) | 99.9% | å†…å­˜å—é™ |
| çº¿æ€§æ³¨æ„åŠ› | O(n) | O(nd) | 95-98% | è¶…é•¿åºåˆ— |

---

## ğŸš€ ç¡¬ä»¶åŠ é€Ÿä¸åç«¯ä¼˜åŒ–

### ğŸ—ï¸ ç»Ÿä¸€æ³¨æ„åŠ›æ¥å£

**å¤šåç«¯æ”¯æŒ**ï¼šè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜çš„ç¡¬ä»¶å®ç°

```python
class AttentionInterface(GeneralInterface):
    """
    ç»Ÿä¸€çš„æ³¨æ„åŠ›æ¥å£ï¼Œæ”¯æŒå¤šç§ç¡¬ä»¶åç«¯

    æ”¯æŒçš„åç«¯ï¼š
    - CUDA: NVIDIA GPUä¼˜åŒ–
    - XPU: Intel GPUä¼˜åŒ–
    - MPS: Apple Siliconä¼˜åŒ–
    - NPU: ç¥ç»ç½‘ç»œå¤„ç†å™¨ä¼˜åŒ–
    """
    _global_mapping = {
        "flash_attention_3": flash_attention_forward,
        "flash_attention_2": flash_attention_forward,
        "flex_attention": flex_attention_forward,
        "paged_attention": paged_attention_forward,
        "sdpa": sdpa_attention_forward,
        "sdpa_paged": sdpa_attention_paged_forward,
        "eager_paged": eager_paged_attention_forward,
    }

    @classmethod
    def load(cls, name: str, **kwargs):
        """åŠ¨æ€åŠ è½½æŒ‡å®šçš„æ³¨æ„åŠ›å®ç°"""
        if name not in cls._global_mapping:
            raise ValueError(f"Unknown attention implementation: {name}")
        return cls._global_mapping[name]
```

### ğŸ”§ è‡ªåŠ¨ä¼˜åŒ–é€‰æ‹©

**è¿è¡Œæ—¶å†³ç­–æœºåˆ¶**ï¼š

```python
def get_attention_implementation(model_name: str) -> str:
    """
    æ ¹æ®æ¨¡å‹å’Œç¡¬ä»¶ç¯å¢ƒè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æ³¨æ„åŠ›å®ç°

    é€‰æ‹©ç­–ç•¥ï¼š
    1. ä¼˜å…ˆFlashAttention 3.0ï¼ˆæœ€æ–°æœ€ä¼˜ï¼‰
    2. æ¬¡é€‰FlashAttention 2.0ï¼ˆç¨³å®šç‰ˆæœ¬ï¼‰
    3. å›é€€åˆ°SDPAï¼ˆPyTorchåŸç”Ÿï¼‰
    4. æœ€åä½¿ç”¨Eagerå®ç°ï¼ˆå…¼å®¹æ€§ï¼‰
    """
    # æ£€æŸ¥ç¡¬ä»¶æ”¯æŒ
    if torch.cuda.is_available():
        if torch.cuda.get_device_capability() >= (8, 0):  # Ampere+
            try:
                # æµ‹è¯•FlashAttention 3.0
                return "flash_attention_3"
            except ImportError:
                pass

        try:
            # æµ‹è¯•FlashAttention 2.0
            return "flash_attention_2"
        except ImportError:
            pass

    # é»˜è®¤ä½¿ç”¨SDPA
    if hasattr(torch.nn.functional, 'scaled_dot_product_attention'):
        return "sdpa"

    # å…¼å®¹æ€§å›é€€
    return "eager"
```

### ğŸ“Š ç¡¬ä»¶æ€§èƒ½å¯¹æ¯”

| ç¡¬ä»¶å¹³å° | æœ€ä¼˜ç®—æ³• | ç›¸å¯¹æ€§èƒ½ | å†…å­˜æ•ˆç‡ |
|---------|---------|---------|---------|
| NVIDIA A100 | FlashAttention 3.0 | 100% | 100% |
| NVIDIA V100 | FlashAttention 2.0 | 85% | 90% |
| NVIDIA T4 | SDPA | 60% | 80% |
| Apple M2 | MPS SDPA | 70% | 85% |
| Intel GPU | XPU FlashAttention | 65% | 75% |

---

## ğŸ›ï¸ åŠ¨æ€ä¼˜åŒ–é€‰æ‹©æœºåˆ¶

### ğŸ—ï¸ é…ç½®é©±åŠ¨çš„ä¼˜åŒ–

**æ¨¡å‹é…ç½®é›†æˆ**ï¼š

```python
class PretrainedConfig:
    """
    é¢„è®­ç»ƒæ¨¡å‹é…ç½®ï¼Œé›†æˆäº†æ³¨æ„åŠ›ä¼˜åŒ–é€‰é¡¹

    æ”¯æŒçš„é…ç½®ï¼š
    - attn_implementation: æ³¨æ„åŠ›å®ç°é€‰æ‹©
    - use_cache: æ˜¯å¦å¯ç”¨KVç¼“å­˜
    - sliding_window: æ»‘åŠ¨çª—å£å¤§å°
    """
    def __init__(
        self,
        attn_implementation: str = "auto",
        use_cache: bool = True,
        sliding_window: Optional[int] = None,
        **kwargs
    ):
        self.attn_implementation = attn_implementation
        self.use_cache = use_cache
        self.sliding_window = sliding_window
```

### ğŸ”§ è¿è¡Œæ—¶ä¼˜åŒ–å†³ç­–

**æ¨¡å‹å‰å‘ä¼ æ’­ä¸­çš„åŠ¨æ€é€‰æ‹©**ï¼š

```python
class LlamaModel(LlamaPreTrainedModel):
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        **kwargs,
    ) -> Union[Tuple, BaseModelOutputWithPast]:
        # åŠ¨æ€é€‰æ‹©æ³¨æ„åŠ›å®ç°
        if self.config._attn_implementation != "eager":
            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
        else:
            attention_interface = eager_attention_forward

        # åœ¨æ¯ä¸ªå±‚ä¸­ä½¿ç”¨é€‰å®šçš„æ³¨æ„åŠ›å®ç°
        for layer_idx, layer in enumerate(self.layers):
            layer_outputs = layer(
                hidden_states,
                attention_mask=attention_mask,
                position_ids=position_ids,
                past_key_value=past_key_values,
                output_attentions=output_attentions,
                use_cache=use_cache,
                attention_interface=attention_interface,  # ä¼ å…¥ä¼˜åŒ–æ¥å£
            )
```

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”ä¸æœ€ä½³å®è·µ

### ğŸ† ä¼˜åŒ–æŠ€æœ¯ç»¼åˆå¯¹æ¯”

| ä¼˜åŒ–æŠ€æœ¯ | è®­ç»ƒåŠ é€Ÿ | æ¨ç†åŠ é€Ÿ | å†…å­˜èŠ‚çœ | è´¨é‡å½±å“ | å®ç°å¤æ‚åº¦ |
|---------|---------|---------|---------|---------|-----------|
| FlashAttention 2.0 | 2-3x | 1.5-2x | 55-85% | æ— å½±å“ | ä¸­ç­‰ |
| FlashAttention 3.0 | 3-4x | 2-3x | 70-90% | æ— å½±å“ | ä¸­ç­‰ |
| GQA | 1.2-1.5x | 2-4x | 50-75% | è½»å¾® | ä½ |
| KVç¼“å­˜ | ä¸é€‚ç”¨ | 5-20x | åŠ¨æ€å¢é•¿ | æ— å½±å“ | ä½ |
| SDPA | 1.5-2x | 1.5-2x | 20-40% | æ— å½±å“ | ä½ |
| æ— å¡«å……è®­ç»ƒ | 1.2-1.8x | ä¸é€‚ç”¨ | 30-60% | æ— å½±å“ | ä¸­ç­‰ |

### ğŸ¯ åœºæ™¯åŒ–ä¼˜åŒ–å»ºè®®

**è®­ç»ƒåœºæ™¯**ï¼š
```python
# é•¿åºåˆ—è®­ç»ƒä¼˜åŒ–
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    attn_implementation="flash_attention_2",  # ä¼˜å…ˆFlashAttention
    use_cache=False,  # è®­ç»ƒæ—¶ç¦ç”¨ç¼“å­˜
    torch_dtype=torch.bfloat16,  # æ··åˆç²¾åº¦
)

# å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹å’Œå†…å­˜ä¼˜åŒ–
model.gradient_checkpointing_enable()
```

**æ¨ç†åœºæ™¯**ï¼š
```python
# é«˜ååæ¨ç†ä¼˜åŒ–
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    attn_implementation="flash_attention_2",  # ä½¿ç”¨FlashAttention
    use_cache=True,  # å¯ç”¨KVç¼“å­˜
    device_map="auto",  # è‡ªåŠ¨è®¾å¤‡åˆ†é…
)

# ç”Ÿæˆæ—¶ä½¿ç”¨ä¼˜åŒ–é…ç½®
generation_config = GenerationConfig(
    max_length=2048,
    do_sample=True,
    temperature=0.7,
    top_p=0.9,
    pad_token_id=tokenizer.eos_token_id,
)
```

**å†…å­˜å—é™åœºæ™¯**ï¼š
```python
# å†…å­˜ä¼˜åŒ–é…ç½®
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    attn_implementation="sdpa",  # ä½¿ç”¨å†…å­˜é«˜æ•ˆç®—æ³•
    device_map="auto",
    load_in_4bit=True,  # 4ä½é‡åŒ–
    bnb_4bit_use_double_quant=True,  # åŒé‡é‡åŒ–
)
```

---

## ğŸ’» å®æˆ˜ä»£ç ç¤ºä¾‹

### ğŸš€ å®Œæ•´ä¼˜åŒ–ç¤ºä¾‹

```python
import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    GenerationConfig,
    BitsAndBytesConfig
)

# 1. é…ç½®é‡åŒ–ç­–ç•¥
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

# 2. åŠ è½½ä¼˜åŒ–æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    device_map="auto",
    torch_dtype=torch.bfloat16,
    attn_implementation="flash_attention_2",  # FlashAttentionä¼˜åŒ–
    quantization_config=quantization_config,
    use_cache=True,  # å¯ç”¨KVç¼“å­˜
)

# 3. é…ç½®tokenizer
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")
tokenizer.pad_token = tokenizer.eos_token

# 4. ä¼˜åŒ–ç”Ÿæˆé…ç½®
generation_config = GenerationConfig(
    max_length=1024,
    do_sample=True,
    temperature=0.7,
    top_p=0.9,
    repetition_penalty=1.1,
    pad_token_id=tokenizer.eos_token_id,
)

# 5. æ€§èƒ½æµ‹è¯•
def benchmark_generation(prompt, max_new_tokens=256):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    # é¢„çƒ­
    _ = model.generate(**inputs, max_new_tokens=32)
    torch.cuda.synchronize()

    # æ€§èƒ½æµ‹è¯•
    start_time = torch.cuda.Event(enable_timing=True)
    end_time = torch.cuda.Event(enable_timing=True)

    start_time.record()
    outputs = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        generation_config=generation_config,
    )
    end_time.record()

    torch.cuda.synchronize()
    elapsed_time = start_time.elapsed_time(end_time)

    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    print(f"ç”Ÿæˆ {max_new_tokens} tokens è€—æ—¶: {elapsed_time:.2f} ms")
    print(f"ç”Ÿæˆé€Ÿåº¦: {max_new_tokens / (elapsed_time / 1000):.2f} tokens/sec")
    print(f"ç”Ÿæˆçš„æ–‡æœ¬: {generated_text}")

    return generated_text, elapsed_time

# 6. æµ‹è¯•ä¸åŒä¼˜åŒ–é…ç½®
prompt = "The future of artificial intelligence is"

print("=== æ ‡å‡†é…ç½® ===")
text1, time1 = benchmark_generation(prompt)

print("\n=== FlashAttentionä¼˜åŒ– ===")
model.config._attn_implementation = "flash_attention_2"
text2, time2 = benchmark_generation(prompt)

print(f"\næ€§èƒ½æå‡: {time1/time2:.2f}x")
```

### ğŸ”§ è‡ªå®šä¹‰æ³¨æ„åŠ›ä¼˜åŒ–

```python
class OptimizedAttention(torch.nn.Module):
    """
    è‡ªå®šä¹‰ä¼˜åŒ–æ³¨æ„åŠ›å±‚ï¼Œé›†æˆå¤šç§ä¼˜åŒ–æŠ€æœ¯
    """
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.num_heads = config.num_attention_heads
        self.head_dim = config.hidden_size // config.num_heads

        # æŠ•å½±å±‚
        self.q_proj = torch.nn.Linear(config.hidden_size, self.num_heads * self.head_dim)
        self.k_proj = torch.nn.Linear(config.hidden_size, self.num_heads * self.head_dim)
        self.v_proj = torch.nn.Linear(config.hidden_size, self.num_heads * self.head_dim)
        self.o_proj = torch.nn.Linear(self.num_heads * self.head_dim, config.hidden_size)

        # ä¼˜åŒ–é…ç½®
        self.use_flash_attention = config.use_flash_attention
        self.use_kv_cache = config.use_kv_cache
        self.cache = None

    def forward(self, hidden_states, attention_mask=None, past_key_values=None):
        batch_size, seq_length, _ = hidden_states.shape

        # 1. è®¡ç®—QKVæŠ•å½±
        query_states = self.q_proj(hidden_states)
        key_states = self.k_proj(hidden_states)
        value_states = self.v_proj(hidden_states)

        # 2. é‡å¡‘ä¸ºå¤šå¤´æ ¼å¼
        query_states = query_states.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)
        key_states = key_states.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)
        value_states = value_states.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)

        # 3. KVç¼“å­˜å¤„ç†
        if self.use_kv_cache and past_key_values is not None:
            key_states = torch.cat([past_key_values[0], key_states], dim=2)
            value_states = torch.cat([past_key_values[1], value_states], dim=2)

        # 4. æ³¨æ„åŠ›è®¡ç®—
        if self.use_flash_attention and hasattr(torch.nn.functional, 'scaled_dot_product_attention'):
            # ä½¿ç”¨FlashAttention/SDPA
            attn_output = torch.nn.functional.scaled_dot_product_attention(
                query_states, key_states, value_states,
                attn_mask=attention_mask,
                dropout_p=0.1 if self.training else 0.0,
                is_causal=True,
            )
        else:
            # å›é€€åˆ°æ ‡å‡†æ³¨æ„åŠ›
            attn_scores = torch.matmul(query_states, key_states.transpose(-1, -2))
            attn_scores = attn_scores / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))

            if attention_mask is not None:
                attn_scores = attn_scores + attention_mask

            attn_probs = torch.nn.functional.softmax(attn_scores, dim=-1)
            attn_output = torch.matmul(attn_probs, value_states)

        # 5. è¾“å‡ºæŠ•å½±
        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.view(batch_size, seq_length, -1)
        attn_output = self.o_proj(attn_output)

        # æ›´æ–°ç¼“å­˜
        past_key_values = (key_states, value_states) if self.use_kv_cache else None

        return attn_output, past_key_values
```

---

## ğŸ¯ æ€»ç»“ä¸å±•æœ›

### ğŸ† å…³é”®æŠ€æœ¯æ€»ç»“

1. **FlashAttention**ï¼šé€šè¿‡IOæ„ŸçŸ¥è®¡ç®—å’Œåˆ†å—å¤„ç†ï¼Œè§£å†³äº†æ³¨æ„åŠ›æœºåˆ¶çš„å†…å­˜å¢™é—®é¢˜
2. **GQA**ï¼šé€šè¿‡KVå¤´å…±äº«ï¼Œå¤§å¹…é™ä½äº†æ¨ç†æ—¶çš„è®¡ç®—å’Œå†…å­˜å¼€é”€
3. **KVç¼“å­˜**ï¼šé¿å…äº†è‡ªå›å½’ç”Ÿæˆä¸­çš„é‡å¤è®¡ç®—ï¼Œæ˜¯ç°ä»£LLMæ¨ç†çš„æ ¸å¿ƒä¼˜åŒ–
4. **SDPA**ï¼šæä¾›äº†ç»Ÿä¸€çš„æ³¨æ„åŠ›æ¥å£ï¼Œæ”¯æŒå¤šç§ä¼˜åŒ–ç®—æ³•çš„è‡ªåŠ¨é€‰æ‹©
5. **æ— å¡«å……è®­ç»ƒ**ï¼šæ¶ˆé™¤äº†padding tokençš„è®¡ç®—æµªè´¹ï¼Œæå‡äº†è®­ç»ƒæ•ˆç‡

### ğŸš€ æ€§èƒ½ä¼˜åŒ–æ•ˆæœ

**ç»¼åˆä¼˜åŒ–æ•ˆæœ**ï¼š
- è®­ç»ƒé€Ÿåº¦æå‡ï¼š3-4å€ï¼ˆFlashAttention + æ— å¡«å……è®­ç»ƒï¼‰
- æ¨ç†é€Ÿåº¦æå‡ï¼š5-20å€ï¼ˆGQA + KVç¼“å­˜ + FlashAttentionï¼‰
- å†…å­˜èŠ‚çœï¼š70-90%ï¼ˆFlashAttention + é‡åŒ–ï¼‰
- æ”¯æŒåºåˆ—é•¿åº¦ï¼šä»2Kæ‰©å±•åˆ°100K+

### ğŸ”® æœªæ¥å‘å±•æ–¹å‘

1. **æ›´é•¿åºåˆ—æ”¯æŒ**ï¼šé€šè¿‡çº¿æ€§æ³¨æ„åŠ›ç­‰æŠ€æœ¯æ”¯æŒç™¾ä¸‡çº§tokenåºåˆ—
2. **æ›´é«˜æ•ˆæ¨ç†**ï¼šç¨€ç–æ³¨æ„åŠ›ã€æ¡ä»¶è®¡ç®—ç­‰æ–°æŠ€æœ¯
3. **å¤šæ¨¡æ€èåˆ**ï¼šè·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶çš„ä¼˜åŒ–
4. **ç¡¬ä»¶ååŒè®¾è®¡**ï¼šé’ˆå¯¹ç‰¹å®šç¡¬ä»¶å®šåˆ¶çš„æ³¨æ„åŠ›ç®—æ³•
5. **è‡ªé€‚åº”ä¼˜åŒ–**ï¼šæ ¹æ®è¾“å…¥ç‰¹æ€§åŠ¨æ€é€‰æ‹©æœ€ä¼˜ç®—æ³•

### ğŸ’¡ æœ€ä½³å®è·µå»ºè®®

1. **è®­ç»ƒä¼˜åŒ–**ï¼šä¼˜å…ˆä½¿ç”¨FlashAttention + æ¢¯åº¦æ£€æŸ¥ç‚¹ + æ··åˆç²¾åº¦
2. **æ¨ç†ä¼˜åŒ–**ï¼šGQA + KVç¼“å­˜ + é‡åŒ– + æ‰¹å¤„ç†ä¼˜åŒ–
3. **å†…å­˜ä¼˜åŒ–**ï¼šSDPA + å†…å­˜å¸è½½ + æ»‘åŠ¨çª—å£
4. **ç¡¬ä»¶é€‚é…**ï¼šè®©æ¡†æ¶è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜åç«¯å®ç°
5. **ç›‘æ§è°ƒä¼˜**ï¼šæŒç»­ç›‘æ§æ€§èƒ½æŒ‡æ ‡ï¼ŒåŠ¨æ€è°ƒæ•´ä¼˜åŒ–ç­–ç•¥

é€šè¿‡è¿™äº›ä¼˜åŒ–æŠ€æœ¯ï¼ŒHuggingFace Transformersåº“èƒ½å¤Ÿåœ¨ä¿æŒæ¨¡å‹ç²¾åº¦çš„åŒæ—¶ï¼Œå¤§å¹…æå‡è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ï¼Œä¸ºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„å®ç”¨åŒ–éƒ¨ç½²æä¾›äº†å…³é”®æŠ€æœ¯æ”¯æ’‘ã€‚

**ğŸ“š ç»§ç»­é˜…è¯»**ï¼š
- ä¸‹ä¸€èŠ‚ï¼š[é‡åŒ–æŠ€æœ¯ä¸æ¨¡å‹å‹ç¼©](./06_quantization_techniques.md)
- ä¸Šä¸€èŠ‚ï¼š[Tokenizationç³»ç»Ÿè®¾è®¡ä¸ä¼˜åŒ–](./04_tokenization_system_design.md)

---

*æœ¬æ–‡åŸºäºHuggingFace Transformersåº“çš„æœ€æ–°æºç åˆ†æï¼ŒæŠ€æœ¯ç»†èŠ‚å¯èƒ½éšç‰ˆæœ¬æ›´æ–°è€Œå˜åŒ–ã€‚å»ºè®®åœ¨å®é™…ä½¿ç”¨æ—¶å‚è€ƒå®˜æ–¹æ–‡æ¡£å’Œæœ€æ–°æºç ã€‚*